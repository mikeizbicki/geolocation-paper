%\documentclass{article}
\documentclass[sigconf,10pt]{acmart}

\setcopyright{rightsretained}
%\acmDOI{10.475/123_4}
%\acmISBN{123-4567-24-567/08/06}
\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El Paso, Texas USA}
%\acmYear{1997}
%\copyrightyear{2016}
%\acmArticle{4}
%\acmPrice{15.00}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{xcolor}
\usepackage{CJKutf8}

\usepackage[inline]{enumitem}
\setlist[description]{topsep=0.1cm,itemsep=0.1cm,style=unboxed,leftmargin=0cm,listparindent=\parindent}

%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
%\usepackage{algorithmicx}
%\usepackage{algorithm2e}

%\usepackage[authordate,bibencoding=auto,strict,backend=biber,natbib]{biblatex-chicago}
%\usepackage[round]{natbib}   % omit 'round' option if you prefer square brackets
%\bibliographystyle{plainnat}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

\usepackage{tikz}
\usetikzlibrary{matrix, positioning, fit}
\usepackage{pgfplots}
\pgfplotsset{width=7cm,compat=1.8}
\definecolor{darkgreen}{RGB}{0,127,0}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{cor}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\vecspan}{span}
\DeclareMathOperator*{\affspan}{aff}
\DeclareMathOperator*{\subG}{subG}
\DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\E}{\mathbb{E}}

\newcommand{\str}[1]{\texttt{#1}}
\newcommand{\defn}[1]{\textit{#1}}

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\trans}[1]{{#1}^{\top}}

\newcommand{\abs}[1]{\lvert{#1}\rvert}
\newcommand{\ltwo}[1]{\lVert {#1} \rVert_2}
\newcommand{\set}{\mathcal}
\renewcommand{\vec}{\mathbf}

\newcommand{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{Z}

\newcommand{\w}{W}
\newcommand{\what}{\hat\w}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{y}

\newcommand{\dist}{d}
\newcommand{\Dist}{\mathcal D}
\newcommand{\kernel}{k}
\newcommand{\loc}{_{\textit{loc}}}
\newcommand{\gmap}{_{\textit{gmap}}}
\newcommand{\gcirc}{_\textit{gcirc}}

\newcommand{\loss}{\ell}
\newcommand{\reg}{r}

%\newcommand{\prob}[1]{\text{Pr}\left({#1}\right)}
\newcommand{\prob}[1]{p\!\left({#1}\right)}
\newcommand{\cprob}[2]{\prob{{#1} | {#2}}}
\newcommand{\normal}[2]{\mathcal{N}({#1},{#2})}
\newcommand{\eye}{I}

%\newcommand{\plots}[1]{}
\newcommand{\plots}[1]{#1}
\newcommand{\ignore}[1]{}
\newcommand{\fixme}[1]{\textcolor{red}{\textbf{FIXME:} {#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\uniloc}{\textsc{UniLoc}}

\newcommand{\tweetdata}[1]{{\texttt{#1}~}}
\newcommand{\place        }{\tweetdata{place}}
\newcommand{\pseudoplace  }{\tweetdata{pseudo-place}}
\renewcommand{\country      }{\tweetdata{country}}
\newcommand{\geo          }{\tweetdata{geo}}

\newcommand{\lata}{\text{lat}_1}
\newcommand{\latb}{\text{lat}_2}
\newcommand{\latd}{(\lata-\latb)}
\newcommand{\lona}{\text{lon}_1}
\newcommand{\lonb}{\text{lon}_2}
\newcommand{\lond}{(\lona-\lonb)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Geolocating Tweets in any Language at any Location}
%\titlenote{Produces the permission block, and copyright information}
%\subtitle{}
%\subtitlenote{The full version of the author's guide is available as \texttt{acmart.pdf} document}

\author{Anonymous Authors}
\affiliation{}
\email{}

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{shortauthors}

\begin{document}

\begin{abstract}
    We consider the problem of determining the GPS coordinates that a tweet was sent from based only on its message contents.
    Our method works on tweets sent in any language,
    and our evaluation dataset contains \fixme{} tweets in more than \fixme{} languages from around the world.
    This dataset contains the largest number of languages used for any currently published natural language processing task.
    We achieve high accuracy across many languages through a novel character embedding that lets us develop a convolutional neural network that shares information between the languages.
    The output layer of our network exploits the non-Euclidean properties of the Earth's surface to accurately predict GPS locations.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
    \fixme{}
\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}


\keywords{\fixme{}}

\maketitle

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Twitter is one of the Internet's largest social networks,
with users from all over the world sending 500 million tweets everyday.
This paper introduces \uniloc, 
a system that uses linguistic clues present in the tweet's text to determine where in the world the tweet was sent from.

%\item[Example.]
We begin with a motivating example.
Let's try to guess where the following tweet was sent from:
\begin{quote}
    \str{Qu\'e gran ma\~nana... emocionante.  Pod\'eis ver el v\'ideo abajo en el enlace.}%
\footnote{Translation: ``What a great morning... exciting.  You can see the video below the post.''}
\end{quote}
The first clue we consider is the tweet's language.
Because this tweet was written in Spanish,
it was likely sent from a country with a large Spanish-speaking population
(such as Spain, Mexico, or the United States).
%Approximately twenty countries satisfy this criteria (e.g.\ Spain, Mexico, and the United States).
%This leaves us with about twenty countries that the tweet might have been sent from.
%or a country with a large Spanish speaking minority (such as the US).
%While this narrows our search considerably,
%Subtle differences in dialect between these countries can be used to further reduce the possibilities.
%The clue that this tweet was sent from Spain, however, is the word \str{pod\'eis}.
%This word is a conjugation of the verb \str{poder} in the \defn{vosotros} form.
%We can narrow down the tweet's location further using d
We can further refine our guess by using differences in the Spanish dialects spoken in these countries.
In this tweet, the word \str{pod\'eis} is the most important clue.
\str{pod\'eis} is a conjugation of the verb \str{poder} in the vosotros form.
The vosotros form is only used in the Castilian dialect,
which is only used in Spain.
All other Spanish dialects use the ustedes form, 
which would conjugate the verb as \str{pueden}.
%We therefore can (correctly) conclude that the tweet was sent from Spain.

\uniloc\ uses the above reasoning to correctly identify that this tweet comes from Spain,
and further creates a detailed probability distribution over the GPS coordinates that the tweet is likely to have been sent from
(see Figure \ref{fig:poder}).
A key feature of \uniloc\ is that it was not preprogrammed with this linguistic knowledge.
Instead, it learned this pattern from scratch using a novel character level \defn{convolutional neural network} (CNN). 
This CNN can take any unicode character as input,
and it learns similar patterns for English, Arabic, Japanese, and the more than 65 languages present in our dataset.
Furthermore, these patterns generalize to words not present in the original training set.
For example, the Spanish verb \str{fallecer} is conjugated as \str{fallec\'eis} in the Castilian vosotros form.
This word is not present in our training set,
yet \uniloc\ is still able to understand that \str{fallec\'eis} is likely to have been written in Spain based on the conjugation.
Previous geolocating systems do not understand these character level differences in dialect,
and they do not work in our highly multilingual context.

%and identifies key letters in the tweet that are highly important in determining its location 

\begin{figure}
    \resizebox{0.485\textwidth}{!}{\includegraphics{img/infer/2}}
    \caption{}
    \label{fig:poder}
\end{figure}

\begin{description}
\item[Why care about geolocation?]
    Geolocating tweets is an important problem for three reasons.

First, many important applications require geolocated tweets as input.
For example, geotagged tweets have been used to map the spread of influenza \citep{paul2014twitter,santillana2015combining},
for measuring the impact of earthquakes \citep{sakaki2010earthquake},
for coordinating emergency services \citep{klein2012detection,imran2016twitter,rudra2016summarizing,pohl2016online},
for measuring the spread of political opinions \citep{conover2011political,barbera2014birds},
for comparing dietary habits in different locations \citep{widener2014using},
for measuring neighborhood happiness levels \citep{nguyen2016leveraging}, 
and for measuring unemployment rates \citep{antenucci2014using,llorente2015social}.
Unfortunately for these applications, only about 1\% of tweets are geotagged by their users.
A system that could predict the location of the other 99\% of tweets would immediately improve the accuracy and applicability of all of these methods.

Second, the models used for geotagging tweets help us understand the differences between different languages and dialects.
For example, previous work has used geotagged language models to map the dialects of Standard American English versus African-American English \citep{huang2016understanding,gonccalves2017fall,blodgett2016demographic}, and the various dialects of Spanish \citep{gonccalves2014crowdsourcing}.
Our language model is more advanced than previous models because it works for significantly more languages,
and it operates on the character level rather than the word level.
This more advanced model lets learn more detail about dialectical differences in many more languages than previous work.

Finally, geolocated tweets provide a large source of high quality labeled text.
Quality labeled text is rare and often the limitting factor in natural language processing tasks.
Transfer learning has therefore become a popular strategy to learn these tasks \citep{wang2015transfer,howard2018fine}.
The model we develop is particularly suitable for transfer learning because it incorporates many languages.
We open source our model and a toolkit to help practitioners use this model as the base for other multilingual NLP tasks.
%Furthermore, because our language model combines so many different languages,
%and can form the basis for multilingual classification tasks using transfer learning \citep{wang2015transfer,howard2018fine}.
%The best existing image transfer learning used billions of Instagram images labelled by their hashtags from FAIR \citep{mahajan2018exploring}.

\ignore{
\item[Our contributions.]
Our contributions include:
\begin{enumerate}
    \item
        Our method works with all tweets, in all languages.
        Twitter has official support for XXX languages,
        but many other languages are represented as well.
        For example, nearly-dead languages like Latin.
        The pope has an official latin-only twitter account \str{@Pontifex\_ln}.
        The esperanto language is not officially supported, but has an active twitter community.
        And web aps will automatically translate a text into Klingon\footnote{The ap appears to have been taken offline, but a news article is available at https://www.tomsguide.com/us/Star-Trek-Online-Klingon-Translator,news-5294.html} and tweet it.
    \item
        Our method requires no ad-hoc preprocessing of the data,
        and in particular can achieve good results with only a single pass over the dataset.
    \item
        A new method for generating features at the character level.
    \item
        Our method can output gps coordinates and take advantage of the non-Euclidean geometry of the Earth's surface.
    \item
        A general framework that lets us easily mix and match the contributions from previous works.
    \item
        An online learning framework that is more natural for twitter than the batch frameworks previously proposed.
    \item
        \fixme{todo}
        A method for visualizing the important features that causes a tweet to be located in a particular location.
\end{enumerate}
}

\item[Contributions and Outline.]
In the next section, we introduce our dataset and formally define our problem.
Then in Section \ref{sec:words} we describe in detail why the word-level approach of previous geolocation systems cannot scale to the massively multilingual dataset we use.
In Section \ref{sec:model} we introduce our character level model that fixes the deficiencies of the world-level models.
We also describe two novel neural network output layers called angualar regression and the mixture of von-Mises,
and show how these methods take advantage of the geometry of the Earth's surface to enable us to predict exact GPS coordinates of a tweet.
Section \ref{sec:experiments} shows the experimental results showing that our method significantly outperforms previous methods.

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Identifying tweet location vs user location}
%
%In this paper, we focus on the task of identifying tweet location, rather than user location.
%Identifying the location of a tweet is the harder problem because there is less information available.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\noindent\input{img/hr-en}
\noindent\input{img/hr-es}
\caption{
    Different countries tweet in different languages and at different times of the day.
    For example, tweets sent in English are almost always sent from the United States,
    except around 9AM UTC (2AM PST).
    Tweets sent in Spanish are likely to be sent from either Argentina (from 2200UTC-0400UTC), Mexico (0500-0600UTC), or Spain (0600-2200UTC).
}
\label{fig:time-lang}
\end{figure*}

%\newpage
\section{The prediction task}
\label{sec:problem}

\newcommand{\dataname}{\textsc{GeoBillion}}
%We consider the problem of predicting a tweet's location given only the text of the tweet and the time the tweet was sent.
In this section we introduce the \dataname~ dataset for Twitter geolocation and describe the resulting prediction problem.
Our task is more difficult than previous work because we consider orders of magnitude more data,
significantly more languages,
and we attempt to predict exact GPS locations rather than just cities.

\begin{description}
\item[Dataset Overview.]
The \dataname\ dataset contains all tweets with geolocation information sent between 26 October 2017 and 08 July 2018.
The dataset contains 948,535,428 tweets written by $3.0\times10^6$ users.
Unlike previous work, 
we perform no filtering to remove ``hard'' tweets from the dataset,
and as a result our dataset is orders of magnitude larger than similar datasets.

%The WORLD dataset \citep{han2012geolocation} contains $1.4\times10^6$ users and $12\times10^6$ tweets.
The largest public dataset for geolocating tweets was previously the WORLD+ML dataset \citep{han2014text},
which contains only $23\times10^6$ tweets written by $2.1\times10^6$ users.
WORLD+ML includes tweets in all languages,
but removes hard-to-geolocate tweets that are not close to major cities.
Whereas the WORLD+ML dataset contained only 47\% non-English tweets,
our dataset contains 68\% non-English tweets.
Other datasets such as the WNUT \citep{han2016twitter}, WORLD \citep{han2012geolocation}, and NA \citep{roller2012supervised}
additionally remove non-English tweets and so are considerably smaller.

Each tweet is associated with three types of content:
The text of the tweet,
the language of the tweet,
and the time of the tweet.

\item[Language.]
The Twitter API associates with each tweet one of 65 possible officially supported languages.
The most popular language is English with $405\times10^6$ tweets,
and the least popular language is Uighur with only 157 tweets.
Many other languages are also present in our dataset, however.
$74\times10^6$ tweets are classified as being from an undefined language.
This can happen when the tweet contains too little text to determine the language,
or when the language is not one of Twitter's officially recognized 65 languages.
%For example, we have identified that the set of tweets with undefined language contains tweets in Czechoslovakian.
For example, Croatian is not an officially supported language,
and so tweets written in Croatian get classified as having an undefined language.
Furthermore, many unofficial languages get miscategorized into similar languages that are officially recognized.
For example, 
tweets written in Catalan get classified as Spanish,
and tweets written in Malay get classified as Indonesian.
Because the dataset is so large,
we make no effort to exhaustively identify all the languages it contains.
\fixme{Section XXX of the supplement shows examples of our language-agnostic model discovering these unofficial languages by itself and exploiting them for geolocating.}
Twitter also makes mistakes identifying known languages.

\item[Time.]
The Twitter API specifies the time of a tweet in milliseconds since the standard UNIX epoch 
(midnight 1 Jan 1970).
This time field provides a significant clue about a tweet's location because users are more likely to send tweets at different times of day.
See Figure \ref{fig:time-lang} for details.
Our experimental section uses time as a baseline for comparison purposes.

\item[Location.]
We are interested in predicting the location of tweets at two levels of granularity:
the country of origin and exact GPS coordinates.
There are three fields of the JSON object which contain JSON information at different levels of granularity.
\str{geo}, \str{place}, and \str{country\_code}.
When a tweet is sent, the user has the option of specifying the granularity of the tweet's location information.
The \str{geo} field contains exact GPS coordinates that the tweet was sent from.
Approximately 14 percent of the tweets in our dataset contain exact GPS coordinates.
The \str{place} field contains detailed information about the neighborhood, city, county, state, or country that the tweet was sent from.
The field defines a bounding box around the region and we take the centroid of the region as the tweet's true GPS coordinates.
The \str{country\_code} field specifies the country the tweet was sent from.

\item[User Metadata.]
The JSON object also contains significant information about the user who sent the tweet.
This metadata can be used to greatly increase the accuracy of geolocation 
\citep{hecht2011tweets,schulz2013multi,han2014text}.
We do not consider this metadata in our prediction problem;
however, because it obscures the ability to predict locations from the text.

For example, the metadata contains a user specified timezone field.
The timezone provides a surprisingly large amount of detail about a tweet's location.
At the bare minimum, 
the timezone determines the tweets longitude with high accuracy,
and so the problem of geolocating with timezone information reduces to the problem of determining the tweet's latitude.
Some timezones (like Hawaiian Standard Time) determine both longitude and latitude with high accuracy,
and so the tweet's text doesn't even need to be examined to perform geolocation.
%Furthermore, weets in Hawaiian Standard Time fit within a small 100km radius, which means the text doesn't even have to be examined.
%For timezones like Hawaiian Standard Time,
%this is an exceedingly easy problem.
%Knowledge of the timezone transforms the problem into only predicting latitude.
%For example, essentially all tweets in Hawaiian Standard Time fit within a small 100km radius, which means the text doesn't even have to be examined.
Time zone information can also encode country location directly.
For example, Korea and Japan have essentially the same longitude, 
and so use the same offset from UTC for their time.
But they have different timezones (denoted Korea Standard Time and Japan Standard Time).
Knowledge of the timezone in these cases is sufficient to completely determine the country of origin.

The Twitter API also has tools to retrieve information about a user's previous tweets and the user's social graph.
We do not consider this metadata either.

Parts of the social graph do, however, appear in the tweet's text,
and so do get used explicitly.

\item[Prior work on geolocating tweets.]
A large body of work has formed around the problem of geolocating tweets,
but existing solutions work only on a small subset of available tweets that are particularly easy to geolocate.
The earliest methods work for English language tweets in the United States \citep{},
and state-of-the-art methods generalize to English tweets from major cities around the world \citep{}.
These methods all take advantage of the fact that certain English words are highly correlated with certain regions.
\fixme{For example, the word \str{Chargers} is highly correlated with tweets from San Diego, CA,
because the Chargers football team is located there.}
These existing methods do not work for most tweets, however,
because most tweets are neither located in the United States nor in English (see Figure \ref{fig:country/lang}).
\citet{maier2014language} considers geolocating Spanish tweets.
Our method works on tweets in any language sent from any location in the world.

\citet{park2013emoticon} studies emoticon usage in the first three years of the twitter platform.
They show, for example, that vertical emoticons are indicative of asian cultures and horizontal emoticons of european cultures.
\citet{lu2016learning} show that emoji usage varies around the world on smart phone SMS messages using a keyboard app.
\citet{ljubevsic2016global} shows that emoji usage can be used to predict tweet location.

\citet{nguyen2017kernel} use a RKHS method for geolocation.

%\citet{hays2008im2gps} performs image geolocation down to the gps coordinate level using $k$-nearest neighbor queries and then builds a kernel density estimate of the distribution.
%\citet{crandall2009mapping} uses a classification strategy.
%\citet{weyand2016planet} discretizes the world in a semantically meaningful method.

\begin{figure}
    \resizebox{0.225\textwidth}{!}{\input{img/country}}
    \resizebox{0.225\textwidth}{!}{\input{img/lang}}
    \caption{Most tweets are neither located in the United States nor in English,
    but prior work focuses on these two special cases.}
    \label{fig:country/lang}
\end{figure}

\item[Prior work on multilanguage models.]
Bilingual models have been developed for many tasks,
but models supporting more than 2 languages remain uncommon \citep{ruder2017survey}.

\citet{zaidan2014arabic} Arabic dialects.

\citet{refaee2014arabic} Arabic with twitter.

\citet{mohammad2016translation} translation alters sentiment.


The \str{lang} field of a tweet is determined automatically by the twitter software using a combination of unpublished tweet and user level features.
\citet{blodgett2016demographic} show that existing language prediction tools such as \str{langid.py} \citep{lui2012langid} perform poorly relative to the \str{lang} field of the json object.
This effect is exacerbated when the tweets exhibit dialectical differences from the formal version of their language.

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problems with word-level features}
\label{sec:words}

Traditional text mining generates word level features in a three step process:
First, the input text is tokenized to extract words from the text.
Second, similar tokens are grouped together via lemmatization and stemming subroutines.
Finally, a feature vector is created using simple summary statistics such as $n$-grams or TF-IDF \citep{}.
This process works well for English-only text corpora,
but is essentially impossible in the highly multi-lingual corpus we consider.
In this section, we highlight four problems with these word-level features.

\begin{description}
    \item[Tokenization in multilingual corpora is difficult.]
        Tokenization of English text is easy because spaces are used to separate lexemes.%
        \footnote{Recall that a \defn{lexeme} is the basic lexical unit in language.
        In English and other European languages a lexeme and word are essentially the same.}
        Other languages, however, do not have similar cues to separate lexemes.
        For example, Chinese and Japanese do not use spaces at all,
        and Vietnamese traditionally uses spaces between syllables even within words.
        %Germanic languages use frequent compound words.
        Good tokenizers for these languages have existed for decades \citep[e.g.][]{fung1998extracting,huyen2008hybrid},
        however these tokenizers have four limitations that make them unsuitable for our task:
        First, they require that the text's language be known a priori so that an appropriate tokenization routine can be called.
        The Twitter API has a \str{language} field associated with each tweet,
        however the identified language is often wrong.
        Second, they cannot be used simultaneously on the same text.
        Many tweets are written in multiple languages simultaneously in what linguists call \defn{code switching}.
        No existing tokenizers support code switching.
        Third, existing tokenizers do not consider dialectical differences in their target language which may be of importance in geolocating.
        For example, \citet{blodgett2016demographic} show that standard tools for parsing tweets do not work well for the African American English dialect.
        Finally, these tools require clean input that has both proper spelling and grammar,
        but social data is notoriously unclean.
        Besides obvious mistakes, tweets often use language in nonstandard ways to create new meaning, for example with emoticons \citep{}.
        Twitter specific tokenizers have been developed, 
        but only for the English language \citep{o2010tweetmotif,gimpel2011part,owoputi2013improved}.

    \item[Many languages use multiple writing systems.]
        The choice of writing system both provides clues about the location of a tweet.
        For example, The small European country of Moldova uses Romanian as its official writing system.
        Romanian was traditionally written with the Cyrillic (Russian) alphabet,
        but after the collapse of the Soviet Union in 1991,
        Moldova officially switched to using the Latin alphabet instead.
        Except for the province of Transnistria which continues to use the Cyrillic alphabet.

        Similarly, the Indonesian family of languages was traditionally written using the \defn{Jawi} alphabet,
        which is a variant of the Arabic alphabet.
        Recently, the countries of Indonesia, Malaysia, and Singapore stopped officially using the Jawi alphabet in favor of the latin-based \defn{Rumi} alphabet.
        Brunei, in contrast, continues to officially use both the Jawi and Rumi alphabets.
        On twitter, approximately \fixme{} tweets in these languages use the Jawi alphabet.

        The choice of alphabet to use can be highly location-indicative.
        \defn{Transliteration} is the process of converting a text written in one writing system to another.
        There is an official transliteration system between the Jawi and Rumi systems,
        but official transliterations are not always available or followed.
        For example, the \defn{Arabizi} writing system has developed in Arabic speaking countries as a way to write arabic on social media platforms like Twitter in Latin script.
        Many users mix Arabizi with standard arabic characters \citep{bies2014transliteration,tobaili2016arabizi,van2016simple}.

        The Japanese language uses three different writing systems.
        For example, the capital city Tokyo may be written in
        Hiragana as \begin{CJK}{UTF8}{min}とうきょう\end{CJK}, 
        Katakana as \begin{CJK}{UTF8}{min}トンキン\end{CJK}, 
        or Kanji as \begin{CJK}{UTF8}{min}東京\end{CJK}.


    \item[Verb conjugations.]
        English verbs are relatively simple.
        For example, the verb \str{talk} has only three forms:
        \str{talk},\str{talks},and \str{talking}.
        Verbs in other languages, however, 
        are much more complicated and exhibit regional variation.
        Consider the following example from Spanish.
        The verb \text{poder} (which means ``to be able to'') is written

        Even English has the \str{Imma} conjugation.

        AAE frequently omits the copula (i.e.\ \str{to be}) \citep{pullum1999african} 

        \citet{tinoco2017variation} use twitter to study the geographic differences of \str{iste} vs \str{istes}.

    \item[Spelling errors.]
    %\item[Phonology influences spelling errors].
        \citep{treiman2000dialect} studies the differences in spelling mistakes between American and British English.
        \citet{ahmed2015lexical} proposes a method for automatically fixing the spelling mistakes inherrent in twitter data,
        but the method is limitted only to English and does not account for dialectical variations.
%
    %\item[Consonant cluster reduction].
        One particular form of  
        For example, the word \str{left} is likely to get simplified to \str{lef} when the subsequent word is a vowel.
        \citet{guy1991contextual,tagliamonte2005new} study this phenomenon in standard English,
        \citet{pullum1999african} argues that this is a particular feature of the African American English dialect,
        and \citet{eisenstein2013phonological} shows that this phenomenon carries over into the context of tweets.

    \item[Relationships between words.]
        In standard German the word \str{radio} is neuter and so is written \str{das radio}.
        In Swiss German, however, the word \str{radio} is masculine and so is written \str{der radio}. \citep{hollenstein2014compilation}

        Compounding.

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage
\section{The model}
\label{sec:model}

\begin{figure*}
    \centering
    \fbox{\parbox{\textwidth}{\vspace{2in}}}
    %\input{fig/model}
    \caption{The general model framework. \fixme{}}
    \label{fig:model}
\end{figure*}

%\begin{table}
    %\centering
    %\input{fig/hyperparam}
    %\caption{The model's hyperparameters.}
    %\label{tab:hyperparam}
%\end{table}

The structure of our model is shown in Figure \ref{fig:model}.
The model is divided into three stages:
feature generation, feature mixing, and output.
The model's output is then compared against the location data in the tweet to measure its efficacy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Feature generation}

Our feature generation procedure is inspired by recent trends in deep learning and natural language processing.
A key idea of recent deep learning methods is that automatically learned features can perform much better than hand-crafted features.
The experiments in Section \ref{sec:experiments} shows how each of these sets of features contributes to the final output.

\begin{description}
\item[Character-level features.]
We use a \defn{convolutional neural network (CNN)} to generate character-level features.
Specifically, our model is a variant of the CLCNN \citep{zhang2015character} which we've modified to work in the multi-language setting.

Alternative character-level models use deeper CNNs with resnet connections \citep{conneau2017very}, recurrent neural networks \citep{chung2016character}, or complex combinations of CNNs and RNNs \citep{kim2016character,jozefowicz2016exploring}.
Each of these techniques requires considerable processing resources, however, 
so we did not have the resources to exhaustively compare these techniques to each other on this task.
We chose to base our model off of the CLCNN model because it had the best performance on a small held-out subset of tweets.
All of these techniques were originally designed to work on relatively small mono-lingual corpuses.
The largest dataset used in the papers above is \fixme{},
whereas our dataset is \fixme{}.

Deep learning techniques have also been applied to Twitter data.
%Character-level models have previously been applied to twitter NLP tasks \citep[e.g.][]{dhingra2016tweet2vec,severyn2015unitn},
%but never to the geolocation problem.
For example, 
\citet{severyn2015unitn} use a word level CNN for sentiment classification on tweets,
and \citet{dhingra2016tweet2vec} use a character level GRU network to predict hashtags.
Ours is the first application of deep learning techniques to the geolocation problem,
uses the largest dataset,
and the first multilingual dataset.
%Their dataset contains only 2 million tweets, whereas our dataset contains \fixme{}.

\item[Word-level features.]
All previous work on twitter geolocation has essentially relied on hand-crafted world-level features.
\citet{han2012geolocation} study a method of determining location indicative words using an information gain strategy.

\item[Time features.]
The twitter API specifies time using the standard Unix Epoch Time 
(i.e. milliseconds since 1 Jan 1970).
From this we generate 

\item[Language features.]
There are two ways to incorporate language of a tweet into our model.
The first is to use the Twitter API's language field directly 1-hot encoded.
This is a simple technique, but has three drawbacks:
(i) the Twitter API supports many fewer languages than are actually used on Twitter;
(ii) the Twitter API occasionally makes mistakes on which language the tweet was sent in;
(iii) Twitter uses an undisclosed algorithm to determine the language,
and this algorithm is likely to leak user-level meta-information.

An alternative is to try to learn the language directly.

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Feature mixing}

We propose a mixing layer that lets the model capture dependencies between the features extracted in the previous stage.

Previous work only considers linear models that do not allow for feature dependencies.
Linear models have the advantage of being convex,
which means that optimizers are guaranteed to find a unique global minumum.
Because these linear models do not capture dependencies between features,
previous work has focused on developing good manual feature extraction techniques that incoporate these dependencies.
\fixme{New York, USA vs. York, England}

Recent advances in machine learning, however, have shown that non-linear models can be efficiently learned.
There is no guarantee that a globally optimal solution will be found;
however, the optimizers have been shown to find good solutions in practice.

Our model uses two fully connected layers with 2048 hidden units each and relu activation functions.
To reduce overfitting, each of these layers is trained using dropout and a 50\% keep probability \citep{}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Prediction losses}

Our general framework has two types of outputs:
discrete outputs that estimate the \place and \country fields of the tweet,
and continuous outputs that estimate the \geo field.
%Whereas previous work requires a number of assumptions about the output fields,
%our work makes few assumptions.

\begin{description}
\item[Discrete outputs.]
The discrete outputs are conceptually the simplest,
and for this reason they are the most commonly used in previous work.
The idea is to treat a tweet's \place and \country fields as labels for the tweet,
then perform ordinary multi-label classification using the cross entropy loss.
Our work differs from previous work because no previous work has attempted to directly predict a tweet's \place field.
Because there are millions of distinct \place entries in twitter,
this task was considered too hard.
Instead, previous work constructed \pseudoplace labels that filter out uncommonly seen entries and merge them into other nearby locations.
This reduces the number of class labels from millions down to thousands,
making a much easier problem.

There is no standard method for constructing these \pseudoplace labels in the literature.
\citet{han2012geolocation} proposed a more sophisticated method.
Their method combines suburbs with nearby cities,
and identifies a total of 3709 cities to use as class labels.
Those tweets that do not originate from a city are either discarded or assigned to the closest city.
More complicated methods of discretizing the Earth's surface have also been developed for non-Twitter applications.
For example, Google developed a large scale system for geolocating images that uses specially designed partition of the earth's surface that they call S2 \citep{weyand2016planet}.

\item[Angular regression.]
We introduce novel methods for predicting the \geo field that take advantage of the non-euclidean nature of the earth's surface.
\citet{duong2016near} is the only previous work to attempt to estimate the \geo field of a tweet.
They use the ordinary least squares regression model with the standard $L2$ loss between GPS coordinates:
\begin{equation}
    \latd^2 + \lond^2
    .
\end{equation}
Unfortunately, the surface of the earth is highly non-euclidean, 
and the OLS model is known to work only in the euclidean setting \citep[e.g.][]{fisher1992regression}.
For example:
\begin{enumerate}
    \item
        There is a difficulty at the antimeridian (180 degrees west).
        The antimeridean approximately separates Russian from Alaska.
        The true distance between these locations is very short (about 100km),
        but the distance using the L2 norm is very large.
    \item
        Second, the standard $L2$ loss does not accurately capture the distance between GPS coordinates.
        For example, at a latitude of 80 degrees north, 1 degree of longitude is approximately equal to 20 km;
        but at the equator, 1 degree of longitude is approximately equal to 111 km.
\end{enumerate}

The so-called \emph{angular regression} fixes these problems \citep{fisher1992regression}.
\citet{fisher1992regression} was the first to propose a method for regression onto the surface of a sphere.

The \emph{great circle distance} is a better distance metric for gps coordinates because it is the distance of the shortest path between two points on the earth's surface.
The naive computation of the great circle distance is unfortunately numerically unstable and cannot be used directly.
Fortunately, \citet{vincenty1975direct} proposed a numerically stable method for computing the GCD which we use in our work.
Figure \ref{fig:vincenty} shows the formula.

\begin{figure*}
    \centering
    %\begin{align}
    $
    \displaystyle
        \delta\sigma 
        =
        \arctan\left(
            \frac
            {\sqrt{(\cos\lata\cdot\sin\lond)^2 + (\cos\lata\cdot\sin\latb-\sin\lata\cdot\cos\latb\cdot\cos\lond)^2}}
            {\sin\lata\cdot\sin\latb + \cos\lata\cdot\cos\latb\cdot\cos\lond}
        \right)
    $
    %\end{align}
    \caption{The Vincenty formula for numerically stable great circle distances.}
    \label{fig:vincenty}
\end{figure*}

\item[Mixture of FvM.]

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage
\section{Experiments}
\label{sec:experiments}

\begin{table*}
\input{img/tables/all}
\end{table*}

\begin{description}
\item[Largescale quantitative experiment.]

\item[Qualitative experiments.]

\item[Language identification experiment.]
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Visualization}

A major disadvantage of CNNs is that they lack interpretability compared to simpler models.
To alleviate this problem, many techniques have been proposed to visualize CNNs in the context of image classification \citep{zeiler2014visualizing,seifert2017visualizations}.
We adapt these methods to provide the first visualization method of CNNs applicable to the text domain.
This visualization technique helps us understand the dialectical patterns that \uniloc~CNN discovers in the text.

The method is simple and inspired by the occlusion method proposed by \citet{zeiler2014visualizing} for visualizing image CNNs.
Given an input text with $n$ characters,
we generate $n-1$ new texts by removing the $i$th character from the original text.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\ignore{
\section{Languages and Day of Week}
\noindent\input{img/day-en}

\noindent\input{img/day-ar}

\noindent\input{img/day-es}

\noindent\input{img/day-fr}

\noindent\input{img/day-in}

\noindent\input{img/day-tl}

\noindent\input{img/day-tr}

\noindent\input{img/day-ja}

\noindent\input{img/day-ko}

\noindent\input{img/day-pt}

\noindent\input{img/day-zh}

\noindent\input{img/day-und}

\noindent\input{img/day-de}

\noindent\input{img/day-nl}

\noindent\input{img/day-cs}

\noindent\input{img/day-da}

\noindent\input{img/day-el}

\noindent\input{img/day-pl}

\noindent\input{img/day-ru}

\noindent\input{img/day-vi}

\noindent\input{img/day-th}
}

\onecolumn
\section{Languages and Time of Day}
\noindent\input{img/hr-ar}

\noindent\input{img/hr-en}

\noindent\input{img/hr-es}

\noindent\input{img/hr-fr}

\noindent\input{img/hr-in}

\noindent\input{img/hr-tl}

\noindent\input{img/hr-tr}

\noindent\input{img/hr-ja}

\noindent\input{img/hr-ko}

\noindent\input{img/hr-pt}

\noindent\input{img/hr-zh}

\noindent\input{img/hr-und}

\noindent\input{img/hr-de}

\noindent\input{img/hr-nl}

\noindent\input{img/hr-cs}

\noindent\input{img/hr-da}

\noindent\input{img/hr-el}

\noindent\input{img/hr-pl}

\noindent\input{img/hr-ru}

\noindent\input{img/hr-vi}

\noindent\input{img/hr-th}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Reference notes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Languages}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Twitter applications}

\citet{cheng2010you} uses a probabilistic framework to generate city-level user home location using only tweet text.
Introduces a naive bayes model, but for some reason uses a sum instead of a product (bad math?).
Shows that this model does poorly,
and introduces a 'local words' weighting of the naive model to improve performance.
Introduces several smoothing mechanisms to help with the sparsity of tweet data.
They only consider tweets in the contiguous US.

\citet{kinsella2011m} predicts the location of individual tweets and a user's home location.
Provides a simple probabilistic model: 
For each location, estimate a distribution of terms associated with that location.
Does not incorporate time.
A straightforward generalization of \citet{cheng2010you} to tweet location instead of user location.

\citet{li2012towards} uses a probabilistic framework to generate city-level location using the content of the tweet and the network of tweet replies.
Does not use a gazetteer or the underlying social graph.
This seems like a straightforward extension of \citet{cheng2010you}.

\citet{han2013stacking} predicts the city of a twitter user using both text and metadeta using stacking.
I believe this is the first paper to consider the effect of semantic shift in geolocation.
They show that user declared location metadeta is more sensitive to temporal change than the message text.

\citet{mahmud2014home} identifies the home location of a user rather than the location of an individual tweet.
Incorporates temporal information in the tweets to identify when a user is travelling.
\fixme{Has good twitter geolocation references that I don't have elsewhere.}
Uses an ensemble classifier with a gazeteer as one of the key features.
Lots of manual constructions and NLP-based preprocessing.
Hierarchical model that first predicts a general geographic region (e.g. timezone or state), then predicts the city.

\citet{han2014text} has many new ideas.
Introduce a multilingual dataset and the first methods for geolocating non-English tweets.
They use a hierarchical model that first determines the language,
then selects a model appropriate for the language.
Make heavy use of twitter metadata (e.g. tweet time) to determine location.
Perform a test on time where they evaluate their model on data collected 1 year after the training data, and show a significant performance loss.
Studies the privacy implications of geolocation.

\citet{compton2014geotagging} propose a simple convex problem for geolocating twitter users to city level accuracy using the social network graph only.
Whereas previous methods rely on local heuristics, their convex program uses global properties of the social graph.
Has lots of empirical results showing accuracy of self reported locations, location homophily among friends, and typical travel habits of twitter users.

\citet{rahimi2015twitter} uses both twitter text and the network graph for geolocation, but does not include time.
\fixme{Has good twitter geolocation references that I don't have elsewhere.}
Good experiments with good datasets.
Uses kd-tree for faster search.
Spatial labels are discretized over an adaptive grid based on the number of users in the region.
The @-mention information is used to build an undirected graph between users.
They convert the graph into a ``collapsed graph'', and there's lots of subtleties here about how they handle the train/test split and edges that pass between the two sets.
Uses Model Adsorption over the graph to predict geolocations within the test set, 
with two key modification:
(i) removing celebrity nodes from the network graph (lets them scale to larger networks)
and (ii) incorporating textual information as ``dongle nodes''.

\citet{dredze2016geolocation} studies time's effect on geolocation of individual tweets to the city level.
Demonstrates cyclical temporal effects on geolocation accuracy and rapid drops in accuracy as test data moves beyond the training data's time period.
They show that this temporal drift can be countered with modest online model updates.
Introduces a particularly large new dataset.
Used vowpal wabbit to learn the model.

\citet{duong2016near} does regression to predict the gps coordinates without taking into account the geometry of the earth.
Uses matrix factorization of a bag-of-words type model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Twitter analytics}

\citet{hecht2011tweets} measures the accuracy of the location field in twitter user profiles.

\citet{dredze2013carmen} introduces the Carmen system for twitter geolocation.
Then use it to improve influenza surveillance.
Carmen does not do prediction of location from text,
but instead only measures accuracy of the various location fields.

\citet{he2015hawkestopic} uses Hawkes process to model the social graph of Twitter,
but does not apply the idea to geolocation.

\citet{graham2014world} survey of geolocation methods for geographers.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Emojis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Generic applications}

\citet{eisenstein2010latent} presents a multilevel generative model that resents jointly about latent topics and geographical regions.
Highly cited, and probably the right foundation for my graphical model.
Does not incorporate time or model the geometry of the regions.
Uses mean field variational inference.
\fixme{Think about this more.}

\citet{speriosu2010connecting} models language and geography outside the Twitter context for toponym resolution (disambiguating place names).
Uses a graphical model based on probabalistic topic models,
where regions of the earth are represented by different topics.
Inference is done with a collapsed Gibbs sampler.
Does not incorporate the Earth's spherical geometry or any distance relations between locations.

\citet{wu2017link} proposes a model for predicting the generation of new links in social networks. 
Uses a convex optimization problem with closed form solution.

\citet{yu2017temporally} uses matrix factorization to predict the formation of new edges in social networks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Less important work}

\citet{leidner2011detecting} provides a tutorial on methods for parsing geographical references in natural language.

\citet{jurgens2015geolocation} surveys existing methods.
They show a large performance gap between real world performance and the idealized laboratory-performance reported in the compared methods' publications.
\fixme{Review their references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Other applications of social network analysis}
%
%\citet{ruiz2012correlating} uses tweets to predict financial markets.
%
%\citet{wiley2014pharmaceutical} uses tweets to measure the effectiveness and user satisfaction of new drugs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Deep learning}
%
%\citet{kim2016character} proposes using character level CNNs, RNNs, and Highway Networks for language translation.
%\citet{chung2016character} also proposes character level RNNs and highway nets for translation.
%\citet{jozefowicz2016exploring} is a generic classification paper for character level text processing.  
%Uses a combination of CNNs and RNNs, and a hierarchical softmax which might be useful for locations.
%
%\citet{dhingra2016tweet2vec} creates a vector space model of tweets using a character level recurrent GRU network.
%\citet{severyn2015unitn} use a word level CNN for sentiment classification on tweets.
%
%\citet{conneau2017very} use resnet like connections to create very deep character cnns for text classification.
%\citet{zhang2015character} use a smaller depth character CNN, which is what I've implemented so far.
%
%\citet{weyand2016planet} do geolocation of photos using image CNNs.
%They divide the globe into 26263 regions, and use a xentropy loss over those regions.
%Regions are of different sizes so that they all contain approximately the same number of photos.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ignore{
\section{Problem Overview}

Previous work focuses on learning a parameterized model for tweet location.
The advantages of these methods are:
\begin{enumerate*}[label=(\arabic*)]
    \item the resulting models are simple, and
    \item the models can be trained and deployed on low-power devices.
\end{enumerate*}
The disadvantages are:
\begin{enumerate}
    \item These methods can model recurring space/time interactions 
        (e.g. patterns caused by timezone differences and weekend/nonweekend behavioral patterns), 
        but they cannot handle one time outlier events such as natural disasters or entertainment events.
    %\item They provide point estimates a tweet's location rather than a distribution of possible locations.
    \item Experiments by \citet{mahmud2014home} and \citet{dredze2016geolocation} show that the models do not generalize well to unseen time periods.
\end{enumerate}
I propose a nonparametric approach to geolocation that should improve these disadvantages.
The tradeoff is that the model is more complex and not deployable on low-power devices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proposed Solution}

Let $\{e_i\}_{i=1}^n$ be the set of observed tweets,
and $e$ be a new tweet not in the database.
Then define the nonparametric distribution over $e$ 
\begin{equation}
    \prob e
    \propto
    %\frac 1 n 
    \sum_{i=1}^n \exp\left(-\dist_\theta(e,e_i)^2\right)
    ,
\end{equation}
where $\dist_\theta$ is a distance metric between two tweets that depends on parameter vector $\theta$.
If we let $e(\ell)$ denote the tweet $e$ updated to have location $\ell$,
then the distribution of the tweet's location is given by
\begin{equation}
    \prob {e(\ell)} 
    \propto
    %\frac 1 n 
    \sum_{i=1}^n \exp\left(-\dist_\theta(e(\ell),e_i)^2\right)
    ,
\end{equation}
and the maximum likelihood point estimate of the tweet's location is
\begin{equation}
    \label{eq:hatell}
    \hat\ell
    =
    \argmax_{\ell} 
    \sum_{i=1}^n \exp\left(-\dist_\theta(e(\ell),e_i)^2\right)
    .
\end{equation}
The quality of the estimate $\hat\ell$ is affected by three factors:
\begin{enumerate}
    \item \emph{The number of tweets in the database.}
        Standard results in nonparametric distributions show that as $n\to\infty$, 
        the distribution $\prob{e}$ will approach the true underlying distribution.
        %Since the underlying distribution is highly complex, 
        %many samples will be needed to get a reasonable approximation.
        %Fortunately, we have many tweets available to us,
%
        The downside of this strategy is that the summations above are summations over the entire database.
        This is not practical, so the summation will need to be approximated.
        There are many good metric data structures that can restrict the summation to only the most relevant portions of the space.

    \item \emph{The family of distance metrics $\dist_\theta$.}
        The distance metric needs to tie information about a tweet's location to information about the other features in a tweet.
        There are many possible metric families,
        and finding the optimal one is likely a difficult challenge.

        A simple approach is to use an ``ensemble of metrics.''
        Let $\{d^{(i)}\}_{i=1}^m$ be a set of $m$ metrics.
        Then define the Mahalanobis distance
        \begin{equation}
            \label{eq:mahalanobis}
            \dist_\theta(e_1,e_2) ^2
            =
            \trans{
            \begin{pmatrix}
                \dist^{(1)}(e_1,e_2) \\
                \dist^{(2)}(e_1,e_2) \\
                \vdots \\
                \dist^{(m)}(e_1,e_2) \\
            \end{pmatrix}
            }
            \theta
            \begin{pmatrix}
                \dist^{(1)}(e_1,e_2) \\
                \dist^{(2)}(e_1,e_2) \\
                \vdots \\
                \dist^{(m)}(e_1,e_2) \\
            \end{pmatrix}
        \end{equation}
        %where $A : \R^{m\times m}$ is the parameter vector $\theta$ that is to be learned.
        where $\theta$ is a $m\times m$ matrix that needs to be estimated from the data.

        The ensemble should include several metrics related to gps coordinates 
        (e.g. the geodesic distance \citep{vincenty1975direct}, the google maps distance),
        time 
        (e.g. total distance in time, and the distance in time mod hourly, daily, weekly, monthly, and yearly intervals),
        social graph distances
        (e.g. total number of hops in a friendship graph),
        and any features of the tweet itself.

        There are two possible drawbacks of a large ensemble.
        First, a larger ensemble will increase the computational burden of computing distances.
        This can possibly be ameliorated by a pruning strategy that only evaluates the more expensive distances after the cheaper ones if it is actually necessary.
        Second, more metrics increases the number of parameters, increasing the possiblity of overfitting.
        The number of available tweets is so large, however, that this seems unlikely for a linear Mahalinobis distance.

        %These ensemble metrics can be extended by:

    \item \emph{The quality of the estimated parameter vector $\theta$.}
        %We can solve for $\hat\theta$ using the formula
        A good parameter vector $\hat\theta$ will maximize the likelihood that
        \begin{equation}
            \label{eq:hattheta}
            \hat\theta
            =
            \argmax_{\theta} 
            \sum_{i=1}^n 
            \sum_{j=1}^{i-1}
            \exp\left(-\dist_\theta(e_i,e_j)^2\right)
            .
        \end{equation}
        This can be solved with an SGD procedure.
        Using only a small sample of the data should be sufficient.
        %For the Mahalanobis metric above,
        %this optimization is convex.
        %In general, however, for other metrics, it need not be.
        %An important decision will be the choice of regularizer for $\theta$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
\section{Research Questions}

\noindent
Applied questions
\begin{enumerate}
    %\item \textbf{How do we efficiently perform the sums?}
    \item Does jointly training the feature space and metric parameters improve performance?
    \item Many possible choices of metric to experiment with to determine which is best for different applications.
        For example, how should we incorporate information related to ``location'' vs ``gps'' fields of the tweet?
    \item Which regularizer should be chosen to optimize $\hat\theta$ in \eqref{eq:hattheta}?
        Is there a better loss function to use?
    \item Queries in this framework can be significantly more complicated:
        \begin{enumerate}
        \item We can easily in incorporate constraints into the queries.
            For example: Given that I know the tweet came from the midwest,
            which city was it most likely to come from?
        \item Given that a tweet came from a particular location,
            what time was it most likely tweeted?
            What user mostly likely sent the tweet?
        \item
            Generate a tweet that is likely to have been sent from location X at time Y.
            (This requires that the metrics in the ensemble have a generative semantics.)
        \end{enumerate}
\end{enumerate}

\noindent
Theoretical questions:
\begin{enumerate}
    \item \emph{Nonconvex optimization.}
        The optimization in \eqref{eq:hatell} is nonconvex.
        Finding a unique global optimum, however, is not likely to be important for practitioners.
        Instead, reporting several representative solutions will likely be more useful.
        These solutions should:
        \begin{enumerate}
            \item not get stuck in ``small'' local optima, and
            \item represent all the largest modes in the distribution.
        \end{enumerate}
        What is the best way to optimize under these constraints?

    \item \emph{Metric ensembles.}
        The theoretical properties of ensembles of metrics like proposed in \eqref{eq:mahalanobis} have not been explicitly studied before.
        \begin{enumerate}
            \item How does the dimension of the ``submetrics'' influence the dimension of the resulting ensembled metric?
                It is likely that each metric will have ``different dimensions at different scales,''
                and how do we incorporate this information to improve runtime bounds on queries?
            \item What are the best ways to ensemble the metrics? 
                \begin{enumerate}
                    \item Other $L_p$ combinations can be used besides $L_2$.
                    \item The matrix $\theta$ could be replaced by a higher order tensor to capture more complicated relationships between the submetrics.
                    \item We can kernelize the Mahalanobis distance to create a nonlinear family of metrics.
                    \item Using parameterized metrics as the base metrics naturally results in a ``deep'' metric learning problem.
                \end{enumerate}
            \item Can we develop metric data structures specifically tuned for ensemble metrics?
                \begin{enumerate}
                    \item Can we efficiently add/delete/modify metrics in the ensemble without recreating the data structure?
                    \item Can we query using only a portion of the metrics in the ensemble?
                    \item Can we support database style queries distributed over multiple machines? 
                \end{enumerate}
        \end{enumerate}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem description}

Existing work on tweet geolocation uses only text features to predict location.
I want to use time as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Intuitive motivation}

Time by itself is a good predictor of tweet location.
People are more likely to tweet during daytime hours,
and daytime is determined by longitude.
Combining time with textual features is even more powerful.

Consider a tweet with the text ``I am at the Taylor Swift concert''
(or alternatively a video of Taylor Swift performing).
If we knew the time of the tweet, 
and we knew Taylor Swift's concert schedule,
then we could get a good prediction of the location of the tweet.
Without knowing the time, however, we cannot say which city the tweet was sent from.
We actually don't even need to know Taylor Swift's concert schedule.
These concerts have many thousands of attendees, many of whom are tweeting.
If any of these attendees tweets about Taylor Swift and has geotagging enabled,
we should be able to use this information to infer that the original tweet was from the same location
(since it happened at the same time).

The example of a Taylor Swift concert happens over a narrow time scale and in a small location.
Other examples of space/time dependencies occur at larger scale.
For example: 
(i) natural disasters such as floods can affect multiple cities over many weeks;
(ii) normal weather conditions (such as snow, rain, or heat) affect latitudinal bands over the course of seasons;
and (iii) elections can affect entire cities, states, and countries for months.
We would like a system that can automatically identify these space/time dependencies and use them to predict tweet location.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection {Formal problem statement}

We call each tweet an event $e$ and decompose it into three components: 
\begin{align*}
    %g    &= \text{the gps coordinates} \\ 
    %\ell &= \text{the location (either the location field in the tweet or exact gps coordinates)} \\
    \ell : \R^2 &= \text{the location represented as gps coordinates} \\
    x    : \R^d &= \text{document features (constructed from any text/images/video/urls in the tweet)} \\
    t    : \R~ &= \text{time}
\end{align*}
%Existing approaches model the probability distribution $\cprob{\ell}{d}$.
%Existing baseline approaches estimate the distribution $\cprob{\ell}{d}$.
Our goal is to estimate the conditional distribution $\cprob{\ell}{x,t}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection {Proposed technique}

I propose a nonparametric model for estimating the density.
Specifically, 
\begin{equation}
    \label{eq:nonparam}
    %\prob{e} = \frac 1 n \sum_{i=1}^n k(e,e_i)
    \prob{e} = \frac 1 n \sum_{i=1}^n \exp(-\dist(e,e_i)^2)
\end{equation}
where $\dist$ is a metric distance that describes the similarity of two tweets.
Learning the model will consist of setting parameters in the distance function.
The main challenge of large scale nonparametric models is that the summation in \eqref{eq:nonparam} is over the entire data set.
A clever metric data structure will be needed so that only a small fraction of the data will need to be searched.

Many metrics can be defined between tweets,
and each metric will determine a corresponding probability distribution.
The simplest metrics for location estimation ignore all the information in the tweets except the location.
The most obvious distance function is to use the great circle distance $\dist\gcirc$ between two gps coordinates.
The obvious formula contains numerical instabilities,
and so Vincenty's formula should be used instead \citep{vincenty1975direct}.
A bandwidth parameter would need to be estimated from the data.
%More powerful distance functions would take into account the rate that information can travel between two locations.
%We can define the google maps distance $\dist\gmap$ to be the length of time that google maps says it takes to drive between two locations.
%This distance is more likely to be too expensive to compute;
%and while it may be more accurate than $\dist\gcirc$,
%it is unlikely to be the best distance measure possible.
%Ideally, we would use a metric learning algorithm to achieve the best possible metric.

I think a learned metric would be a good way to associate textual and spatial features.
If $\Dist$ is a family of metrics and $\loss$ a loss function,
then the training procedure solves
\begin{equation}
    \hat\dist = \argmin_{\dist\in\Dist} \sum_{e_1,e_2\in E} \loss(\dist(e_1,e_2))
    .
\end{equation}
As in the testing procedure, evaluating the sum is computationally expensive and clever data structures are needed to compute it efficiently.
The efficacy of the method will be determined by the choice of distance family $\Dist$.

\ignore{
If the location is independent of the document and time, then $\cprob{\ell}{x,t} = \prob{\ell}$.
This can be modeled as a mixture of $n$ isotropic Gaussians.
That is,
\begin{equation}
    \prob{\ell} = \sum_{i=1}^n w_i\normal{\mu_i}{\eye\sigma_i}
    .
\end{equation}
There are 488 cities in the worldwith at least 1 million people, 
so around 1000 seems like a reasonable choice of $n$.
The parameters $w_i$, $\mu_i$, and $\sigma_i$ must be learned from the data.

Next we incorporate a dependence on time.
Let $k : E \times E \to \R$ be a kernel function that assigns a similarity to two tweets.
As simple gaussian kernel could be
\begin{equation}
    k(e_1,e_2) 
    = \exp(-\tau\abs{e_1(t) - e_2(t)}^2
           -\lambda\ltwo{e_1(\ell) - e_2(\ell)}^2)
\end{equation}
where $\tau$ and $\lambda$ are parameters that control the influence of temporal and spatial distance in the events' similarity.
We can then define the conditional distribution
\begin{equation}
    \cprob{\ell}{t} = p(\ell)\exp(-\sum_{\substack{\text{events $e'$ s.t.}\\e'(t)<t}} k(e,e'))
    .
\end{equation}

Finally, we can incorporate a dependence on the textual features simply by updating the kernel to include a dependence on the text.
}

%A Hawkes process is an appropriate model for $\cprob{\ell}{t}$.
%In a Hawkes process,
%the likelihood of an event at location $\ell$ is high when events have recently happened in nearby locations,
%and low when events have not recently happened nearby.
%Formally,
%\begin{equation}
    %\cprob{\ell}{t} 
    %%\propto
    %=
    %\prob{\ell}\exp(-\lambda(\ell;t))
%\end{equation}
%where $\lambda$ is called the rate function and is given by
%\begin{equation}
    %\label{eq:kernel}
    %\lambda(\ell;t) = \sum_{e} k(\ell,t,e,\theta)
%\end{equation}
%where $k$ is a kernel function that depends on parameter $\theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
}

\ignore{
\subsection{Computational notes}

\begin{enumerate}
    \item
        The summation in \eqref{eq:kernel} is over all tweets,
        which is obviously infeasible.
        Any reasonable choice of kernel will have a large value only for tweets nearby in space/time.
        Therefore distant points can be pruned from the summation to improve computation time.

    \item
        As formulated, the features can be computed completely independently from modeling of the space/time relations.
        It may be possible to improve the performance by jointly optimizing the feature selection algorithms with the space/time objectives.

    \item
        Modeling the locations $\ell$ as points in $\R^2$ induces distortions,
        because the data is actually gps coordinates on a sphere.
        Restricting the data to lie on the sphere would be better,
        but I'm not sure how to enforce that type of constraint.
        It might be easier (and more accurate!) to learn the topology of the underlying space from the data.
        This could help incorporate non-gps location data as well,
        and make the system usable on non-twitter data.
        For example, we could predict the university/funding agency for a piece of research in a citation social network.

    \item
        We should be able to use a semisupervised learning procedure to use all the tweets without location data.

\end{enumerate}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
