%\documentclass{article}
\documentclass[sigconf,10pt]{acmart}

\setcopyright{rightsretained}
%\acmDOI{10.475/123_4}
%\acmISBN{123-4567-24-567/08/06}
\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El Paso, Texas USA}
%\acmYear{1997}
%\copyrightyear{2016}
%\acmArticle{4}
%\acmPrice{15.00}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{xcolor}
\usepackage[inline]{enumitem}

%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
%\usepackage{algorithmicx}
%\usepackage{algorithm2e}

%\usepackage[authordate,bibencoding=auto,strict,backend=biber,natbib]{biblatex-chicago}
%\usepackage[round]{natbib}   % omit 'round' option if you prefer square brackets
%\bibliographystyle{plainnat}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

\usepackage{tikz}
\usetikzlibrary{matrix, positioning, fit}
\usepackage{pgfplots}
\pgfplotsset{width=7cm,compat=1.8}
\definecolor{darkgreen}{RGB}{0,127,0}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{cor}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\vecspan}{span}
\DeclareMathOperator*{\affspan}{aff}
\DeclareMathOperator*{\subG}{subG}
\DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\E}{\mathbb{E}}

\newcommand{\str}[1]{\texttt{#1}}
\newcommand{\defn}[1]{\textit{#1}}

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\trans}[1]{{#1}^{\top}}

\newcommand{\abs}[1]{\lvert{#1}\rvert}
\newcommand{\ltwo}[1]{\lVert {#1} \rVert_2}
\newcommand{\set}{\mathcal}
\renewcommand{\vec}{\mathbf}

\newcommand{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{Z}

\newcommand{\w}{W}
\newcommand{\what}{\hat\w}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{y}

\newcommand{\dist}{d}
\newcommand{\Dist}{\mathcal D}
\newcommand{\kernel}{k}
\newcommand{\loc}{_{\textit{loc}}}
\newcommand{\gmap}{_{\textit{gmap}}}
\newcommand{\gcirc}{_\textit{gcirc}}

\newcommand{\loss}{\ell}
\newcommand{\reg}{r}

%\newcommand{\prob}[1]{\text{Pr}\left({#1}\right)}
\newcommand{\prob}[1]{p\!\left({#1}\right)}
\newcommand{\cprob}[2]{\prob{{#1} | {#2}}}
\newcommand{\normal}[2]{\mathcal{N}({#1},{#2})}
\newcommand{\eye}{I}

%\newcommand{\plots}[1]{}
\newcommand{\plots}[1]{#1}
\newcommand{\ignore}[1]{}
\newcommand{\fixme}[1]{\textcolor{red}{\textbf{FIXME:} {#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\uniloc}{\textsc{UniLoc}}

\newcommand{\tweetdata}[1]{{\texttt{#1}~}}
\newcommand{\place        }{\tweetdata{place}}
\newcommand{\pseudoplace  }{\tweetdata{pseudo-place}}
\renewcommand{\country      }{\tweetdata{country}}
\newcommand{\geo          }{\tweetdata{geo}}

\newcommand{\lata}{\text{lat}_1}
\newcommand{\latb}{\text{lat}_2}
\newcommand{\latd}{(\lata-\latb)}
\newcommand{\lona}{\text{lon}_1}
\newcommand{\lonb}{\text{lon}_2}
\newcommand{\lond}{(\lona-\lonb)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{filecontents}
\begin{filecontents}{main.bib}

@article{guy1991contextual,
  title={Contextual conditioning in variable lexical phonology},
  author={Guy, Gregory R},
  journal={Language variation and change},
  volume={3},
  number={2},
  pages={223--239},
  year={1991},
  publisher={Cambridge University Press}
}

@article{fung1998extracting,
  title={Extracting key terms from Chinese and Japanese texts},
  author={Fung, Pascale},
  journal={Computer Processing of Oriental Languages},
  volume={12},
  number={1},
  pages={99--121},
  year={1998}
}

@article{treiman2000dialect,
  title={Dialect and authography: Some differences between American and British spellers.},
  author={Treiman, Rebecca and Barry, Christopher},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={26},
  number={6},
  pages={1423},
  year={2000},
  publisher={American Psychological Association}
}

@article{tagliamonte2005new,
  title={New perspectives on an ol'variable:(t, d) in British English},
  author={Tagliamonte, Sali and Temple, Rosalind},
  journal={Language Variation and Change},
  volume={17},
  number={3},
  pages={281--302},
  year={2005},
  publisher={Cambridge University Press}
}

@incollection{huyen2008hybrid,
  title={A hybrid approach to word segmentation of Vietnamese texts},
  author={Huyen, Nguyen Thi Minh and Roussanaly, Azim and Vinh, H{\^o} Tuong and others},
  booktitle={Language and Automata Theory and Applications},
  pages={240--249},
  year={2008},
  publisher={Springer}
}

@inproceedings{hays2008im2gps,
  title={IM2GPS: estimating geographic information from a single image},
  author={Hays, James and Efros, Alexei A},
  booktitle={Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on},
  pages={1--8},
  year={2008},
  organization={IEEE}
}

@inproceedings{crandall2009mapping,
  title={Mapping the world's photos},
  author={Crandall, David J and Backstrom, Lars and Huttenlocher, Daniel and Kleinberg, Jon},
  booktitle={Proceedings of the 18th international conference on World wide web},
  pages={761--770},
  year={2009},
  organization={ACM}
}

@inproceedings{sakaki2010earthquake,
  title={Earthquake shakes Twitter users: real-time event detection by social sensors},
  author={Sakaki, Takeshi and Okazaki, Makoto and Matsuo, Yutaka},
  booktitle={Proceedings of the 19th international conference on World wide web},
  pages={851--860},
  year={2010},
  organization={ACM}
}

@inproceedings{speriosu2010connecting,
  title={Connecting language and geography with region-topic models},
  author={Speriosu, Michael and Brown, Travis and Moon, Taesun and Baldridge, Jason and Erk, Katrin},
  year={2010}
}

@inproceedings{cheng2010you,
  title={You are where you tweet: a content-based approach to geo-locating twitter users},
  author={Cheng, Zhiyuan and Caverlee, James and Lee, Kyumin},
  booktitle={Proceedings of the 19th ACM international conference on Information and knowledge management},
  pages={759--768},
  year={2010},
  organization={ACM}
}

@inproceedings{eisenstein2010latent,
  title={A latent variable model for geographic lexical variation},
  author={Eisenstein, Jacob and O'Connor, Brendan and Smith, Noah A and Xing, Eric P},
  booktitle={Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
  pages={1277--1287},
  year={2010},
  organization={Association for Computational Linguistics}
}

@article{conover2011political,
  title={Political polarization on twitter.},
  author={Conover, Michael and Ratkiewicz, Jacob and Francisco, Matthew R and Gon{\c{c}}alves, Bruno},
  year={2011}
}

@inproceedings{kinsella2011m,
  title={I'm eating a sandwich in Glasgow: modeling locations with tweets},
  author={Kinsella, Sheila and Murdock, Vanessa and O'Hare, Neil},
  booktitle={Proceedings of the 3rd international workshop on Search and mining user-generated contents},
  pages={61--68},
  year={2011},
  organization={ACM}
}

@article{leidner2011detecting,
  title={Detecting geographical references in the form of place names and associated spatial natural language},
  author={Leidner, Jochen L and Lieberman, Michael D},
  journal={SIGSPATIAL Special},
  volume={3},
  number={2},
  pages={5--11},
  year={2011},
  publisher={ACM}
}

@inproceedings{hecht2011tweets,
  title={Tweets from Justin Bieber's heart: the dynamics of the location field in user profiles},
  author={Hecht, Brent and Hong, Lichan and Suh, Bongwon and Chi, Ed H},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={237--246},
  year={2011},
  organization={ACM}
}

@inproceedings{klein2012detection,
  title={Detection and extracting of emergency knowledge from twitter streams},
  author={Klein, Bernhard and Laiseca, Xabier and Casado-Mansilla, Diego and L{\'o}pez-de-Ipi{\~n}a, Diego and Nespral, Alejandro Prada},
  booktitle={International Conference on Ubiquitous Computing and Ambient Intelligence},
  pages={462--469},
  year={2012},
  organization={Springer}
}

@inproceedings{lui2012langid,
  title={langid. py: An off-the-shelf language identification tool},
  author={Lui, Marco and Baldwin, Timothy},
  booktitle={Proceedings of the ACL 2012 system demonstrations},
  pages={25--30},
  year={2012},
  organization={Association for Computational Linguistics}
}

@article{han2012geolocation,
  title={Geolocation prediction in social media data by finding location indicative words},
  author={Han, Bo and Cook, Paul and Baldwin, Timothy},
  journal={Proceedings of COLING 2012},
  pages={1045--1062},
  year={2012}
}

@inproceedings{li2012towards,
  title={Towards social user profiling: unified and discriminative influence model for inferring home locations},
  author={Li, Rui and Wang, Shengjie and Deng, Hongbo and Wang, Rui and Chang, Kevin Chen-Chuan},
  booktitle={Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={1023--1031},
  year={2012},
  organization={ACM}
}

@inproceedings{ruiz2012correlating,
  title={Correlating financial time series with micro-blogging activity},
  author={Ruiz, Eduardo J and Hristidis, Vagelis and Castillo, Carlos and Gionis, Aristides and Jaimes, Alejandro},
  booktitle={Proceedings of the fifth ACM international conference on Web search and data mining},
  pages={513--522},
  year={2012},
  organization={ACM}
}

@inproceedings{eisenstein2013phonological,
  title={Phonological factors in social media writing},
  author={Eisenstein, Jacob},
  booktitle={Proceedings of the Workshop on Language Analysis in Social Media},
  pages={11--19},
  year={2013}
}

@inproceedings{schulz2013multi,
  title={A Multi-Indicator Approach for Geolocalization of Tweets.},
  author={Schulz, Axel and Hadjakos, Aristotelis and Paulheim, Heiko and Nachtwey, Johannes},
  year={2013}
}

@inproceedings{dredze2013carmen,
  title={Carmen: A twitter geolocation system with applications to public health},
  author={Dredze, Mark and Paul, Michael J and Bergsma, Shane and Tran, Hieu},
  year={2013}
}

@inproceedings{han2013stacking,
  title={A Stacking-based Approach to Twitter User Geolocation Prediction.},
  author={Han, Bo and Cook, Paul and Baldwin, Timothy},
  year={2013}
}

@article{gonccalves2014crowdsourcing,
  title={Crowdsourcing dialect characterization through Twitter},
  author={Gon{\c{c}}alves, Bruno and S{\'a}nchez, David},
  journal={PloS one},
  volume={9},
  number={11},
  pages={e112074},
  year={2014},
  publisher={Public Library of Science}
}

@techreport{antenucci2014using,
  title={Using social media to measure labor market flows},
  author={Antenucci, Dolan and Cafarella, Michael and Levenstein, Margaret and R{\'e}, Christopher and Shapiro, Matthew D},
  year={2014},
  institution={National Bureau of Economic Research}
}

@article{widener2014using,
  title={Using geolocated Twitter data to monitor the prevalence of healthy and unhealthy food references across the US},
  author={Widener, Michael J and Li, Wenwen},
  journal={Applied Geography},
  volume={54},
  pages={189--197},
  year={2014},
  publisher={Elsevier}
}

@article{barbera2014birds,
  title={Birds of the same feather tweet together: Bayesian ideal point estimation using Twitter data},
  author={Barber{\'a}, Pablo},
  journal={Political Analysis},
  volume={23},
  number={1},
  pages={76--91},
  year={2014},
  publisher={Oxford University Press}
}

@article{paul2014twitter,
  title={Twitter improves influenza forecasting},
  author={Paul, Michael J and Dredze, Mark and Broniatowski, David},
  journal={PLoS currents},
  volume={6},
  publisher={Public Library of Science}
}

@inproceedings{hollenstein2014compilation,
  title={Compilation of a Swiss German dialect corpus and its application to PoS tagging},
  author={Hollenstein, Nora and Aepli, No{\"e}mi},
  booktitle={Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects},
  pages={85--94},
  year={2014}
}

@inproceedings{refaee2014arabic,
  title={An Arabic Twitter Corpus for Subjectivity and Sentiment Analysis.},
  author={Refaee, Eshrag and Rieser, Verena},
  booktitle={LREC},
  pages={2268--2273},
  year={2014}
}

@article{zaidan2014arabic,
  title={Arabic dialect identification},
  author={Zaidan, Omar F and Callison-Burch, Chris},
  journal={Computational Linguistics},
  volume={40},
  number={1},
  pages={171--202},
  year={2014},
  publisher={MIT Press}
}

@inproceedings{compton2014geotagging,
  title={Geotagging one hundred million twitter accounts with total variation minimization},
  author={Compton, Ryan and Jurgens, David and Allen, David},
  booktitle={Big Data (Big Data), 2014 IEEE International Conference on},
  pages={393--401},
  year={2014},
  organization={IEEE}
}

@article{graham2014world,
  title={Where in the world are you? Geolocation and language identification in Twitter},
  author={Graham, Mark and Hale, Scott A and Gaffney, Devin},
  journal={The Professional Geographer},
  volume={66},
  number={4},
  pages={568--578},
  year={2014},
  publisher={Taylor \& Francis}
}

@article{mahmud2014home,
  title={Home location identification of twitter users},
  author={Mahmud, Jalal and Nichols, Jeffrey and Drews, Clemens},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={5},
  number={3},
  pages={47},
  year={2014},
  publisher={ACM}
}

@article{han2014text,
  title={Text-based twitter user geolocation prediction},
  author={Han, Bo and Cook, Paul and Baldwin, Timothy},
  journal={Journal of Artificial Intelligence Research},
  volume={49},
  pages={451--500},
  year={2014}
}

@article{wiley2014pharmaceutical,
  title={Pharmaceutical drugs chatter on online social networks},
  author={Wiley, Matthew T and Jin, Canghong and Hristidis, Vagelis and Esterling, Kevin M},
  journal={Journal of biomedical informatics},
  volume={49},
  pages={245--254},
  year={2014},
  publisher={Elsevier}
}

@article{wang2015transfer,
  title={Transfer learning for speech and language processing},
  author={Wang, Dong and Zheng, Thomas Fang},
  journal={arXiv preprint arXiv:1511.06066},
  year={2015}
}

@article{llorente2015social,
  title={Social media fingerprints of unemployment},
  author={Llorente, Alejandro and Garcia-Herranz, Manuel and Cebrian, Manuel and Moro, Esteban},
  journal={PloS one},
  volume={10},
  number={5},
  pages={e0128692},
  year={2015},
  publisher={Public Library of Science}
}

@article{santillana2015combining,
  title={Combining search, social media, and traditional data sources to improve influenza surveillance},
  author={Santillana, Mauricio and Nguyen, Andr{\'e} T and Dredze, Mark and Paul, Michael J and Nsoesie, Elaine O and Brownstein, John S},
  journal={PLoS computational biology},
  volume={11},
  number={10},
  pages={e1004513},
  year={2015},
  publisher={Public Library of Science}
}

@inproceedings{ahmed2015lexical,
  title={Lexical normalisation of twitter data},
  author={Ahmed, Bilal},
  booktitle={Science and Information Conference (SAI), 2015},
  pages={326--328},
  year={2015},
  organization={IEEE}
}

@inproceedings{he2015hawkestopic,
  title={Hawkestopic: A joint model for network inference and topic modeling from text-based cascades},
  author={He, Xinran and Rekatsinas, Theodoros and Foulds, James and Getoor, Lise and Liu, Yan},
  booktitle={International Conference on Machine Learning},
  pages={871--880},
  year={2015}
}

@article{rahimi2015twitter,
  title={Twitter user geolocation using a unified text and network prediction model},
  author={Rahimi, Afshin and Cohn, Trevor and Baldwin, Timothy},
  journal={arXiv preprint arXiv:1506.08259},
  year={2015}
}

@article{jurgens2015geolocation,
  title={Geolocation Prediction in Twitter Using Social Networks: A Critical Analysis and Review of Current Practice.},
  author={Jurgens, David and Finethy, Tyler and McCorriston, James and Xu, Yi Tian and Ruths, Derek},
  year={2015}
}

@article{huang2016understanding,
  title={Understanding US regional linguistic variation with Twitter data analysis},
  author={Huang, Yuan and Guo, Diansheng and Kasakoff, Alice and Grieve, Jack},
  journal={Computers, Environment and Urban Systems},
  volume={59},
  pages={244--255},
  year={2016},
  publisher={Elsevier}
}

@article{nguyen2016leveraging,
  title={Leveraging geotagged Twitter data to examine neighborhood happiness, diet, and physical activity},
  author={Nguyen, Quynh C and Kath, Suraj and Meng, Hsien-Wen and Li, Dapeng and Smith, Ken R and VanDerslice, James A and Wen, Ming and Li, Feifei},
  journal={Applied Geography},
  volume={73},
  pages={77--88},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{weyand2016planet,
  title={Planet-photo geolocation with convolutional neural networks},
  author={Weyand, Tobias and Kostrikov, Ilya and Philbin, James},
  booktitle={European Conference on Computer Vision},
  pages={37--55},
  year={2016},
  organization={Springer}
}

@article{pohl2016online,
  title={Online indexing and clustering of social media data for emergency management},
  author={Pohl, Daniela and Bouchachia, Abdelhamid and Hellwagner, Hermann},
  journal={Neurocomputing},
  volume={172},
  pages={168--179},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{rudra2016summarizing,
  title={Summarizing situational tweets in crisis scenario},
  author={Rudra, Koustav and Banerjee, Siddhartha and Ganguly, Niloy and Goyal, Pawan and Imran, Muhammad and Mitra, Prasenjit},
  booktitle={Proceedings of the 27th ACM Conference on Hypertext and Social Media},
  pages={137--147},
  year={2016},
  organization={ACM}
}

@article{imran2016twitter,
  title={Twitter as a lifeline: Human-annotated twitter corpora for NLP of crisis-related messages},
  author={Imran, Muhammad and Mitra, Prasenjit and Castillo, Carlos},
  journal={arXiv preprint arXiv:1605.05894},
  year={2016}
}

@article{blodgett2016demographic,
  title={Demographic dialectal variation in social media: A case study of African-American English},
  author={Blodgett, Su Lin and Green, Lisa and O'Connor, Brendan},
  journal={arXiv preprint arXiv:1608.08868},
  year={2016}
}

@article{mohammad2016translation,
  title={How translation alters sentiment},
  author={Mohammad, Saif M and Salameh, Mohammad and Kiritchenko, Svetlana},
  journal={Journal of Artificial Intelligence Research},
  volume={55},
  pages={95--130},
  year={2016}
}

@inproceedings{duong2016near,
  title={Near real-time geolocation prediction in twitter streams via matrix factorization based regression},
  author={Duong-Trung, Nghia and Schilling, Nicolas and Schmidt-Thieme, Lars},
  booktitle={Proceedings of the 25th ACM international on conference on information and knowledge management},
  pages={1973--1976},
  year={2016},
  organization={ACM}
}

@inproceedings{lu2016learning,
  title={Learning from the ubiquitous language: an empirical analysis of emoji usage of smartphone users},
  author={Lu, Xuan and Ai, Wei and Liu, Xuanzhe and Li, Qian and Wang, Ning and Huang, Gang and Mei, Qiaozhu},
  booktitle={Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
  pages={770--780},
  year={2016},
  organization={ACM}
}

@article{gonccalves2017fall,
  title={The fall of the empire: The americanization of English},
  author={Gon{\c{c}}alves, Bruno and Loureiro-Porto, Luc{\'\i}a and Ramasco, Jos{\'e} J and S{\'a}nchez, David},
  journal={arXiv preprint arXiv:1707.00781},
  year={2017}
}

@article{nguyen2017kernel,
  title={A kernel independence test for geographical language variation},
  author={Nguyen, Dong and Eisenstein, Jacob},
  journal={Computational Linguistics},
  volume={43},
  number={3},
  pages={567--592},
  year={2017},
  publisher={MIT Press}
}

@article{tinoco2017variation,
  title={Variation of the second person singular of the simple past tense in twitter: hiciste vs. hicistes" you did"},
  author={Tinoco, Antonio Ruiz},
  journal={Dialectologia: revista electr{\`o}nica},
  pages={149--167},
  year={2017}
}

@article{wu2017link,
    author={Yu, Wenchao and ... },
    title={Link Prediction with Spatial and Temporal Consistency in Dynamic Networks},
    journal={IJCAI},
    year={2017}
}

@inproceedings{yu2017temporally,
  title={Temporally Factorized Network Modeling for Evolutionary Network Analysis},
  author={Yu, Wenchao and Aggarwal, Charu C and Wang, Wei},
  booktitle={Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
  pages={455--464},
  year={2017},
  organization={ACM}
}

@article{howard2018fine,
  title={Fine-tuned Language Models for Text Classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{park2013emoticon,
  title={Emoticon Style: Interpreting Differences in Emoticons Across Cultures.},
  author={Park, Jaram and Barash, Vladimir and Fink, Clay and Cha, Meeyoung},
  year={2013}
}

@inproceedings{dredze2016geolocation,
  title={Geolocation for Twitter: Timing Matters.},
  author={Dredze, Mark and Osborne, Miles and Kambadur, Prabhanjan},
  year={2016}
}

@inproceedings{ljubevsic2016global,
  title={A global analysis of emoji usage},
  author={Ljube{\v{s}}i{\'c}, Nikola and Fi{\v{s}}er, Darja},
  booktitle={Proceedings of the 10th Web as Corpus Workshop},
  pages={82--89},
  year={2016}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{vincenty1975direct,
  title={Direct and inverse solutions of geodesics on the ellipsoid with application of nested equations},
  author={Vincenty, Thaddeus},
  journal={Survey review},
  volume={23},
  number={176},
  pages={88--93},
  year={1975},
  publisher={Taylor \& Francis}
}

@article{fisher1992regression,
  title={Regression models for an angular response},
  author={Fisher, Nicholas I and Lee, Alan J},
  journal={Biometrics},
  pages={665--677},
  year={1992},
  publisher={JSTOR}
}

@inproceedings{severyn2015unitn,
  title={Unitn: Training deep convolutional neural network for twitter sentiment classification},
  author={Severyn, Aliaksei and Moschitti, Alessandro},
  booktitle={Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015)},
  pages={464--469},
  year={2015}
}

@inproceedings{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  booktitle={Advances in neural information processing systems},
  pages={649--657},
  year={2015}
}

@article{jozefowicz2016exploring,
  title={Exploring the limits of language modeling},
  author={Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  journal={arXiv preprint arXiv:1602.02410},
  year={2016}
}

@article{dhingra2016tweet2vec,
  title={Tweet2vec: Character-based distributed representations for social media},
  author={Dhingra, Bhuwan and Zhou, Zhong and Fitzpatrick, Dylan and Muehl, Michael and Cohen, William W},
  journal={arXiv preprint arXiv:1605.03481},
  year={2016}
}

@inproceedings{kim2016character,
  title={Character-Aware Neural Language Models.},
  author={Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M},
  year={2016}
}

@article{chung2016character,
  title={A character-level decoder without explicit segmentation for neural machine translation},
  author={Chung, Junyoung and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1603.06147},
  year={2016}
}

@inproceedings{conneau2017very,
  title={Very deep convolutional networks for text classification},
  author={Conneau, Alexis and Schwenk, Holger and Barrault, Lo{\"\i}c and Lecun, Yann},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
  volume={1},
  pages={1107--1116},
  year={2017}
}
\end{filecontents}
%\immediate\write18{bibtex main}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Geolocating Tweets in any Language at any Location}
%\titlenote{Produces the permission block, and copyright information}
%\subtitle{}
%\subtitlenote{The full version of the author's guide is available as \texttt{acmart.pdf} document}

\author{Anonymous Authors}
\affiliation{}
\email{}

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{shortauthors}

\begin{document}

\begin{abstract}
    We consider the problem of determining the GPS coordinates that a tweet was sent from based only on its message contents.
    Our method works on tweets sent in any language,
    and our evaluation dataset contains \fixme{} tweets in more than \fixme{} languages from around the world.
    This dataset contains the largest number of languages used for any currently published natural language processing task.
    We achieve high accuracy across many languages through a novel character embedding that lets us develop a convolutional neural network that shares information between the languages.
    The output layer of our network exploits the non-Euclidean properties of the Earth's surface to accurately predict GPS locations.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
    \fixme{}
\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}


\keywords{\fixme{}}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Twitter is one of the Internet's largest social networks,
with approximately 500 million tweets sent everyday from all over the world \citep{}.
In this paper, our goal is to use linguistic clues present in the tweet's text to determine where in the world the tweet was sent from.
For example, consider the following Spanish tweet:

\noindent
\resizebox{0.485\textwidth}{!}{\includegraphics{img/infer/2-country_xentropy}}%
\footnote{Translation: ``What a great morning... exciting.  You can see the video below the post.''}

\noindent
A linguist could easily determine that this tweet was sent from Spain.
The fact that the tweet is in Spanish narrows the likely sources to 
Spanish speaking countries (e.g. Spain, Mexico, Argentina) 
or countries with a large Spanish minority (e.g. USA, Philippines).
The clue that this tweet was sent from Spain, however, is the word \str{pod\'eis}.
This word is a conjugation of the verb \str{poder} in the so-called \defn{vosotros} form.
The vosotros verb conjugations are only used in the Castilian Spanish dialect used in Spain;
all other dialects use the so-called \defn{ustedes} form (which would conjugate as \str{pueden}).
Our system \uniloc~ correctly predicts that this tweet comes from Spain,
and creates a detailed probability distribution over the GPS coordinates in Spain the tweet is likely to have been sent from (see Figure \ref{fig:poder}).
\uniloc was given no prior knowledge of these verb conjugations or geographic patterns,
and learned this relationship completely from scratch.

\begin{figure}
    \resizebox{0.485\textwidth}{!}{\includegraphics{img/infer/2}}
    \caption{}
    \label{fig:poder}
\end{figure}

%Our system UniLoc was able to correctly determine that this tweet was sent from Spain by taking advantage of dialectical differences between Spanish as spoken in Spain and Latin
%This tweet was sent from Madrid, Spain, and our system UniLoc was able to correctly determine this.

\noindent
\textbf{Why care about geolocation?}
Many important applications rely on geotagged tweets.
For example, geotagged tweets have been used to map the spread of influenza \citep{paul2014twitter,santillana2015combining},
for measuring the impact of earthquakes \citep{sakaki2010earthquake},
for coordinating emergency services \citep{klein2012detection,imran2016twitter,rudra2016summarizing,pohl2016online},
for measuring the spread of political opinions \citep{conover2011political,barbera2014birds},
for comparing dietary habits in different locations \citep{widener2014using},
for measuring neighborhood happiness levels \citep{nguyen2016leveraging}, 
and for measuring unemployment rates \citep{antenucci2014using,llorente2015social}.
Unfortunately for these applications, only about 1\% of tweets are geotagged by their users.
A system that could predict the location of the other 99\% of tweets could be used to improve the accuracy of all these tasks in a semisupervised manner.

Developing a good geotagging language model, in particular, has important applications.
Previous work has used geotagged language models to map the dialects of American English \citep{huang2016understanding,gonccalves2017fall} and Spanish \citep{gonccalves2014crowdsourcing}.
Furthermore, because our language model combines so many different languages,
and can form the basis for multilingual classification tasks using transfer learning \citep{wang2015transfer,howard2018fine}.

\noindent
\textbf{Prior work on geolocation}.
A large body of work has formed around the problem of geolocating tweets.

\citet{hays2008im2gps} performs image geolocation down to the gps coordinate level using $k$-nearest neighbor queries and then builds a kernel density estimate of the distribution.
\citet{crandall2009mapping} uses a classification strategy.
\citet{weyand2016planet} discretizes the world in a semantically meaningful method.

\citet{nguyen2017kernel} use a RKHS method for geolocation.

Most solutions for geolocating tweets work only on a small subset of tweets that are particularly easy to geolocate.
The earliest methods work for English language tweets in the United States \citep{},
and state-of-the-art methods generalize to English tweets from major cities \citep{}.
These methods all take advantage of the fact that certain English words are highly correlated with certain regions.
\fixme{For example, the word \str{Chargers} is highly correlated with tweets from San Diego, CA,
because the Chargers football team is located there.}
These existing methods do not work for most tweets, however,
because most tweets are neither located in the United States nor in English (see Figure \ref{fig:country/lang}).

\begin{figure}
    \resizebox{0.225\textwidth}{!}{\input{img/country}}
    \resizebox{0.225\textwidth}{!}{\input{img/lang}}
    \caption{Most tweets are neither located in the United States nor in English,
    but prior work focuses on these two special cases.}
    \label{fig:country/lang}
\end{figure}

\noindent
\textbf{Prior work on multilanguage models.}

\noindent
\textbf{Our contributions.}
Our contributions include:
\begin{enumerate}
    \item
        Our method works with all tweets, in all languages.
        Twitter has official support for XXX languages,
        but many other languages are represented as well.
        For example, nearly-dead languages like Latin.
        The pope has an official latin-only twitter account \str{@Pontifex\_ln}.
        The esperanto language is not officially supported, but has an active twitter community.
        And web aps will automatically translate a text into Klingon\footnote{The ap appears to have been taken offline, but a news article is available at https://www.tomsguide.com/us/Star-Trek-Online-Klingon-Translator,news-5294.html} and tweet it.
    \item
        Our method requires no ad-hoc preprocessing of the data,
        and in particular can achieve good results with only a single pass over the dataset.
    \item
        A new method for generating features at the character level.
    \item
        Our method can output gps coordinates and take advantage of the non-Euclidean geometry of the Earth's surface.
    \item
        A general framework that lets us easily mix and match the contributions from previous works.
    \item
        An online learning framework that is more natural for twitter than the batch frameworks previously proposed.
    \item
        \fixme{todo}
        A method for visualizing the important features that causes a tweet to be located in a particular location.
\end{enumerate}

Predicting the location of a tweet has many uses.
\citet{blodgett2016demographic} use tweet geolocation data to study African-American English dialects.
By predicting the location of tweets, potentially many more tweets could be used in their datasets.

The \str{lang} field of a tweet is determined automatically by the twitter software using a combination of unpublished tweet and user level features.
\citet{blodgett2016demographic} show that existing language prediction tools such as \str{lang.py} \citep{lui2012langid} perform poorly relative to the \str{lang} field of the json object.
This effect is exacerbated when the tweets exhibit dialectical differences from the formal version of their language.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Identifying tweet location vs user location}
%
%In this paper, we focus on the task of identifying tweet location, rather than user location.
%Identifying the location of a tweet is the harder problem because there is less information available.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problems with word-level features}

%\begin{description}
    %\item[English]. 
        %Example of \str{York} vs \str{New York}.  
        %Spelling patterns like \str{color} vs \str{colour} and \str{theater} vs \str{theatre}.
    %\item[German]. Lots of compound words.
    %\item[Japanese]. Does not use spaces to differentiate between words.  Uses three different alphabets.
    %\item[Spanish]. Verb conjugations for informal 2nd person are different in different countries.
    %\item[Arabic]. Conjugations differ.
%
%\end{description}

Traditional text mining generates word level features in a three step process:
First, the input text is tokenized.
Second, stemming and lemmatization tools are used to standardize the tokens in the corups.
Finally, a feature vector is created using simple summary statistics such as $n$-grams or TF-IDF \citep{}.
This process works well for English-only text corpora,
but is essentially impossible in the highly multi-lingual corpus we consider.
In this section, we highlight four problems with these word-level features.

\begin{description}[style=unboxed,leftmargin=0cm]
    \item[Tokenization in multilingual corpora is difficult.]
        Tokenization of English text is easy because spaces are used to separate words.
        Other languages, however, do not have similar cues to separate words.
        For example, Chinese and Japanese do not use spaces at all,
        and Vietnamese traditionally uses spaces between syllables even within words.
        Good tokenizers for these languages have existed for decades \citep[e.g.][]{fung1998extracting,huyen2008hybrid},
        however these tokenizers have four limitations that make them unsuitable for our task:
        First, they require that the text's language be known a priori so that an appropriate tokenization routine can be called.
        The Twitter API has a \str{lang} field associated with each tweet,
        however the identified language is often wrong.
        Second, they cannot be used simultaneously on the same text.
        Many tweets are written in multiple languages simultaneously,
        and no existing tokenizers offer this support.
        Third, existing tokenizers do not consider dialectical differences in their target language which may be of importance in geolocating.
        Finally, they require clean input that has both proper spelling and grammar,
        but social data is notoriously unclean.

    \item[Verb conjugations].
        English verbs are relatively simple.
        For example, the verb \str{talk} has only three forms:
        \str{talk},\str{talks},and \str{talking}.
        Verbs in other languages, however, 
        are much more complicated and exhibit regional variation.
        Consider the following example from Spanish.
        The verb \text{poder} (which means ``to be able to'') is written

        \citet{tinoco2017variation} use twitter to study the geographic differences of \str{iste} vs \str{istes}.

    \item[Languages with multiple writing systems.]
        The Malay language was traditionally written using the Arabic alphabet.
        Recently, however, the Latin alphabet has become more common.
        On twitter, approximately \fixme{} tweets in Malay use the Arabic alphabet.

    \item[Spelling errors.]
    %\item[Phonology influences spelling errors].
        \citep{treiman2000dialect} studies the differences in spelling mistakes between American and British English.
        \citet{ahmed2015lexical} proposes a method for automatically fixing the spelling mistakes inherrent in twitter data,
        but the method is limitted only to English and does not account for dialectical variations.
%
    %\item[Consonant cluster reduction].
For example, the word \str{left} is likely to get simplified to \str{lef} when the subsequent word is a vowel.
\citet{guy1991contextual,tagliamonte2005new} study this phenomenon in standard english,
and \citet{eisenstein2013phonological} shows that this phenomenon carries over into the context of tweets.

    \item[Relationships between words.]
        In standard German the word \str{radio} is neuter and so is written \str{das radio}.
        In Swiss German, however, the word \str{radio} is masculine and so is written \str{der radio}. \citep{hollenstein2014compilation}

        Compounding.

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The prediction models}

\begin{figure*}
    \centering
    \input{fig/model}
    \caption{The general model framework. \fixme{}}
    \label{fig:model}
\end{figure*}

\begin{table}
    \centering
    \input{fig/hyperparam}
    \caption{The model's hyperparameters.}
    \label{tab:hyperparam}
\end{table}

The structure of our model is shown in Figure \ref{fig:model},
and Table \ref{tab:hyperparam} shows the model's hyperparameters. 
We use the tweet content (ignoring metadata) as input into the model.
Then we divide the model itself into three stages:
feature generation, feature mixing, and output.
The model's output is then compared against the location data in the tweet to measure its efficacy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Twitter data}

The Twitter API associates a JSON object with each tweet,
and this JSON object contains many details about the tweet.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Metadata}

We do not consider the metadata associated with a tweet. 
Because this metadata is easy to collect,
it is easy to add to other models \citep{han2014text}.

\begin{description}
    \item[user generated metadata] 
        There are too many types of user generated metadata to discuss them all in detail,
        so we focus our discussion on the user's location and timezone fields.

        \citet{schulz2013multi} uses user specified location for prediction.
        \citet{hecht2011tweets} studies accuracy of user specified location field.

        Many tweets have an associated timezone.
        Some previous work \citep{schulz2013multi,han2014text} has considered the timezone to be part of the tweet's contents and used this for prediction,
        however we consider this to be a mistake.
        The timezone provides a surprisingly large amount of detail about a tweet's location.
        Knowledge of the timezone transforms the problem into only predicting latitude.
        For example, essentially all tweets in Hawaiian Standard Time fit within a small 100km radius, which means the text doesn't even have to be examined.
        Similarly, Korea and Japan have essentially the same longitude, and so use the same time.
        But they have different timezones (denoted Korea Standard Time and Japan Standard Time).
        Knowledge of the timezone in these cases is sufficient to completely determine the country of origin.
        
    \item[previous tweets]
    \item[social graph]
\end{description}

Introduce a multilingual dataset and the first methods for geolocating non-English tweets.
They use a hierarchical model that first determines the language,
then selects a model appropriate for the language.
Make heavy use of twitter metadata (e.g. tweet time) to determine location.
Perform a test on time where they evaluate their model on data collected 1 year after the training data, and show a significant performance loss.
Studies the privacy implications of geolocation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Feature generation}

Our feature generation procedure is inspired by recent trends in deep learning and natural language processing.
A key idea of recent deep learning methods is that automatically learned features can perform much better than hand-crafted features.

\begin{description}
    \item[hand-crafted, word-level features]
        All previous work on twitter geolocation has essentially relied on hand-crafted features.

        \citet{han2012geolocation} study a method of determining location indicative words using an information gain strategy.


    \item[automatically generarated, character-level features]
        We use a \defn{convolutional neural network (CNN)} to generate character-level features.
        Character-level models have previously been applied to twitter NLP tasks \citep[e.g.][]{dhingra2016tweet2vec,severyn2015unitn},
        but never to the geolocation problem.
        %\citet{dhingra2016tweet2vec} creates a vector space model of tweets using a character level recurrent GRU network.
        %\citet{severyn2015unitn} use a word level CNN for sentiment classification on tweets.

        Specifically, our model is a variant of the CLCNN \citep{zhang2015character} which we've modified to work in the multi-language setting.
        Alternative character-level models use deeper CNNs with resnet connections \citep{conneau2017very}, recurrent neural networks \citep{chung2016character}, or complex combinations of CNNs and RNNs \citep{kim2016character,jozefowicz2016exploring}.
        Each of these techniques requires considerable processing resources, however, 
        so we did not have the resources to exhaustively compare these techniques to each other on this task.
        We chose to base our model off of the CLCNN model because it has the best performance on a small held-out subset of tweets.

    \item[topological time features]
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Feature mixing}

We propose a mixing layer that lets the model capture dependencies between the features extracted in the previous stage.

Previous work only considers linear models that do not allow for feature dependencies.
Linear models have the advantage of being convex,
which means that optimizers are guaranteed to find a unique global minumum.
Because these linear models do not capture dependencies between features,
previous work has focused on developing good manual feature extraction techniques that incoporate these dependencies.
\fixme{New York, USA vs. York, England}

Recent advances in machine learning, however, have shown that non-linear models can be efficiently learned.
There is no guarantee that a globally optimal solution will be found;
however, the optimizers have been shown to find good solutions in practice.

Our model uses two fully connected layers with 2048 hidden units each and relu activation functions.
To reduce overfitting, each of these layers is trained using dropout and a 50\% keep probability \citep{}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Prediction losses}

Our general framework has two types of outputs:
discrete outputs that estimate the \place and \country fields of the tweet,
and continuous outputs that estimate the \geo field.
%Whereas previous work requires a number of assumptions about the output fields,
%our work makes few assumptions.

The discrete outputs are conceptually the simplest,
and for this reason they are the most commonly used in previous work.
The idea is to treat a tweet's \place and \country fields as labels for the tweet,
then perform ordinary multi-label classification using the cross entropy loss.
Our work differs from previous work because no previous work has attempted to directly predict a tweet's \place field.
Because there are millions of distinct \place entries in twitter,
this task was considered too hard.
Instead, previous work constructed \pseudoplace labels that filter out uncommonly seen entries and merge them into other nearby locations.
This reduces the number of class labels from millions down to thousands,
making a much easier problem.

There is no standard method for constructing these \pseudoplace labels in the literature.
\citet{han2012geolocation} proposed a more sophisticated method.
Their method combines suburbs with nearby cities,
and identifies a total of 3709 cities to use as class labels.
Those tweets that do not originate from a city are either discarded or assigned to the closest city.
More complicated methods of discretizing the Earth's surface have also been developed for non-Twitter applications.
For example, Google developed a large scale system for geolocating images that uses specially designed partition of the earth's surface that they call S2 \citep{weyand2016planet}.

We introduce novel methods for predicting the \geo field that take advantage of the non-euclidean nature of the earth's surface.
\citet{duong2016near} is the only previous work to attempt to estimate the \geo field of a tweet.
They use the ordinary least squares regression model with the standard $L2$ loss between GPS coordinates.
\begin{equation}
    \latd^2 + \lond^2
\end{equation}
Unfortunately, the surface of the earth is highly non-euclidean, 
and the OLS model is known to work only in the euclidean setting \citep[e.g.][]{fisher1992regression}.
For example:
\begin{enumerate}
    \item
        There is a difficulty at the antimeridian (180 degrees west).
        The antimeridean approximately separates Russian from Alaska.
        The true distance between these locations is very short (about 100km),
        but the distance using the L2 norm is very large.
    \item
        Second, the standard $L2$ loss does not accurately capture the distance between GPS coordinates.
        For example, at a latitude of 80 degrees north, 1 degree of longitude is approximately equal to 20 km;
        but at the equator, 1 degree of longitude is approximately equal to 111 km.
\end{enumerate}

The so-called \emph{angular regression} fixes these problems \citep{fisher1992regression}.

The \emph{great circle distance} is a better distance metric for gps coordinates because it is the distance of the shortest path between two points on the earth's surface.
The naive computation of the great circle distance is unfortunately numerically unstable and cannot be used directly.
Fortunately, \citet{vincenty1975direct} proposed a numerically stable method for computing the GCD which we use in our work.
It is defined as
\begin{align}
    \delta\sigma 
    &=
    \arctan\left(
        \frac
        {\sqrt{(\cos\lata\cdot\sin\lond)^2 + (\cos\lata\cdot\sin\latb-\sin\lata\cdot\cos\latb\cdot\cos\lond)^2}}
        {\sin\lata\cdot\sin\latb + \cos\lata\cdot\cos\latb\cdot\cos\lond}
    \right)
\end{align}

\citet{fisher1992regression} was the first to propose a method for regression onto the surface of a sphere.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}

\begin{figure*}
\noindent\input{img/hr-es}
\caption{}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}

\newpage
\appendix
\ignore{
\section{Languages and Day of Week}
\noindent\input{img/day-en}

\noindent\input{img/day-ar}

\noindent\input{img/day-es}

\noindent\input{img/day-fr}

\noindent\input{img/day-in}

\noindent\input{img/day-tl}

\noindent\input{img/day-tr}

\noindent\input{img/day-ja}

\noindent\input{img/day-ko}

\noindent\input{img/day-pt}

\noindent\input{img/day-zh}

\noindent\input{img/day-und}

\noindent\input{img/day-de}

\noindent\input{img/day-nl}

\noindent\input{img/day-cs}

\noindent\input{img/day-da}

\noindent\input{img/day-el}

\noindent\input{img/day-pl}

\noindent\input{img/day-ru}

\noindent\input{img/day-vi}

\noindent\input{img/day-th}
}

\onecolumn
\section{Languages and Time of Day}
\noindent\input{img/hr-ar}

\noindent\input{img/hr-en}

\noindent\input{img/hr-es}

\noindent\input{img/hr-fr}

\noindent\input{img/hr-in}

\noindent\input{img/hr-tl}

\noindent\input{img/hr-tr}

\noindent\input{img/hr-ja}

\noindent\input{img/hr-ko}

\noindent\input{img/hr-pt}

\noindent\input{img/hr-zh}

\noindent\input{img/hr-und}

\noindent\input{img/hr-de}

\noindent\input{img/hr-nl}

\noindent\input{img/hr-cs}

\noindent\input{img/hr-da}

\noindent\input{img/hr-el}

\noindent\input{img/hr-pl}

\noindent\input{img/hr-ru}

\noindent\input{img/hr-vi}

\noindent\input{img/hr-th}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Reference notes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Languages}

\citet{zaidan2014arabic} Arabic dialects.

\citet{refaee2014arabic} Arabic with twitter.

\citet{mohammad2016translation} translation alters sentiment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Twitter applications}

\citet{cheng2010you} uses a probabilistic framework to generate city-level user home location using only tweet text.
Introduces a naive bayes model, but for some reason uses a sum instead of a product (bad math?).
Shows that this model does poorly,
and introduces a 'local words' weighting of the naive model to improve performance.
Introduces several smoothing mechanisms to help with the sparsity of tweet data.
They only consider tweets in the contiguous US.

\citet{kinsella2011m} predicts the location of individual tweets and a user's home location.
Provides a simple probabilistic model: 
For each location, estimate a distribution of terms associated with that location.
Does not incorporate time.
A straightforward generalization of \citet{cheng2010you} to tweet location instead of user location.

\citet{li2012towards} uses a probabilistic framework to generate city-level location using the content of the tweet and the network of tweet replies.
Does not use a gazetteer or the underlying social graph.
This seems like a straightforward extension of \citet{cheng2010you}.

\citet{han2013stacking} predicts the city of a twitter user using both text and metadeta using stacking.
I believe this is the first paper to consider the effect of semantic shift in geolocation.
They show that user declared location metadeta is more sensitive to temporal change than the message text.

\citet{mahmud2014home} identifies the home location of a user rather than the location of an individual tweet.
Incorporates temporal information in the tweets to identify when a user is travelling.
\fixme{Has good twitter geolocation references that I don't have elsewhere.}
Uses an ensemble classifier with a gazeteer as one of the key features.
Lots of manual constructions and NLP-based preprocessing.
Hierarchical model that first predicts a general geographic region (e.g. timezone or state), then predicts the city.

\citet{han2014text} has many new ideas.
Introduce a multilingual dataset and the first methods for geolocating non-English tweets.
They use a hierarchical model that first determines the language,
then selects a model appropriate for the language.
Make heavy use of twitter metadata (e.g. tweet time) to determine location.
Perform a test on time where they evaluate their model on data collected 1 year after the training data, and show a significant performance loss.
Studies the privacy implications of geolocation.

\citet{compton2014geotagging} propose a simple convex problem for geolocating twitter users to city level accuracy using the social network graph only.
Whereas previous methods rely on local heuristics, their convex program uses global properties of the social graph.
Has lots of empirical results showing accuracy of self reported locations, location homophily among friends, and typical travel habits of twitter users.

\citet{rahimi2015twitter} uses both twitter text and the network graph for geolocation, but does not include time.
\fixme{Has good twitter geolocation references that I don't have elsewhere.}
Good experiments with good datasets.
Uses kd-tree for faster search.
Spatial labels are discretized over an adaptive grid based on the number of users in the region.
The @-mention information is used to build an undirected graph between users.
They convert the graph into a ``collapsed graph'', and there's lots of subtleties here about how they handle the train/test split and edges that pass between the two sets.
Uses Model Adsorption over the graph to predict geolocations within the test set, 
with two key modification:
(i) removing celebrity nodes from the network graph (lets them scale to larger networks)
and (ii) incorporating textual information as ``dongle nodes''.

\citet{dredze2016geolocation} studies time's effect on geolocation of individual tweets to the city level.
Demonstrates cyclical temporal effects on geolocation accuracy and rapid drops in accuracy as test data moves beyond the training data's time period.
They show that this temporal drift can be countered with modest online model updates.
Introduces a particularly large new dataset.
Used vowpal wabbit to learn the model.

\citet{duong2016near} does regression to predict the gps coordinates without taking into account the geometry of the earth.
Uses matrix factorization of a bag-of-words type model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Twitter analytics}

\citet{hecht2011tweets} measures the accuracy of the location field in twitter user profiles.

\citet{dredze2013carmen} introduces the Carmen system for twitter geolocation.
Then use it to improve influenza surveillance.
Carmen does not do prediction of location from text,
but instead only measures accuracy of the various location fields.

\citet{he2015hawkestopic} uses Hawkes process to model the social graph of Twitter,
but does not apply the idea to geolocation.

\citet{graham2014world} survey of geolocation methods for geographers.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Emojis}

\citet{park2013emoticon} studies emoticon usage in the first three years of the twitter platform.
They show, for example, that vertical emoticons are indicative of asian cultures and horizontal emoticons of european cultures.

\citet{lu2016learning} show that emoji usage varies around the world on smart phone SMS messages using a keyboard app.

\citet{ljubevsic2016global} shows that emoji usage can be used to predict tweet location.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Generic applications}

\citet{eisenstein2010latent} presents a multilevel generative model that resents jointly about latent topics and geographical regions.
Highly cited, and probably the right foundation for my graphical model.
Does not incorporate time or model the geometry of the regions.
Uses mean field variational inference.
\fixme{Think about this more.}

\citet{speriosu2010connecting} models language and geography outside the Twitter context for toponym resolution (disambiguating place names).
Uses a graphical model based on probabalistic topic models,
where regions of the earth are represented by different topics.
Inference is done with a collapsed Gibbs sampler.
Does not incorporate the Earth's spherical geometry or any distance relations between locations.

\citet{wu2017link} proposes a model for predicting the generation of new links in social networks. 
Uses a convex optimization problem with closed form solution.

\citet{yu2017temporally} uses matrix factorization to predict the formation of new edges in social networks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Less important work}

\citet{leidner2011detecting} provides a tutorial on methods for parsing geographical references in natural language.

\citet{jurgens2015geolocation} surveys existing methods.
They show a large performance gap between real world performance and the idealized laboratory-performance reported in the compared methods' publications.
\fixme{Review their references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Other applications of social network analysis}

\citet{ruiz2012correlating} uses tweets to predict financial markets.

\citet{wiley2014pharmaceutical} uses tweets to measure the effectiveness and user satisfaction of new drugs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Deep learning}

\citet{kim2016character} proposes using character level CNNs, RNNs, and Highway Networks for language translation.
\citet{chung2016character} also proposes character level RNNs and highway nets for translation.
\citet{jozefowicz2016exploring} is a generic classification paper for character level text processing.  
Uses a combination of CNNs and RNNs, and a hierarchical softmax which might be useful for locations.

\citet{dhingra2016tweet2vec} creates a vector space model of tweets using a character level recurrent GRU network.
\citet{severyn2015unitn} use a word level CNN for sentiment classification on tweets.

\citet{conneau2017very} use resnet like connections to create very deep character cnns for text classification.
\citet{zhang2015character} use a smaller depth character CNN, which is what I've implemented so far.

\citet{weyand2016planet} do geolocation of photos using image CNNs.
They divide the globe into 26263 regions, and use a xentropy loss over those regions.
Regions are of different sizes so that they all contain approximately the same number of photos.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ignore{
\section{Problem Overview}

Previous work focuses on learning a parameterized model for tweet location.
The advantages of these methods are:
\begin{enumerate*}[label=(\arabic*)]
    \item the resulting models are simple, and
    \item the models can be trained and deployed on low-power devices.
\end{enumerate*}
The disadvantages are:
\begin{enumerate}
    \item These methods can model recurring space/time interactions 
        (e.g. patterns caused by timezone differences and weekend/nonweekend behavioral patterns), 
        but they cannot handle one time outlier events such as natural disasters or entertainment events.
    %\item They provide point estimates a tweet's location rather than a distribution of possible locations.
    \item Experiments by \citet{mahmud2014home} and \citet{dredze2016geolocation} show that the models do not generalize well to unseen time periods.
\end{enumerate}
I propose a nonparametric approach to geolocation that should improve these disadvantages.
The tradeoff is that the model is more complex and not deployable on low-power devices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proposed Solution}

Let $\{e_i\}_{i=1}^n$ be the set of observed tweets,
and $e$ be a new tweet not in the database.
Then define the nonparametric distribution over $e$ 
\begin{equation}
    \prob e
    \propto
    %\frac 1 n 
    \sum_{i=1}^n \exp\left(-\dist_\theta(e,e_i)^2\right)
    ,
\end{equation}
where $\dist_\theta$ is a distance metric between two tweets that depends on parameter vector $\theta$.
If we let $e(\ell)$ denote the tweet $e$ updated to have location $\ell$,
then the distribution of the tweet's location is given by
\begin{equation}
    \prob {e(\ell)} 
    \propto
    %\frac 1 n 
    \sum_{i=1}^n \exp\left(-\dist_\theta(e(\ell),e_i)^2\right)
    ,
\end{equation}
and the maximum likelihood point estimate of the tweet's location is
\begin{equation}
    \label{eq:hatell}
    \hat\ell
    =
    \argmax_{\ell} 
    \sum_{i=1}^n \exp\left(-\dist_\theta(e(\ell),e_i)^2\right)
    .
\end{equation}
The quality of the estimate $\hat\ell$ is affected by three factors:
\begin{enumerate}
    \item \emph{The number of tweets in the database.}
        Standard results in nonparametric distributions show that as $n\to\infty$, 
        the distribution $\prob{e}$ will approach the true underlying distribution.
        %Since the underlying distribution is highly complex, 
        %many samples will be needed to get a reasonable approximation.
        %Fortunately, we have many tweets available to us,
%
        The downside of this strategy is that the summations above are summations over the entire database.
        This is not practical, so the summation will need to be approximated.
        There are many good metric data structures that can restrict the summation to only the most relevant portions of the space.

    \item \emph{The family of distance metrics $\dist_\theta$.}
        The distance metric needs to tie information about a tweet's location to information about the other features in a tweet.
        There are many possible metric families,
        and finding the optimal one is likely a difficult challenge.

        A simple approach is to use an ``ensemble of metrics.''
        Let $\{d^{(i)}\}_{i=1}^m$ be a set of $m$ metrics.
        Then define the Mahalanobis distance
        \begin{equation}
            \label{eq:mahalanobis}
            \dist_\theta(e_1,e_2) ^2
            =
            \trans{
            \begin{pmatrix}
                \dist^{(1)}(e_1,e_2) \\
                \dist^{(2)}(e_1,e_2) \\
                \vdots \\
                \dist^{(m)}(e_1,e_2) \\
            \end{pmatrix}
            }
            \theta
            \begin{pmatrix}
                \dist^{(1)}(e_1,e_2) \\
                \dist^{(2)}(e_1,e_2) \\
                \vdots \\
                \dist^{(m)}(e_1,e_2) \\
            \end{pmatrix}
        \end{equation}
        %where $A : \R^{m\times m}$ is the parameter vector $\theta$ that is to be learned.
        where $\theta$ is a $m\times m$ matrix that needs to be estimated from the data.

        The ensemble should include several metrics related to gps coordinates 
        (e.g. the geodesic distance \citep{vincenty1975direct}, the google maps distance),
        time 
        (e.g. total distance in time, and the distance in time mod hourly, daily, weekly, monthly, and yearly intervals),
        social graph distances
        (e.g. total number of hops in a friendship graph),
        and any features of the tweet itself.

        There are two possible drawbacks of a large ensemble.
        First, a larger ensemble will increase the computational burden of computing distances.
        This can possibly be ameliorated by a pruning strategy that only evaluates the more expensive distances after the cheaper ones if it is actually necessary.
        Second, more metrics increases the number of parameters, increasing the possiblity of overfitting.
        The number of available tweets is so large, however, that this seems unlikely for a linear Mahalinobis distance.

        %These ensemble metrics can be extended by:

    \item \emph{The quality of the estimated parameter vector $\theta$.}
        %We can solve for $\hat\theta$ using the formula
        A good parameter vector $\hat\theta$ will maximize the likelihood that
        \begin{equation}
            \label{eq:hattheta}
            \hat\theta
            =
            \argmax_{\theta} 
            \sum_{i=1}^n 
            \sum_{j=1}^{i-1}
            \exp\left(-\dist_\theta(e_i,e_j)^2\right)
            .
        \end{equation}
        This can be solved with an SGD procedure.
        Using only a small sample of the data should be sufficient.
        %For the Mahalanobis metric above,
        %this optimization is convex.
        %In general, however, for other metrics, it need not be.
        %An important decision will be the choice of regularizer for $\theta$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
\section{Research Questions}

\noindent
Applied questions
\begin{enumerate}
    %\item \textbf{How do we efficiently perform the sums?}
    \item Does jointly training the feature space and metric parameters improve performance?
    \item Many possible choices of metric to experiment with to determine which is best for different applications.
        For example, how should we incorporate information related to ``location'' vs ``gps'' fields of the tweet?
    \item Which regularizer should be chosen to optimize $\hat\theta$ in \eqref{eq:hattheta}?
        Is there a better loss function to use?
    \item Queries in this framework can be significantly more complicated:
        \begin{enumerate}
        \item We can easily in incorporate constraints into the queries.
            For example: Given that I know the tweet came from the midwest,
            which city was it most likely to come from?
        \item Given that a tweet came from a particular location,
            what time was it most likely tweeted?
            What user mostly likely sent the tweet?
        \item
            Generate a tweet that is likely to have been sent from location X at time Y.
            (This requires that the metrics in the ensemble have a generative semantics.)
        \end{enumerate}
\end{enumerate}

\noindent
Theoretical questions:
\begin{enumerate}
    \item \emph{Nonconvex optimization.}
        The optimization in \eqref{eq:hatell} is nonconvex.
        Finding a unique global optimum, however, is not likely to be important for practitioners.
        Instead, reporting several representative solutions will likely be more useful.
        These solutions should:
        \begin{enumerate}
            \item not get stuck in ``small'' local optima, and
            \item represent all the largest modes in the distribution.
        \end{enumerate}
        What is the best way to optimize under these constraints?

    \item \emph{Metric ensembles.}
        The theoretical properties of ensembles of metrics like proposed in \eqref{eq:mahalanobis} have not been explicitly studied before.
        \begin{enumerate}
            \item How does the dimension of the ``submetrics'' influence the dimension of the resulting ensembled metric?
                It is likely that each metric will have ``different dimensions at different scales,''
                and how do we incorporate this information to improve runtime bounds on queries?
            \item What are the best ways to ensemble the metrics? 
                \begin{enumerate}
                    \item Other $L_p$ combinations can be used besides $L_2$.
                    \item The matrix $\theta$ could be replaced by a higher order tensor to capture more complicated relationships between the submetrics.
                    \item We can kernelize the Mahalanobis distance to create a nonlinear family of metrics.
                    \item Using parameterized metrics as the base metrics naturally results in a ``deep'' metric learning problem.
                \end{enumerate}
            \item Can we develop metric data structures specifically tuned for ensemble metrics?
                \begin{enumerate}
                    \item Can we efficiently add/delete/modify metrics in the ensemble without recreating the data structure?
                    \item Can we query using only a portion of the metrics in the ensemble?
                    \item Can we support database style queries distributed over multiple machines? 
                \end{enumerate}
        \end{enumerate}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem description}

Existing work on tweet geolocation uses only text features to predict location.
I want to use time as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Intuitive motivation}

Time by itself is a good predictor of tweet location.
People are more likely to tweet during daytime hours,
and daytime is determined by longitude.
Combining time with textual features is even more powerful.

Consider a tweet with the text ``I am at the Taylor Swift concert''
(or alternatively a video of Taylor Swift performing).
If we knew the time of the tweet, 
and we knew Taylor Swift's concert schedule,
then we could get a good prediction of the location of the tweet.
Without knowing the time, however, we cannot say which city the tweet was sent from.
We actually don't even need to know Taylor Swift's concert schedule.
These concerts have many thousands of attendees, many of whom are tweeting.
If any of these attendees tweets about Taylor Swift and has geotagging enabled,
we should be able to use this information to infer that the original tweet was from the same location
(since it happened at the same time).

The example of a Taylor Swift concert happens over a narrow time scale and in a small location.
Other examples of space/time dependencies occur at larger scale.
For example: 
(i) natural disasters such as floods can affect multiple cities over many weeks;
(ii) normal weather conditions (such as snow, rain, or heat) affect latitudinal bands over the course of seasons;
and (iii) elections can affect entire cities, states, and countries for months.
We would like a system that can automatically identify these space/time dependencies and use them to predict tweet location.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection {Formal problem statement}

We call each tweet an event $e$ and decompose it into three components: 
\begin{align*}
    %g    &= \text{the gps coordinates} \\ 
    %\ell &= \text{the location (either the location field in the tweet or exact gps coordinates)} \\
    \ell : \R^2 &= \text{the location represented as gps coordinates} \\
    x    : \R^d &= \text{document features (constructed from any text/images/video/urls in the tweet)} \\
    t    : \R~ &= \text{time}
\end{align*}
%Existing approaches model the probability distribution $\cprob{\ell}{d}$.
%Existing baseline approaches estimate the distribution $\cprob{\ell}{d}$.
Our goal is to estimate the conditional distribution $\cprob{\ell}{x,t}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection {Proposed technique}

I propose a nonparametric model for estimating the density.
Specifically, 
\begin{equation}
    \label{eq:nonparam}
    %\prob{e} = \frac 1 n \sum_{i=1}^n k(e,e_i)
    \prob{e} = \frac 1 n \sum_{i=1}^n \exp(-\dist(e,e_i)^2)
\end{equation}
where $\dist$ is a metric distance that describes the similarity of two tweets.
Learning the model will consist of setting parameters in the distance function.
The main challenge of large scale nonparametric models is that the summation in \eqref{eq:nonparam} is over the entire data set.
A clever metric data structure will be needed so that only a small fraction of the data will need to be searched.

Many metrics can be defined between tweets,
and each metric will determine a corresponding probability distribution.
The simplest metrics for location estimation ignore all the information in the tweets except the location.
The most obvious distance function is to use the great circle distance $\dist\gcirc$ between two gps coordinates.
The obvious formula contains numerical instabilities,
and so Vincenty's formula should be used instead \citep{vincenty1975direct}.
A bandwidth parameter would need to be estimated from the data.
%More powerful distance functions would take into account the rate that information can travel between two locations.
%We can define the google maps distance $\dist\gmap$ to be the length of time that google maps says it takes to drive between two locations.
%This distance is more likely to be too expensive to compute;
%and while it may be more accurate than $\dist\gcirc$,
%it is unlikely to be the best distance measure possible.
%Ideally, we would use a metric learning algorithm to achieve the best possible metric.

I think a learned metric would be a good way to associate textual and spatial features.
If $\Dist$ is a family of metrics and $\loss$ a loss function,
then the training procedure solves
\begin{equation}
    \hat\dist = \argmin_{\dist\in\Dist} \sum_{e_1,e_2\in E} \loss(\dist(e_1,e_2))
    .
\end{equation}
As in the testing procedure, evaluating the sum is computationally expensive and clever data structures are needed to compute it efficiently.
The efficacy of the method will be determined by the choice of distance family $\Dist$.

\ignore{
If the location is independent of the document and time, then $\cprob{\ell}{x,t} = \prob{\ell}$.
This can be modeled as a mixture of $n$ isotropic Gaussians.
That is,
\begin{equation}
    \prob{\ell} = \sum_{i=1}^n w_i\normal{\mu_i}{\eye\sigma_i}
    .
\end{equation}
There are 488 cities in the worldwith at least 1 million people, 
so around 1000 seems like a reasonable choice of $n$.
The parameters $w_i$, $\mu_i$, and $\sigma_i$ must be learned from the data.

Next we incorporate a dependence on time.
Let $k : E \times E \to \R$ be a kernel function that assigns a similarity to two tweets.
As simple gaussian kernel could be
\begin{equation}
    k(e_1,e_2) 
    = \exp(-\tau\abs{e_1(t) - e_2(t)}^2
           -\lambda\ltwo{e_1(\ell) - e_2(\ell)}^2)
\end{equation}
where $\tau$ and $\lambda$ are parameters that control the influence of temporal and spatial distance in the events' similarity.
We can then define the conditional distribution
\begin{equation}
    \cprob{\ell}{t} = p(\ell)\exp(-\sum_{\substack{\text{events $e'$ s.t.}\\e'(t)<t}} k(e,e'))
    .
\end{equation}

Finally, we can incorporate a dependence on the textual features simply by updating the kernel to include a dependence on the text.
}

%A Hawkes process is an appropriate model for $\cprob{\ell}{t}$.
%In a Hawkes process,
%the likelihood of an event at location $\ell$ is high when events have recently happened in nearby locations,
%and low when events have not recently happened nearby.
%Formally,
%\begin{equation}
    %\cprob{\ell}{t} 
    %%\propto
    %=
    %\prob{\ell}\exp(-\lambda(\ell;t))
%\end{equation}
%where $\lambda$ is called the rate function and is given by
%\begin{equation}
    %\label{eq:kernel}
    %\lambda(\ell;t) = \sum_{e} k(\ell,t,e,\theta)
%\end{equation}
%where $k$ is a kernel function that depends on parameter $\theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
}

\ignore{
\subsection{Computational notes}

\begin{enumerate}
    \item
        The summation in \eqref{eq:kernel} is over all tweets,
        which is obviously infeasible.
        Any reasonable choice of kernel will have a large value only for tweets nearby in space/time.
        Therefore distant points can be pruned from the summation to improve computation time.

    \item
        As formulated, the features can be computed completely independently from modeling of the space/time relations.
        It may be possible to improve the performance by jointly optimizing the feature selection algorithms with the space/time objectives.

    \item
        Modeling the locations $\ell$ as points in $\R^2$ induces distortions,
        because the data is actually gps coordinates on a sphere.
        Restricting the data to lie on the sphere would be better,
        but I'm not sure how to enforce that type of constraint.
        It might be easier (and more accurate!) to learn the topology of the underlying space from the data.
        This could help incorporate non-gps location data as well,
        and make the system usable on non-twitter data.
        For example, we could predict the university/funding agency for a piece of research in a citation social network.

    \item
        We should be able to use a semisupervised learning procedure to use all the tweets without location data.

\end{enumerate}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
