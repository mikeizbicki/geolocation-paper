%\documentclass{article}
\documentclass[sigconf,10pt]{acmart}

\setcopyright{rightsretained}
%\acmDOI{10.475/123_4}
%\acmISBN{123-4567-24-567/08/06}
\acmConference[]{}{}{}
%\acmConference[WSDM2019]{ACM Woodstock conference}{July 1997}{El Paso, Texas USA}
%\acmYear{1997}
%\copyrightyear{2016}
%\acmArticle{4}
%\acmPrice{15.00}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[greek,vietnam,russian,arabic,english]{babel}
\usepackage{CJKutf8}

% needed unicode character hacks
%\newcommand\dottedcircle{\tikz \draw [line cap=round, line width=0.25ex, dash pattern=on 0pt off 2pt] (0,0) circle [radius=0.75ex];}
%\usepackage{newunicodechar}
%\DeclareFontEncoding{LS1}{}{}
%\DeclareFontSubstitution{LS1}{stix}{m}{n}
%\DeclareFontFamily{LS1}{stixscr}{\skewchar\font127 }
%\DeclareFontShape{LS1}{stixscr}{m}{n} {<->s*[.7] stix-mathscr}{}
%\newunicodechar{◌}{{\usefont{LS1}{stixscr}{m}{n}\symbol{\string"E3}}}

% one of the foreign fonts deletes the font for \times
\renewcommand{\times}{\mathbin{\tikz [x=1.4ex,y=1.4ex,line width=.1ex] \draw (0,0) -- (1,1) (0,1) -- (1,0);}}%

% FIXME: required for arabic font;
% messes up numbering
\makeatletter
%\newcommand{\c@chapter}{0}
\def\thesection{\protect\if@rl\protect\I{\number\c@section}%
    \protect\else\protect\textLR{\number\c@section}%
\protect\fi}
\def\thesubsection{\protect\if@rl\protect\I{\number\c@subsection.\number\c@section}%
    \protect\else\protect\textLR{\number\c@section.\number\c@subsection}%
\protect\fi}
\def\thefigure{\protect\if@rl\protect\I{\number\c@figure}%
    \protect\else\protect\textLR{\number\c@figure}%
\protect\fi}
\def\thetable{\protect\if@rl\protect\I{\number\c@table}%
    \protect\else\protect\textLR{\number\c@table}%
\protect\fi}
\makeatother

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancypagestyle{plain}
{\fancyhf{}\cfoot{\thepage}}

\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\vspace{0.2\baselineskip}}
\AtEndEnvironment{quote}{\vspace{0.2\baselineskip}}

\usepackage[inline]{enumitem}
\setlist[description]{topsep=0.1cm,itemsep=0.1cm,style=unboxed,leftmargin=0cm,listparindent=\parindent}

%\usepackage{algorithm}
%\usepackage[noend]{algpseudocode}
%\usepackage{algorithmicx}
%\usepackage{algorithm2e}

%\usepackage[authordate,bibencoding=auto,strict,backend=biber,natbib]{biblatex-chicago}
%\usepackage[round]{natbib}   % omit 'round' option if you prefer square brackets
%\bibliographystyle{plainnat}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = blue  %Colour of citations
}

\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{matrix, positioning, fit}
\usepackage{pgfplots}
\pgfplotsset{width=7cm,compat=1.8}
\definecolor{darkgreen}{RGB}{0,127,0}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{cor}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\vecspan}{span}
\DeclareMathOperator*{\affspan}{aff}
\DeclareMathOperator*{\subG}{subG}
\DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\E}{\mathbb{E}}

\newcommand{\str}[1]{\texttt{#1}}
\newcommand{\defn}[1]{\textit{#1}}

\newcommand{\N}{\mathbb{N}}
%\newcommand{\R}{\mathbb{R}}
\newcommand{\trans}[1]{{#1}^{\top}}

\newcommand{\abs}[1]{\lvert{#1}\rvert}
\newcommand{\ltwo}[1]{\lVert {#1} \rVert_2}
\newcommand{\set}{\mathcal}
\renewcommand{\vec}{\mathbf}

\newcommand{\W}{\mathcal{W}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\Z}{Z}

\newcommand{\w}{\mathbf{w}}
\newcommand{\what}{\hat\w}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{y}

\newcommand{\dist}{d}
\newcommand{\Dist}{\mathcal D}
\newcommand{\kernel}{k}
\newcommand{\loc}{_{\textit{loc}}}
\newcommand{\gmap}{_{\textit{gmap}}}
\newcommand{\gcirc}{_\textit{gcirc}}

\newcommand{\loss}{\ell}
\newcommand{\reg}{r}

%\newcommand{\prob}[1]{\text{Pr}\left({#1}\right)}
\newcommand{\prob}[1]{p\!\left({#1}\right)}
\newcommand{\cprob}[2]{\prob{{#1} | {#2}}}
\newcommand{\normal}[2]{\mathcal{N}({#1},{#2})}
\newcommand{\eye}{I}

%\newcommand{\plots}[1]{}
\newcommand{\plots}[1]{#1}
\newcommand{\ignore}[1]{}
\newcommand{\fixme}[1]{\textcolor{red}{\textbf{FIXME:} {#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\uniloc}{\textsc{UniLoc}}

\newcommand{\tweetdata}[1]{{\texttt{#1}~}}
\newcommand{\place        }{\tweetdata{place}}
\newcommand{\pseudoplace  }{\tweetdata{pseudo-place}}
\renewcommand{\country      }{\tweetdata{country}}
\newcommand{\geo          }{\tweetdata{geo}}

\newcommand{\lata}{\text{lat}_1}
\newcommand{\latb}{\text{lat}_2}
\newcommand{\latd}{(\lata-\latb)}
\newcommand{\lona}{\text{lon}_1}
\newcommand{\lonb}{\text{lon}_2}
\newcommand{\lond}{(\lona-\lonb)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\title{Geolocating Tweets in any Language at any Location}
%\title{A Universal Language Model with Application to Tweet Geolocation}
\title{Highly Multilingual Tweet Geolocation with \\ Unicode Convolutional Neural Networks}
%\titlenote{Produces the permission block, and copyright information}
%\subtitle{}
%\subtitlenote{The full version of the author's guide is available as \texttt{acmart.pdf} document}

\author{Anonymous Authors}
\affiliation{}
\email{}

% The default list of authors is too long for headers.
%\renewcommand{\shortauthors}{shortauthors}

\begin{document}

\begin{abstract}
    %We consider the problem of determining the GPS coordinates that a tweet was sent from based only on its text.
    %We introduce a novel character-level convolutional neural network that can accept any Unicode character as input,
    Most social media messages are written in languages other than English,
    but commonly used text mining tools were designed only for English.
    %Those that work for non-English languages require that the language be known in advance and do not allow the language to change mid-sentence.
    This paper introduces the \defn{Unicode Convolutional Neural Network} (UnicodeCNN) for analyzing text written in any language.
    The UnicodeCNN does not require the language to be known in advance,
    allows the language to change arbitrarily mid-sentence,
    and is robust to the misspellings and grammatical mistakes commonly found in social media.
    %that accepts any Unicode string as input.
    %We make no assumption that the text's language is known in advance,
    %and we allow the language to change arbitrarily mid-sentence.
    We demonstrate the UnicodeCNN's effectiveness on the challenging task of content-based tweet geolocation using a dataset with 900 million tweets written in more than 100 languages.
    %To validate our approach, we consider the task of determining the GPS coordinates that a tweet was sent from based only on its message contents.
    %Our dataset contains 900 million tweets written in more than 100 languages.
    %We show that the UnicodeCNN significantly outperforms baseline best-practices when working with English-only text.
    Remarkably, the UnicodeCNN can learn geographical knowledge in one language and automatically transfer that knowledge to other languages.
    %(ii) we predict exact GPS coordinates, whereas previous work predicted only country or city of origin;
    %(iii) we 
    %Due to the high-quality features of the UnicodeCNN,
    %we are able to predict the exact GPS coordinates that tweets were sent from using a novel mixture of Fisher-von Mises distributions.
    %Previous state-of-the-art work restricted itself to predicting only the country or city the tweet was sent from, and filtered out particularly ``hard'' tweets from their evaluation datasets.
    %A disadvantage of our UnicodeCNN approach is that deep neural networks are notoriously difficult to interpret compared to simpler linear models, but we mitigate this disadvantage by introducing a novel method for visualizing the activations of our UnicodeCNN.
    %and we apply this visualization to discover dialectical differences between geographical regions.

    %CNNs are notoriously difficult to interpret,
    %and we introduce a method for visualizing the CNN.

    %Our model works on tweets written in any language,
    %and our evaluation dataset contains more than 900 million tweets written in more than 65 languages from around the world.
    %Three novel techniques make our model successful:
    %(1) We introduce a character-level convolutional neural network (CNN) that can accepts any Unicode character as input.
    %(2) We use a mixture of Fischer-von Mises distributions to predict the GPS coordinates of a tweet.
    %(3) We introduce a simple method for visualizing which letters of a tweet contribute most to the decision making process.

    %This dataset contains the largest number of languages used for any currently published natural language processing task.
    %We achieve high accuracy across many languages through a novel character embedding that lets us develop a convolutional neural network that shares information between the languages.
    %The output layer of our network exploits the non-Euclidean properties of the Earth's surface to accurately predict GPS locations.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
    \fixme{}
\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}


\keywords{Multilingual; Unicode; Geotagging; Twitter; Convolutional Neural Networks}

\maketitle

%\newpage
%\renewcommand{\figurename}{Figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{figure*}
    %\centering
    %\begin{tabular}{p{0.3\textwidth}p{0.3\textwidth}p{0.3\textwidth}c}
        %\resizebox{0.3\textwidth}{!}{\includegraphics{img/infer/es-vosotros-good/1-med-zoom}} &
        %\resizebox{0.3\textwidth}{!}{\includegraphics{img/infer/ar-kuwait-japanese/0-med-zoom}} &
        %\resizebox{0.3\textwidth}{!}{\includegraphics{img/infer/ar-kuwait-japanese/5-med-zoom}} &
        %\\
        %\small
        %\str{Qu\'e gran ma\~nana... emocionante.  Pod\'eis ver el v\'ideo abajo en el enlace.}%
        %&
        %\str{I'm at }\foreignlanguage{arabic}{شارع المطاعم}\str{ in }
       % 
        %\foreignlanguage{arabic}{الكويت}.%
        %&
        %\str{I'm at }\begin{CJK}{UTF8}{min}通りレストラン\end{CJK}\str{ in }
        %\begin{CJK}{UTF8}{min}クウェート\end{CJK}.
    %\end{tabular}
    %\caption{test}
    %\label{fig:poder}
%\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Social media platforms are becoming increasingly multilingual.
More than 100 languages are known to be actively used on Twitter \citep{hong2011language},
and while English is still the most popular, its popularity is shrinking.
An analysis of tweets sent in 2012 found that 53\% of tweets were written in English \citep{han2014text},
but our analysis of tweets sent in 2018 shows that only 42\% of tweets were written in English.
%While English and other popular languages have been well studied,
%less popular languages have received relatively little attention.
%This is unfortunate because approximately 80 million tweets are sent everyday in a language that is not one of Twitter's ten most popular languages.
This percentage will likely continue to fall
as more people in non-English speaking countries connect to the internet and begin using social media.
%Approximately 16\% of tweets are sent in either a language that Twitter doesn't officially support or one of Twitter's 55 least popular official languages.
%In absolute numbers, this is about 80 million tweets per day.
%Approximately 8\% of tweets are sent in Twitter's 55 least popular languages,
%and a further 8\% of tweets are sent in languages that the Twitter API does not officially support.
New multilingual data analysis tools are needed to better serve these emerging non-English communities.
%Many language communities are poorly served by state-of-the-art text analysis tools,
%which are designed for English and similarly popular languages.
%Better data analysis tools are need to serve these non-English communities.
%Good methods for analyzing multilingual social text is therefore highly important
%and understudied.

This paper introduces the \defn{Unicode Convolutional Neural Network} (UnicodeCNN) for analyzing multilingual social media text.
The UnicodeCNN generates features directly from the Unicode characters in the input text and requires no lexical analysis, stemming, or other preprocessing.
Character-level convolutional neural networks have previously been successfully applied to text corpora containing only English \citep{zhang2015character,conneau2017very} or a limited number of European languages \citep{lee2016fully,wehrmann2017character},
but our UnicodeCNN can take any Unicode string as input and therefore works with all languages.
We use the same model to analyze English, Arabic, Japanese, and the more than 100 languages present in our Twitter dataset.
%Amazingly, we show that information learned in one language can be transfered to other languages.
%Many bi-lingual models have been developed for various natural language tasks,
%but multi-lingual models remain rare \citep{ruder2017survey}.
%We believe the UnicodeCNN is the first model designed to work on all languages simultaneously.
The UnicodeCNN is particularly well-suited to analyzing social media text because it is naturally robust to the spelling mistakes and non-standard grammar commonly found in this domain.
%As an extreme example, \defn{Arabizi} is a newly developing dialect of the Arabic language found only in social media platforms.
%Arabizi consists of a mixture of Latin and Arabic characters in a single message,

We apply the UnicodeCNN to the challenging problem of \defn{content-based tweet geolocation},
or predicting the GPS coordinates a tweet was sent from based only on its text.
Geolocation is a well-studied problem,
and our UnicodeCNN significantly improves the state-of-the-art in three ways.
First, most prior work considers only English-language tweets \citep{cheng2010you,li2012towards,han2013stacking,mahmud2014home,compton2014geotagging,zhang2014geocoding,rahimi2015twitter,dredze2016geolocation,rahimi2017neural},
or tweets in a single non-English language like Spanish \citep{maier2014language,gonccalves2015learning,tinoco2017variation}. % or Italian \citep{paraskevopoulos2015fine}.
\citet{han2014text} propose a multilingual system that first detects the language of the tweet,
then passes the tweet to an appropriate single-language model.
Each of these single-language models must be developed independently and from scratch,
and \citet{han2014text} only support Twitter's most popular languages.
The UnicodeCNN, in contrast, processes all languages in a unified method.
Remarkably, we show that geographic information learned in one language is automatically learned in other languages as well.
Second, all previous work considered only a subset of tweets that were particularly easy to geolocate.
In particular, all works cited above filter their datasets to contain only tweets sent from certain countries or major cities.
We perform no such filtering because our method works on all tweets, written in all languages, sent from anywhere in the world.
%The UnicodeCNN generates high quality features that work everywhere in the world,
%and so we perform no such filtering in our experiments.
Finally, most previous work solves a classification problem,
where each tweet is associated with either its country or city of origin.
%Two papers \citep{thomas2017twitter,duong2016near} propose to learn the exact GPS coordinates of tweets,
%but fail to consider the non-Euclidean nature of the Earth's surface.
We instead use a mixture of Fischer-von Mises distributions to predict the exact GPS location of tweets.
We show that the high quality features generated by the UnicodeCNN enable us to solve this more challenging prediction problem.
%We introduce a new technique based on the mixture of Fisher-von Mises distributions that lets us accurately predict the exact GPS location of tweets. 
On average, our model is able to geolocate tweets more than \fixme{450km} closer to their true location than previous methods.

%We evaluate the UnicodeCNN on the twitter geolocation task for two reasons.
%First, geolocation has many important applications.
Tweet geolocation has many important applications.
Geotagged tweets have been used to map the spread of influenza \citep{paul2014twitter,santillana2015combining},
for measuring the impact of earthquakes \citep{sakaki2010earthquake},
for coordinating emergency services \citep{klein2012detection,rudra2016summarizing},%imran2016twitter,pohl2016online,
for measuring the spread of political opinions \citep{conover2011political,barbera2014birds},
for comparing dietary habits in different locations \citep{widener2014using},
for measuring neighborhood happiness levels \citep{nguyen2016leveraging}, 
and for measuring unemployment rates \citep{antenucci2014using,llorente2015social}.
Unfortunately for these applications, only about 1\% of tweets are geotagged by their users.
Our system can predict the location of the other 99\% of tweets and therefore improve the accuracy and applicability of all of these methods.

\ignore{
Second, we believe our UnicodeCNN trained on 
Finally, geolocated tweets provide a large source of high quality labeled text.
Quality labeled text is rare and often the limitting factor in natural language processing tasks.
Transfer learning has therefore become a popular strategy to learn these tasks \citep{wang2015transfer,howard2018fine}.
The model we develop is particularly suitable for transfer learning because it incorporates many languages.
We open source our model and a toolkit to help practitioners use this model as the base for other multilingual NLP tasks.
}

\ignore{
\begin{description}
%\begin{description}
\item[Why care about geolocation?]
    Geolocating tweets is an important problem for three reasons.

First, many important applications require geolocated tweets as input.
For example, geotagged tweets have been used to map the spread of influenza \citep{paul2014twitter,santillana2015combining},
for measuring the impact of earthquakes \citep{sakaki2010earthquake},
for coordinating emergency services \citep{klein2012detection,imran2016twitter,rudra2016summarizing,pohl2016online},
for measuring the spread of political opinions \citep{conover2011political,barbera2014birds},
for comparing dietary habits in different locations \citep{widener2014using},
for measuring neighborhood happiness levels \citep{nguyen2016leveraging}, 
and for measuring unemployment rates \citep{antenucci2014using,llorente2015social}.
Unfortunately for these applications, only about 1\% of tweets are geotagged by their users.
A system that could predict the location of the other 99\% of tweets would immediately improve the accuracy and applicability of all of these methods.

Second, the models used for geotagging tweets help us understand the differences between different languages and dialects.
For example, previous work has used geotagged language models to map the dialects of Standard American English versus African-American English \citep{huang2016understanding,gonccalves2017fall,blodgett2016demographic}. %, and the various dialects of Spanish \citep{gonccalves2014crowdsourcing}.
%Our language model is more advanced than previous models because it works for significantly more languages,
%and it operates on the character level rather than the word level.
%This more advanced model lets learn more detail about dialectical differences in many more languages than previous work.
Our language model is the first to operate on the character level using a highly multilingual dataset.
Therefore, we can map significantly more dialectical variations in language useage than previous work.

Finally, geolocated tweets provide a large source of high quality labeled text.
Quality labeled text is rare and often the limitting factor in natural language processing tasks.
Transfer learning has therefore become a popular strategy to learn these tasks \citep{wang2015transfer,howard2018fine}.
The model we develop is particularly suitable for transfer learning because it incorporates many languages.
We open source our model and a toolkit to help practitioners use this model as the base for other multilingual NLP tasks.
%Furthermore, because our language model combines so many different languages,
%and can form the basis for multilingual classification tasks using transfer learning \citep{wang2015transfer,howard2018fine}.
%The best existing image transfer learning used billions of Instagram images labelled by their hashtags from FAIR \citep{mahajan2018exploring}.
}

\ignore{
\begin{description}
\item[Paper Outline.]
    \fixme{
The next section qualitatively illustrates the advantage of the UnicodeCNN over other models.
%Then in Section \ref{sec:words} we describe in detail why the word-level approach of previous geolocation systems cannot scale to the massively multilingual dataset we use.
%In Section \ref{sec:model} we introduce our character level model that fixes the deficiencies of the world-level models.
%We also describe two novel neural network output layers called angualar regression and the mixture of von-Mises,
%and show how these methods take advantage of the geometry of the Earth's surface to enable us to predict exact GPS coordinates of a tweet.
%Section \ref{sec:experiments} shows the experimental results showing that our method significantly outperforms previous methods.
Section \ref{sec:model} describes the UnicodeCNN and our novel mixture of Fisher-von Mises layer for predicting GPS coordinates.
Section \ref{sec:experiments} describes our dataset of 900 million tweets,
and our experimental results.
}
\end{description}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Identifying tweet location vs user location}
%
%In this paper, we focus on the task of identifying tweet location, rather than user location.
%Identifying the location of a tweet is the harder problem because there is less information available.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Qualitative Overview}

Before describing our model in detail,
we present some simple examples.
We use these examples to introduce the details of the geolocation problem,
and demonstrate the qualitative ability of the UnicodeCNN to learn complex language features.

%We now consider two examples that illustrate the difficulty of the geolocation problem and the power of the UnicodeCNN compared with other methods.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{description}
\item[Example 1.]
%We begin with an example that illustrates the difficulty of the problem.
%We now consider an example which illustrates the difficulty of the geolocation problem and the power of the UnicodeCNN model.
%Let's try to guess where the following tweet was sent from:
    %Consider the following real tweet:
    %Let's try to guess where the following tweet was sent from:
    %This example illustrates the difficulty of the geolocaiton problem by showing how location information is encoded in subtle linguistic patterns within a tweet.
    Our first example illustrates how the UnicodeCNN is able to learn verb conjugation rules and use them for geolocation.
    %Different dialects of the same language use different verb conjugation rules,
    %and we use this fact to geolocate tweets.
    %Remarkably, our model is able to understand conjugations of verbs it has never been trained on and is robust to misspellings.
    Consider the following short Spanish tweet:%
%\begin{quote}
    %\str{Qu\'e gran ma\~nana... emocionante.  Pod\'eis ver el v\'ideo abajo en el enlace.}%
    %\footnote{Translation: ``What a great morning... exciting.  You can see the video below the post.''}
%\end{quote}
    %\footnote{Tweet ID: 927275629061591040.}
    \begin{quote}
        \str{No me habl\'eis}
    \end{quote}
    which translates as
    \begin{quote}
        \str{Don't speak to me}
    \end{quote}
Figure \ref{fig:poder} shows the GPS coordinates that our model predicts this tweet was sent from,
%and here's the top three predicted countries:
and the plot below shows the model's top three predicted countries and associated probabilities.

\noindent\input{img/infer/es-vosotros-hablar/0-lo-bar-country2.pgf}

%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    \resizebox{0.485\textwidth}{!}{\includegraphics{img/infer/es-vosotros-good/1-med-zoom}}
    \vspace{-1cm}

    \textbf{
    \caption{
    %The probability distribution of where our model locates the tweet in Example 1 below.
        For each tweet, our model outputs a distribution over the Earth's surface that represents the likelihood that the tweet was sent from that location.
        This map shows the distribution for the tweet \str{No me hablen} in Example 1 below.
        \fixme{Beautify.}
        \label{fig:poder}
    }
}
\end{figure}

%%%%%%%%%%%%%%%%%%%%

\noindent
This tweet was in fact sent from Spain as our model predicts.
Our model made this prediction by learning that the word \str{habl\'eis} is a conjugation of the verb \str{hablar} in the vosotros form,
which is only used in the Castilian Spanish dialect of Spain.
American Spanish dialects instead use the ustedes form instead,
which conjugates the verb as \str{hablen}.
If we modify the tweet to use the American Spanish dialect:
\begin{quote}
    \str{No me hablen}
\end{quote}
%
%word \str{hablen} instead of \str{habl\'eis},
then the model's predictions change accordingly:

\noindent\input{img/infer/es-vosotros-hablar/1-lo-bar-country2.pgf}

%\noindent
%Remarkably, our model learns this pattern even for words it has never been trained on before.
UnicodeCNN's ability to understand verb conjugations is quite robust.
For example, if we modify the original tweet by removing spaces (to get \str{Nomohabl\'eis}),
by removing the accent (\str{No me hableis})
or by dropping letters (\str{No me ableis}),
then our model still predicts the tweet was sent from Spain.
%For example, if we remove the second space to get
%\begin{quote}
    %\str{No mehabl\'eis}
%\end{quote}
%then our model still predicts that Spain is the most likely country, just with slightly less probability:
%
%\noindent\input{img/infer/es-vosotros-hablar/2-lo-bar-country2.pgf}
%
%\noindent
Remarkably, the UnicodeCNN even understands verb conjugations it has never seen before.
The Spanish verb \str{apresar} does not appear in our training data in either the vosotros form (\str{apres\'eis}) or the ustedes form (\str{apresen}).
%If we replace the run our model on the Spanish phrase
Nevertheless, our model correctly predicts that the Castillian phrase
\begin{quote}
    \str{No me apres\'eis}
\end{quote}
is from Spain:
%\begin{quote}
    %\str{No me apres\'eis}
%\end{quote}
%then our model still understands the tweet is using the vosotros form and should be located in Spain:

\noindent\input{img/infer/es-vosotros-hablar/11-lo-bar-country2.pgf}

\noindent
%And similarly, if we run our model on the American dialect
%\begin{quote}
    %\str{No me apresen}
%\end{quote}
%then our model still understands the tweet is from the Americas:
And that the Latin American phrase 
\begin{quote}
    \str{No me apresen}
\end{quote}
is from the Americas:

\noindent\input{img/infer/es-vosotros-hablar/12-lo-bar-country2.pgf}

There are many other geographical variations in Spanish verb conjugations which the UnicodeCNN understands,
but no previous work studying Spanish language geolocation takes advantage of these variations \citep{maier2014language,gonccalves2015learning,tinoco2017variation,han2014text}. 
%Spanish is the third most popular language on Twitter,
Many other less common (and less studied) languages also have sophisticated verb conjugation systems.
The UnicodeCNN, is able to learn conjugations in all of these languages in a simple, unified manner.

%In this case, our model was correct because the tweet was in fact sent from Spain.

\ignore{
How did our model realize this tweet wasn't from another Spanish speaking country like Mexico or Argentina?
The word \str{pod\'eis} is the most important clue.
\str{pod\'eis} is a conjugation of the verb \str{poder} in the vosotros form,
which is only used in the Castilian Spanish dialect of Spain.
All other Spanish dialects use the ustedes form, 
which would conjugate the verb as \str{pueden}.

%It is easy for standard tools such as \str{langid.py} \citep{lui2012langid} to identify that this tweet is written in Spanish.
%Using this language information, we can conclude
%that the tweet was likely sent from a country with a large Spanish-speaking population
%(such as Spain, Mexico, or the United States).


We can further refine our guess by using differences in the Spanish dialects spoken in these countries.
In this tweet, the word \str{pod\'eis} is the most important clue.
\str{pod\'eis} is a conjugation of the verb \str{poder} in the vosotros form.
The vosotros form is only used in the Castilian dialect,
which is only used in Spain.
All other Spanish dialects use the ustedes form, 
which would conjugate the verb as \str{pueden}.
%We therefore can (correctly) conclude that the tweet was sent from Spain.

%\begin{figure}
    %\resizebox{0.1\textwidth}{!}{\rotatebox{90}{\includegraphics{img/infer/bar-lang-placeholder}}}
    %\caption{test}
%\end{figure}

Our UnicodeCNN model correctly identifies that the previous tweet was sent from Spain,
as shown in the following graph:

\noindent\input{img/infer/ar-kuwait-japanese/0-lo-bar-country2.pgf}

\noindent
%To demonstrate that our model is able to distinguish between the words \str{pod\'eis} and \str{pueden},
%we create two artificial tweets
%\begin{figure}[H]
    %\centering
    %\includegraphics[width=0.45\textwidth]{img/infer/bar-lang-placeholder}
%\end{figure}
%And running the same model on the word \str{pueden} gives
If we were to instead replace the word \str{pod\'eis} in the tweet above by the word \str{pueden},
then our model predicts the following countries

\noindent\input{img/infer/ar-kuwait-japanese/0-lo-bar-country2.pgf}

\noindent
Remarkably, these patterns generalize to misspellings and words not present in the original training set.
For example, the Spanish word \str{fallec\'eis} is another verb conjugated in the Castilian vosotros form.
This word is not present in our training set,
yet if we replace the word \str{pod\'eis} by \str{fallec\'eis} the model still predicts that the tweet was sent from Spain.

\noindent\input{img/infer/ar-kuwait-japanese/0-lo-bar-country2.pgf}

\noindent
Previous geolocating systems do not understand these character level differences in dialect,
and they do not work in our highly multilingual context.
}

%\uniloc\ uses the above reasoning to correctly identify that this tweet was sent from Spain,
%and uses even subtler clues to create a detailed probability distribution over the exact GPS coordinates that the tweet is likely to have been sent from
%(see Figure \ref{fig:poder}).

%A key feature of \uniloc\ is that it was not preprogrammed with any linguistic knowledge.
%\uniloc\ was not preprogrammed with any linguistic knowledge,
%and learned this pattern from scratch using a novel character-level \defn{convolutional neural network} (CNN). 
%This CNN can take any Unicode character as input,
%and can learn similar patterns in any language.
%Our dataset includes the 65 languages officially recognized by the Twitter API,
%and an unknown number of unofficial languages as well.
%and it learns similar patterns for English, Arabic, Japanese, and the more than 65 languages present in our training data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item[Example 2.]
    This example shows how the UnicodeCNN can learn geographic knowledge in one language and transfer that knowledge to other languages.
Consider the following tweet sent from Kuwait written in a mixture of English and Arabic:
\begin{quote}
\str{I'm at }\foreignlanguage{arabic}{شارع المطاعم}\str{ in }\foreignlanguage{arabic}{الكويت}.%
\end{quote}
Translated fully into English, this tweet reads:
\begin{quote}
    \str{I'm at a street restaurant in Kuwait.}
\end{quote}
In this case, there is no need to analyze subtle linguistic clues to determine where the tweet was sent from because the location is written directly in the text.
To identify this tweet's location, all we need to do is understand that the Arabic word 
\foreignlanguage{arabic}{الكويت}
is the name of the country Kuwait.
Our model successfully does this, 
and its top predicted countries are

\noindent\input{img/infer/ar-kuwait-japanese/5-lo-bar-country2.pgf}

%\noindent
%Our model is able to successfully identify all countries and many large cities or other landmarks in this way.
%As you would expect, replacing the Arabic name for Kuwait 
%(\foreignlanguage{arabic}{الكويت})
%with any other country in the tweet causes the model to output that country with high probability.

Now consider the following plausible scenario:
A Japanese tourist visits Kuwait and sends a similar tweet but with the Arabic portions translated into Japanese.
The translated tweet is:
%The fact that the word Kuwait is written in Arabic should have little influence on where we guess the tweet was written.
%A good geolocation system should be able to understand messages like this easily.
%Unsurprisingly, this tweet is easy to geolocate to a region in Kuwait because the location information is directly encoded into the text.
%If we change the language that the tweet is written in,
%the geolocation problem should remain just as easy.
%For example, if we construct the artificial tweet
%So, for example, if we translate the Arabic word for Kuwait (\foreignlanguage{arabic}{الكويت}) into Japanese (\begin{CJK}{UTF8}{min}クウェート\end{CJK}) to get the tweet
%If we translate the Arabic in the previous tweet into Japanese, we get
\begin{quote}
%\str{I'm at }\foreignlanguage{arabic}{شارع المطاعم}\str{ in }\begin{CJK}{UTF8}{min}クウェート\end{CJK}.
\str{I'm at }\begin{CJK}{UTF8}{min}通りレストラン\end{CJK}\str{ in }\begin{CJK}{UTF8}{min}クウェート\end{CJK}.
\end{quote}
A good model should still guess this tweet was written in Kuwait despite the language change,
and our model does (but with lower confidence).
Its output is
%Clearly, no matter which language this tweet was written in, 
%we should still guess that it was written in Kuwait.
%Clearly, the language this tweet is written in should have no influence on where we guess the tweet was written.
%and in this particular case, we should still guess this Japanese tweet was written in Japan.
%The language used to write a location should have little influence on where we guess the tweet was written.
%and we should still predict that this Japanese translation was sent from Kuwait.
%(A tweet like this might be sent by a Japanese speaker on a work trip to Kuwait.)
%When given this Japenese tweet, our model's top three predicted countries are
%And our model still predicts that Kuwait is the most likely country for this Japanese tweet to have been sent from:

\noindent\input{img/infer/ar-kuwait-japanese/8-lo-bar-country2.pgf}

\noindent
This result is remarkable because the Japanese word for Kuwait (\begin{CJK}{UTF8}{min}クウェート\end{CJK}) never appears in our training data.
The UnicodeCNN learned that \begin{CJK}{UTF8}{min}クウェート\end{CJK} is Japanese for Kuwait by learning that this word sounds similar to the Arabic and English words for Kuwait.
    \ignore{
Furthermore, if we misspell the Japanese word for Kuwait (for example as \begin{CJK}{UTF8}{min}ウェート\end{CJK}),
then the UnicodeCNN still understands that the location referred to is Kuwait.
The model's output in this case is
%}
%\fixme{
\noindent\input{img/infer/ar-kuwait-japanese/9-lo-bar-country2.pgf}
}
\noindent
Similar transfers of knowledge happen for many other location and language combinations,
and more generally for loan words adopted from foreign languages.
Previous work has used \defn{gazeteers} (databases that map place names to GPS coordinates) to improve the quality of prediction \citep{zhang2014geocoding},
but these gazeteers are only usable in English or a limited number of other languages.

In this example, we chose to mix Japanese and English in a single tweet to demonstrate another advantage of the UnicodeCNN.
Many Asian languages (such as Chinese, Japanese, and Vietnamese) do not represent words using spaces,
and so prior work on multilingual tweet geolocation \citep{han2014text} used specialized lexical analyzer to extract words from Japanese and ignored all other languages with this property.
The UnicodeCNN, in constrast, does not require special lexical analyzers and works with all languages.
%Automatically extracting words from mixed language Japanese/English text is difficult because word boundaries in Japanese are not represented using spaces;
%they must be inferred from context.
%Previous work on geolocating Japanese tweets \citep{han2014text} used a special Japanese-only language parser.
%Special Japanese-only parsers have been developed for Japanese-only text,
%but they do not work on mixed languages texts.
%Our model, however, requires no such special parsers and works with any arbitrary combination of languages in a text.

%\begin{figure}[H]
    %\centering
    %\includegraphics[width=0.45\textwidth]{img/infer/bar-lang-placeholder}
%\end{figure}

%Japanese (\begin{CJK}{UTF8}{min}クウェート\end{CJK})
%The translation of Kuwait in Chinese
%\begin{quote}
%\end{quote}
%
%Chinese (\begin{CJK}{UTF8}{min}科威特\end{CJK})
%Russian (\foreignlanguage{russian}{Кувейт})
%Greek (\foreignlanguage{greek}{κουβέιτ})

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Our Model}
\label{sec:model}

\begin{figure*}
    \centering
    %\fbox{\parbox{\textwidth}{\vspace{2in}}}
    \resizebox{\textwidth}{!}{\input{fig/model}}
    \textbf{
    \caption{
        The UnicodeCNN adds a novel character embedding and language estimator to the \defn{character level convolutional neural network} (CLCNN) \citep{zhang2015character}.
        For the task of geolocation, we add two outputs to predict the country using the standard cross entropy loss and the GPS coordinates using a novel mixture of Fisher-von Mises distributions.
    %Structure of our UnicodeCNN model.  
    %Blue layers require supervision. 
    %We emphasize that the UnicodeCNN component is application agnostic,
    %and only the rightmost output layers are used for our geotagging application.
    %Other output layers can be used for other applications through transfer learning.
    %\fixme{beautify.}
    \label{fig:model}
    }
    }
\end{figure*}

The structure of our model is shown in Figure \ref{fig:model}.
The UnicodeCNN transforms the input text through four stages:
a character encoder,
convolutional layers,
a language estimator,
and a feature mixing layer.
The model is divided into two stages.
The output of the UnicodeCNN is a set of application agnostic features,
and for our geolocation application we use a standard cross entropy layer to estimate the country and our novel mixture of Fisher-von Mises layer to estimate the GPS location.
Other output layers can easily be used for other applications by either training from scratch or using transfer learning \citep{wang2015transfer,howard2018fine}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The UnicodeCNN}

We first present necessary background from the Unicode standard \citep{Unicode},
then we describe the four states of the UnicodeCNN feature generation.
%We now describe each of the UnicodeCNN's four stages in detail.
%The first step of our model is to convert the text of a tweet into a $d\times\ell$ matrix using a novel character hashing strategy.
%The UnicodeCNN is divided into four steps.
%First, the characters are encoded into a matrix;
%Second, this matrix is the input to a series of convolutional layers;
%these convolutional layers feed a language predictor module;
%and the language predictor and convolutional layers together generate the final features.

\ignore{
Our encoding method has three important properties:
\begin{enumerate*}
    \item 
    Our encoding supports all Unicode code points%
    \footnote{
        A \defn{code point} is the basic unit of Unicode.
        Each ready-made character, diacritic mark, and combining instruction is represented by a different code point.
        %We very briefly describe relevant aspects of the Unicode standard,
        %but refer the reader to the official documentation \citep{Unicode} for further details.
    }.
    This is in contrast to previous work using character-level CNNs that only support English characters \citep{zhang2015character,conneau2017very}. 
    Supporting all Unicode code points is the fundamental improvement that lets our model use any language as input.
    \item 
    Our encoding is necessarily lossy because the Unicode standard can encode more than one million possible code points,
    whereas previous encodings could be lossless because they supported only the small English alphabet.
    Our experimental results show that a lossless encoding is not necessary.
    The later stages of the model are able to disambiguate encoding collisions from context.
    \item
    Our encoding associates similar encodings to similar characters from different scripts.
    %For example, the Cyrillic character П has a similar representation to the Latin P because they correspond to similar sounds.
    This helps ensure that our model is robust to both misspellings and transliteration between writing systems.
    %\item
    %Some complex code points have longer encodings than simple code points.
    %For example, the Japanese character
\end{enumerate*}
}

\begin{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item[Unicode Background.]
%\citep{Unicode}
The fundamental building block of Unicode strings is called a \defn{code point}.
A code point is a number (typically written as \str{U+} the number's hexadecimal representation)
that represents either a character or formatting command.
For example, the code point \str{U+004F} represents the character \str{O}.
Complex characters can be encoded directly or as a combination of simple letters plus formatting commands called \defn{marks}.
For example, the Vietnamese character \str{\foreignlanguage{vietnam}{Ớ}} can be represented by the single code point \str{U+1EDA},
%or by the sequence of code points \str{U+004F} (\str{O}) \str{U+031B} (\str{◌̛}) \str{U+0301} (\str{◌́}).
or by the sequence of code points \str{U+004F} (Latin capital \str{O}) \str{U+031B} (combining horn) \str{U+0301} (combining acute accent),
or a number of other strings.
Each of these strings is \defn{semantically equivalent},
because they represent the same character.
\defn{Normalization} is the process of converting semantically equivalent strings into a canonical form,
and the Unicode standard defines several standard normalization procedures.

A Unicode \defn{encoding} associates each code point with a binary representation. 
UTF-8 is a popular encoding that uses 1 byte to represent Latin characters,
2 bytes to represent most other European and Arabic characters,
and 3 bytes to represent most Asian characters.
Previous work on Unicode aware deep learning systems worked directly on UTF-8 encoded representations of the input string \citep{gillick2015multilingual,plank2016multilingual}.
This is a simple method for incorporating Unicode inputs into a learning pipeline,
but it has several disadvantages.
First, strings written in English will have simpler encodings than strings written in other languages,
and so the learning pipeline will be biased to produce worse results for non-English languages.
Second, Unicode's linguistic knowledge of normalization, transliteration, and character semantics is not used in the learning pipeline.
Our character encoder, however, works at the code point level.
It has reduced bias for English language strings and takes full advantage of Unicode's many features.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item[Character Encoder.]
The character encoder converts the input text into a $280\times d$ encoding matrix,
where each entry is either 0 or 1,
and $d$ is a hyperparameter that specifies the size of the encoding.
The number of rows is set to 280 because this is the maximum number of characters in a tweet.
Most Unicode characters are encoded as a single row in the encoding matrix,
but certain complex characters (e.g.\ Chinese, Japanese, and Korean characters) get encoded into multiple rows.
Twitter limits the size of tweets in these languages to only 140 characters,
so tweets in these languages still typically fit in the 280 row encoding matrix.

%We now describe how the encoding matrix is generated.
The first step to generating the encoding matrix is to normalize the string's representation using Unicode's NFKC normalization strategy.
%This normalization is needed because many strings have multiple equivalent encodings in Unicode.
Some Unicode strings can be represented in multiple ways,
and normalization converts all these representations into a single canonical form.
For example, consider the Vietnamese character \str{\foreignlanguage{vietnam}{Ớ}}.
This character can be encoded using the \str{U+1EDA} code point directly,
the code points \str{U+00D3} \str{U+031B}
(which corresponds to the accented \str{Ó} and a modifying mark),
the code points \str{U+004F} \str{U+031B} \str{U+0301}
(which corresponds to the Latin \str{O} and two modifying diacritic marks),
or many other combinations.
NFKC normalization converts all of these strings into the single \str{U+1EDA} code point.
%Normalizing the text ensures that all of these code point combinations get converted into a single canonical form.
%Different normalization strategies consider different code point combinations equivalent,
%and we choose the NFKC strategy because it combines code points most aggressively.

%Next, we process each normalized code point individually.
%Most code points are assigned a $d$-bit encoding,
%but certain complex code points are assigned a $2d$ or $3d$ bit encoding.
Next, we loop through each code point in the normalized string and update the corresponding rows in the encoding matrix.
We determine the number of rows a code point occupies as follows:
%Every non-character code point gets a single row.
%For character code points,
If the code point is not a letter, then it gets a single row in the encoding matrix.
Otherwise, we first transliterate the letter into the Latin script.
The number of characters in the transliteration is the number of rows the code point gets in the matrix.
For example, the Vietnamese character \str{\foreignlanguage{vietnam}{Ớ}} is transliterated into \str{O} and so gets only a single row,
but the Chinese character \begin{CJK}{UTF8}{min}東\end{CJK} gets transliterated into \str{dong} and so occupies 4 rows.

The contents of each row are set so that similar letters will generate similar rows.
The procedure is:

\begin{description}[font=\normalfont\itshape]
\item[Columns 1-7:]
Each code point is assigned a \defn{Unicode category},
which can be one of the following seven options:
letter,
mark,
number,
punctuation,
symbol,
separator,
or other.
These columns are a 1-hot encoding of the Unicode category.
This means that each category is assigned a column.
The column corresponding to the code point's category is set to 1 and all other columns to 0.
%A one-hot encoding of the code point's \defn{Unicode category}.
%Each code point is assigned one of seven possible categories:

\item[Column 8:]
Set to one if the code point is either an upper case or title case character,
and zero otherwise.

\item[Columns 9-13:]
%A one-hot encoding of the \defn{Unicode directionality}.
Each code point can have one of five possible Unicode directionalities:
strongly left-to-right (e.g.\ Latin letters),
strongly right-to-left (e.g.\ Arabic or Hebrew letters),
weak (e.g.\ numbers),
neutral (e.g.\ paragraph separators),
and explicit formatting commands.
These columns are a 1-hot encoding of the Unicode directionality.

\item[Columns 14-29:]
These columns encode diacritic marks on letter code points as follows.
The code point is decomposed into a ready-made character and combining marks using the NFD normalization scheme.
For example, the \str{U+1EDA} code point (\str{\foreignlanguage{vietnam}{Ớ}}) is decomposed into the Latin \str{O} and the two marks \str{U+031B} and \str{U+0301}.
Each mark is assigned a number between 0 and 15 by first multiplying the code point's value by a large prime, 
then taking the remainder mod 16.
Then, column $14+r$ is set to 1 for each mark.
This part of the encoding is lossy because many diacritic marks will be assigned the same encoding.

\item[Columns 30-31:]
    Column 30 is set to 1 if the character is \str{\#},
    and column 31 is set to 1 if the character is \str{@}.
    These characters have special importance in Twitter (indicating hashtags and mentions),
    and so are important enough to have their own column.

\item[Columns 32-57:]
    These columns encode the romanization of the code point.
    Column 32 is set to 1 if the romanization of the code point is \str{a},
    column 33 if the romanization is \str{b},
    and so on until column 57 if the romanization is \str{z}.

\item[Column 58:]
    Set to 1 if the character is actually from the Latin alphabet.

\item[Column 59:]
    Set to 1 if the character is the first character in a transliteration string.

\item[Columns 60-$d$:]
    If the character is from the Latin alphabet (i.e.\ Column 58 is 1), 
    then all these bits are set to 0.
    Otherwise, the code point is multiplied by a large prime and the remainder $r$ with respect to $d-60$ is taken.
    Column $60+r$ is then set to 1 and all others to 0.
\end{description}

\ignore{
A weakness of our transliteration scheme is that certain characters should be transliterated in different ways depending on context,
but we ignore context.
The simplest type of context is the language of the text.
Different languages pronounce different characters differently,
and hence the character should be transliterated differently.
For example, the Chinese character \begin{CJK}{UTF8}{min}東\end{CJK} (which means East) should be transliterated as \str{dong} if the text is written in Mandarin Chinese 
(using the Pinyin transliteration scheme),
as \str{dung} if the text is written in Cantonese Chinese 
(using the Jyutping transliteration scheme),
or as \str{tang} if the text is written in Hokkien Chinese 
(using the POJ transliteration scheme).
Sometimes, however, just knowing the language is not enough information to effectively transliterate.
For example, the Japense Kanji writing system uses Chinese characters,
but the pronunciation of those characters changes from word to word.
The two most common pronunciation schemes are the Onyomi and Kunyomi,
which transliterate \begin{CJK}{UTF8}{min}東\end{CJK} as \str{tou} and \str{higashi} respectively.
Certain proper nouns, however, use the rarer Nanori pronunciation, which for  \begin{CJK}{UTF8}{min}東\end{CJK} can be any of \str{ai}, \str{agari}, \str{ko}, \str{saki}, \str{shino}, \str{hajime}, \str{higa}, or \str{moto} depending on the name.
These difficulties are most pronounced in East Asian languages,
and a more sophisticated transliteration scheme could therefore possibly improve performance for these languages.
}

\begin{table}
    \begin{tabular}{l|ccc}
        Hyperparameter & Small & Large & Huge \\
        \hline
        & \\ [-1.0em] 
        Encoding Size ($d$) & 128 & 128 & 256 \\
        Convolutional channels & 256 & 1024 & 2048 \\
        Hidden layer size & 1024 & 2048 & 4096 \\
    \end{tabular}
    \textbf{
    \caption{
        Hyperparameters for the three variants of the UnicodeCNN.
        \label{table:hyperparam}
    }}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item[Character Convolutions.]
    The character encoding matrix is passed as the input to a series of six temporal convolutional layers.
    Temporal convolution is a standard deep learning method,
    and a full description is beyond the scope of this paper.
    Here, we briefly describe the intuition behind these convolutional layers and state the parameters that we use.
    We refer the reader to %the \emph{Deep Learning Book} \citep{Goodfellow-et-al-2016} and 
    the original paper on character level convolutional neural networks (CLCNNs) \citep{zhang2015character} for mathematical details.

    Intuitively, a convolutional layer generates a new ``higher level'' set of features from the input ``low level'' features.
    These higher level features do not make any independence assumptions of traditional language models and are known to be robust to small variations in spelling.
    These higher level features are formed by convolving a suitable \defn{filter} with the input matrix, 
    then optionally performing \defn{max-pooling}.
    There are two important parameters: the number of channels and the width.
    The number of channels determines the number of filters that are applied in parallel,
    each creating its own set of output features.
    Increasing the number of channels increases the model's ability to detect important linguistic features,
    but it also increases the computational complexity of the model and the model's ability to overfit the data.
    Our Large and Small models use the same number of channels as the original CLCNN \citep{zhang2015character},
    and our Huge model uses twice as many channels.
    We are able to use such a large model because our massive dataset prevents overfitting.
    %and we train our model in parallel using 8 GPUs (see Section \ref{sec:experiments}).
    The width of a convolutional or max-pooling layer determines the number of input features that get combined into a single output feature.
    %Increasing the width of a convolutional layer increases model capacity 
    %(again at the expense of computation time and overfitting),
    %whereas increasing the width of a max-pooling layer decreases model capacity.
    All three sizes of our model use the same size widths as the original CLCNN.
    The parameters for each convolutional layer are shown in the table below:

    %The following table describes the parameters of the 6 convolutional layers.
    %Each model size uses a different number of channels, but the same widths.

    \begin{table}[H]
    %\begin{tabular}{c|ccc|cc}
        %& \multicolumn{3}{|c|}{Channels} & \multicolumn{2}{c}{Width} \\
        %Layer & Huge & Large & Small & Kernel & Max-Pool \\
        %\hline
        %1 & 2048 & 1024 & 256 & 7 & 3 \\
        %2 & 2048 & 1024 & 256 & 7 & 3 \\
        %3 & 2048 & 1024 & 256 & 3 & N/A \\
        %4 & 2048 & 1024 & 256 & 3 & N/A \\
        %5 & 2048 & 1024 & 256 & 3 & N/A \\
        %6 & 2048 & 1024 & 256 & 3 & 3 \\
    %\end{tabular}
    \begin{tabular}{ccc}
        %& \multicolumn{3}{|c|}{Channels} & \multicolumn{2}{c}{Width} \\
        Layer & Kernel Width & Max-Pooling Width \\
        \hline
        & \\ [-1.0em] 
        1 & 7 & 3 \\
        2 & 7 & 3 \\
        3 & 3 & N/A \\
        4 & 3 & N/A \\
        5 & 3 & N/A \\
        6 & 3 & 3 \\
    \end{tabular}
    \end{table}

    %\noindent
    %An important property of the output features is the number of input characters that had a role in generating the feature.
    %The formula for calculating this is $1 + \sum(\text{width of each layer} - 1)$.
    %In our case, this comes out to be 27.

%We use a \defn{convolutional neural network (CNN)} to generate character-level features.
%Specifically, our model is a variant of the CLCNN \citep{zhang2015character}.
%The input encoding matrix is passed to 

\ignore{
Alternative character-level models use deeper CNNs with resnet connections \citep{conneau2017very}, recurrent neural networks (RNNs) \citep{chung2016character}, 
or complex combinations of CNNs and RNNs \citep{kim2016character,jozefowicz2016exploring}.
Each of these techniques requires considerable processing resources, however, 
so we did not have the resources to exhaustively compare these techniques to each other on this task.
We chose to base our model off of the CLCNN model because it had the best performance on a small held-out subset of tweets.
All of these techniques were originally designed to work on relatively small mono-lingual corpuses.
The largest dataset used in the papers above is \fixme{},
whereas our dataset is \fixme{}.

Deep learning techniques have also been applied to Twitter data.
%Character-level models have previously been applied to twitter NLP tasks \citep[e.g.][]{dhingra2016tweet2vec,severyn2015unitn},
%but never to the geolocation problem.
For example, 
\citet{severyn2015unitn} use a word level CNN for sentiment classification on tweets,
and \citet{dhingra2016tweet2vec} use a character level GRU network to predict hashtags.
Ours is the first application of deep learning techniques to the geolocation problem,
uses the largest dataset,
and the first multilingual dataset.
%Their dataset contains only 2 million tweets, whereas our dataset contains \fixme{}.
}

%\item[Word-level features.]
%All previous work on twitter geolocation has essentially relied on hand-crafted world-level features.
%\citet{han2012geolocation} study a method of determining location indicative words using an information gain strategy.

\item[Language Encoder.]
    The output of the convolutional layers is passed to two fully connected ReLU layers.
    For all three model sizes we set the width of these layers to 1024.
    The fully connected layers are then passed to a softmax layer which predicts the language of the tweet.

    Training the model requires that the text be labeled with the language in some way.
    For unlabeled text, standard language predictors such as \str{langid.py} \citep{lui2012langid} can be used,
    but the Twitter API associates a language with each tweet and we use this label.
    The Twitter API labels each tweet with one of 65 officially supported languages or \str{Unknown} if it cannot determine a language.
    For example, Croatian is not an officially supported language,
    and so tweets written in Croatian get classified as \str{Unknown}.
    Twitter's language labels are rather noisy.
    Short tweets are likely to get mislabelled,
    and many tweets in unsupported languages get misclassified as a supported language.
    For example, 
    tweets written in Catalan get classified as Spanish,
    and tweets written in Malay get classified as Indonesian.
    Our fully trained system estimates the Twitter API's language labels with about 95\% accuracy for all three UnicodeCNN sizes,
    and we suspect this is close to optimal.

    Why not just directly use labels provided by the Twitter API or a language classifier?
    Two reasons:
    First, we believe our learned language labels are more accurate than the Twitter API's.
    Second, language classifiers like \str{langid.py} are slow.
    Language classifiers use their own alternate pipeline to generate features from text,
    but we can simply reuse the feature generation pipeline we've already created to avoid duplicated work.

    \ignore{
There are two ways to incorporate language of a tweet into our model.
The first is to use the Twitter API's language field directly 1-hot encoded.
This is a simple technique, but has three drawbacks:
(i) the Twitter API supports many fewer languages than are actually used on Twitter;
(ii) the Twitter API occasionally makes mistakes on which language the tweet was sent in;
(iii) Twitter uses an undisclosed algorithm to determine the language,
and this algorithm is likely to leak user-level meta-information.

An alternative is to try to learn the language directly.

\fixme{
We use \str{langid.py} \citep{lui2012langid} to count the number of known languages in our dataset.
\str{langid.py} supports 97 languages, and all 97 were identified in our dataset.
We additionally identified tweets written in three languages not supported by \str{langid.py}: 
Burmese, Maldivian, and Sindhi.
In total, our dataset contains at least 100 languages, 
and likely many more that we have not identified.
}

%The \str{lang} field of a tweet is determined automatically by the twitter software using a combination of unpublished tweet and user level features.
%\citet{blodgett2016demographic} show that existing language prediction tools such as \str{langid.py} \citep{lui2012langid} perform poorly relative to the \str{lang} field of the json object.
%This effect is exacerbated when the tweets exhibit dialectical differences from the formal version of their language.
%\end{description}

%\item[Language.]
The Twitter API associates with each tweet one of 65 possible officially supported languages.
The most popular language is English with $405\times10^6$ tweets,
and the least popular language is Uighur with only 157 tweets.
Many other languages are also present in our dataset, however.
$74\times10^6$ tweets are classified as being from an undefined language.
This can happen when the tweet contains too little text to determine the language,
or when the language is not one of Twitter's officially recognized 65 languages.
%For example, we have identified that the set of tweets with undefined language contains tweets in Czechoslovakian.
For example, Croatian is not an officially supported language,
and so tweets written in Croatian get classified as having an undefined language.
Furthermore, many unofficial languages get miscategorized into similar languages that are officially recognized.
For example, 
tweets written in Catalan get classified as Spanish,
and tweets written in Malay get classified as Indonesian.
Because the dataset is so large,
we make no effort to exhaustively identify all the languages it contains.
Previous efforts to categorize the number of languages found on Twitter fouond more than 100 different languages \citep{hong2011language}.
\fixme{Section XXX of the supplement shows examples of our language-agnostic model discovering these unofficial languages by itself and exploiting them for geolocating.}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item[Feature Mixing.]
The feature mixing stage consists of another two fully connect ReLU layers.
The size of these layers is shown in Table \ref{table:hyperparam}.
The input to these ReLU layers is a concatenation of the output of the convolutional layers and the softmax language estimate.
The output of the feature mixing layer is the features generated by the UnicodeCNN,
and these are passed to the output layers.

%Our model uses two fully connected layers with 2048 hidden units each and relu activation functions.
%To reduce overfitting, each of these layers is trained using dropout and a 50\% keep probability \citep{}.

%\begin{table}[H]
%\begin{tabular}{c|ccc}
    %& \multicolumn{3}{c}{Output Units} \\
    %Layer & Huge & Large & Small \\
    %\hline
    %7 & 4096 & 2048 & 1024 \\
    %8 & 4096 & 2048 & 1024 \\
    %%9 &  &  &  \\
%\end{tabular}
%\end{table}
\end{description}

%%%%%%%%%%%%%%%%%%%%

\subsection{Model Output}

Our model has two output layers that indicate the location of the tweet at two levels of granularity.

\begin{description}

    \item[Country Cross Entropy.]
        Determining which country the tweet was sent from is a standard classification problem,
        and we therefore use the standard softmax cross entropy loss.

    \item[Mixture of Fisher-von Mises.]
        %As discussed in Section \ref{sec:problems:output},
        %determining the exact GPS coordinates of a tweet is not a classification problem,
        %and previous work either chose not to predict GPS coordinates 
        %or used an incorrect model to predict GPS coordinates that did not consider the non-Euclidean nature of the earth's surface.
        %In this section, 
        %we introduce a novel output layer for our network called the \defn{Mixture of Fischer-von Mises distributions} (MFvM layer).
        %In order to predict the exact GPS coordinates, 
        %we use the mixture of Fisher-vom Mises distributions (MFvM) layer.
        %The MFvM layer outputs a probability distribution over the earth's surface for each tweet.
        %A point estimate of the tweet's location can be computed using the maximum likelihood.

        %The MFvM layer is inspired by the field of \defn{directional statistics},
        %which studies distributions on the surface of hyperspheres.

        Previous work attempting to predict the exact GPS coordinates of tweets \citep{duong2016near} models GPS coordinates in Euclidean space,
        but this fails for two reasons.
        First, two points near the poles can have small distance but large difference in GPS coordinates.
        Second, two points on opposite sides of the international date line will have small distance but large difference in GPS coordinates.
        We now present a method for predicting GPS coordinates that does not suffer from these problems because it accounts for the Earth's spherical geometry.

        The \defn{Fisher-von Mises} distribution is one of the standard distributions in the field of \defn{directional statistics},
        which is the study of distributions on hyperspheres.
        The FvM can be considered the spherical analogue of the Gaussian distribution \citep[e.g.][]{mardia2009directional} and enjoys many of the Gaussian's nice properties.
        Thus, mixtures of FvM distributions can be seen as the spherical analogue of the commonly used mixture of Gaussian distributions.
        While the mixture of FvM distributions has previously been used in deep learning for clustering \citep{gopal2014mises} and facial recognition \citep{hasnat2017mises},
        we believe we are the first to apply it to predicting GPS coordinates.

        %The optimization of the mixture of FvM distributions is closely related to optimization of  a mixture of Gaussians, % \citep{variani2015gaussian},
        %except the Gaussian density is replaced by the FvM density.
        We now formally describe the mixture of FvM layer.
        Let $\mathbb{S}^2 = \{ \x \in \mathbb R^3 : \ltwo{\x}=1 \}$ denote the unit sphere.
        Then the density of the FvM distribution is given by
        \begin{equation}
            \text{FvM}(\x;\mu,\kappa) = \frac{\kappa}{\sinh \kappa} \exp(\kappa\trans\mu\x),
        \end{equation}
        where 
        $\x$ is any point in $\mathbb{S}^2$,
        $\mu\in\mathbb{S}^2$ is called the \defn{mean direction},
        $\kappa\in\mathbb R$ is called the \defn{concentration parameter}.
        The density of a mixture of $q$ FvM distributions is given by
        \begin{equation}
            \text{MFvM}(\x) = \prod_{i=1}^q \text{FvM}(\x;\mu_i,\kappa_i)^{\w_i}
        \end{equation}
        where each mixture distribution $i$ has its own mean direction $\mu_i$ and concentration parameter $\kappa_i$, 
        and the $\w_i$ represent the mixture weights.
        Each $\w_i\in(0,1)$ and $\sum_{i=1}^q \w_i = 1$.
        The associated log loss is 
        \begin{equation}
            \ell(\x) = -\log\sum_{i=1}^q {\w}_i \text{FvM}(\x;\mu_i,\kappa_i),
        \end{equation}
        and this is the expression we optimize.
        A linear function maps the features generated by the UnicodeCNN onto the $\w_i$ mixture components.
        Thus, the features extracted from the text determine the relative weight of each component in the mixture,
        but does not determine the mean direction or the concentration parameters of the components.
        Each $\kappa_i$ is initialized to $\exp(10)$,
        which corresponds to a standard deviation for each component of \fixme{50km}.
        %The $\omega_i$, $\kappa_i$, and $\mu_i$ parameters are all trainable.
        %To speed up convergence, we initialize $\omega_i=1/q$ and 
        %$\kappa_i=\exp(10)$.
        The $\mu_i$ are initialized by setting it equal to the location on the sphere of the world's $i$th most populated city.
        In our model, we use $q=10000$ mixture components. 

        After training, the values of each $\mu_i$ and $\kappa_i$ are fixed.
        Inputing a text string fixes the UnicodeCNN's features and thus the $\w_i$,
        and thus the value of the distribution is completely determined.
        Figure \ref{fig:poder} plots the density of an example distribution.
        A point estimate can be computed using the maximum likelihood:
        \begin{equation}
            \hat\x = \argmax_{\x\in\mathbb{S}^2} \text{MFvM}(\x).
        \end{equation}
        In our experiments below, we measure the quality of our point estimate by measuring the distance between the tweets true location and $\hat\x$ on the surface of the sphere using the Vincenty's distance formula \citep{vincenty1975direct}.

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ignore{
\subsection{Prediction losses}

Our general framework has two types of outputs:
discrete outputs that estimate the \place and \country fields of the tweet,
and continuous outputs that estimate the \geo field.
%Whereas previous work requires a number of assumptions about the output fields,
%our work makes few assumptions.

\begin{description}
\item[Discrete outputs.]
The discrete outputs are conceptually the simplest,
and for this reason they are the most commonly used in previous work.
The idea is to treat a tweet's \place and \country fields as labels for the tweet,
then perform ordinary multi-label classification using the cross entropy loss.
Our work differs from previous work because no previous work has attempted to directly predict a tweet's \place field.
Because there are millions of distinct \place entries in twitter,
this task was considered too hard.
Instead, previous work constructed \pseudoplace labels that filter out uncommonly seen entries and merge them into other nearby locations.
This reduces the number of class labels from millions down to thousands,
making a much easier problem.

There is no standard method for constructing these \pseudoplace labels in the literature.
\citet{han2012geolocation} proposed a more sophisticated method.
Their method combines suburbs with nearby cities,
and identifies a total of 3709 cities to use as class labels.
Those tweets that do not originate from a city are either discarded or assigned to the closest city.
More complicated methods of discretizing the Earth's surface have also been developed for non-Twitter applications.
For example, Google developed a large scale system for geolocating images that uses specially designed partition of the earth's surface that they call S2 \citep{weyand2016planet}.

\item[Angular regression.]
We introduce novel methods for predicting the \geo field that take advantage of the non-euclidean nature of the earth's surface.
\citet{duong2016near} is the only previous work to attempt to estimate the \geo field of a tweet.
They use the ordinary least squares regression model with the standard $L2$ loss between GPS coordinates:
\begin{equation}
    \latd^2 + \lond^2
    .
\end{equation}
Unfortunately, the surface of the earth is highly non-euclidean, 
and the OLS model is known to work only in the euclidean setting \citep[e.g.][]{fisher1992regression}.
For example:
\begin{enumerate}
    \item
        There is a difficulty at the antimeridian (180 degrees west).
        The antimeridean approximately separates Russian from Alaska.
        The true distance between these locations is very short (about 100km),
        but the distance using the L2 norm is very large.
    \item
        Second, the standard $L2$ loss does not accurately capture the distance between GPS coordinates.
        For example, at a latitude of 80 degrees north, 1 degree of longitude is approximately equal to 20 km;
        but at the equator, 1 degree of longitude is approximately equal to 111 km.
\end{enumerate}

The so-called \emph{angular regression} fixes these problems \citep{fisher1992regression}.
\citet{fisher1992regression} was the first to propose a method for regression onto the surface of a sphere.

The \emph{great circle distance} is a better distance metric for gps coordinates because it is the distance of the shortest path between two points on the earth's surface.
The naive computation of the great circle distance is unfortunately numerically unstable and cannot be used directly.
Fortunately, \citet{vincenty1975direct} proposed a numerically stable method for computing the GCD which we use in our work.
Figure \ref{fig:vincenty} shows the formula.

\begin{figure*}
    \centering
    %\begin{align}
    $
    \displaystyle
        \delta\sigma 
        =
        \arctan\left(
            \frac
            {\sqrt{(\cos\lata\cdot\sin\lond)^2 + (\cos\lata\cdot\sin\latb-\sin\lata\cdot\cos\latb\cdot\cos\lond)^2}}
            {\sin\lata\cdot\sin\latb + \cos\lata\cdot\cos\latb\cdot\cos\lond}
        \right)
    $
    %\end{align}
    \caption{The Vincenty formula for numerically stable great circle distances.}
    \label{fig:vincenty}
\end{figure*}

\item[Mixture of FvM.]

\end{description}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{sec:experiments}

We now describe our dataset, 
training and evaluation method, 
baseline comparison models, 
and results.

\begin{description}
        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item[Dataset.]
Our dataset contains all tweets with geolocation information sent between 26 October 2017 and 08 July 2018.
The dataset contains approximately 900 million tweets written by $3.0$ million users.
Unlike previous work, 
we perform no filtering to remove ``hard'' tweets from the dataset,
and as a result our dataset is orders of magnitude larger than previously used datasets.

%The WORLD dataset \citep{han2012geolocation} contains $1.4\times10^6$ users and $12\times10^6$ tweets.
The largest previously used dataset for geolocating tweets was previously the WORLD+ML dataset \citep{han2014text},
which contains only $23$ million tweets written by $2.1$ million users.
WORLD+ML includes tweets in all languages,
but removes hard-to-geolocate tweets that are not close to major cities.
Whereas the WORLD+ML dataset contained only 47\% non-English tweets,
our dataset contains 68\% non-English tweets.
Other datasets such as the WNUT \citep{han2016twitter}, WORLD \citep{han2012geolocation}, and NA \citep{roller2012supervised}
additionally remove non-English tweets and so are considerably smaller.

For each tweet in our dataset, we associate a country and GPS coordinates using the Twitter API.
The Twitter API defines a total of 247 unique country codes that a tweet can be sent from.
This number is larger than the number of sovereign states recognized by the United Nations (206)
because many non-countries (e.g.\ Puerto Rico and Hong Kong) are given country codes.
The process of assigning GPS coordinates is slightly more complicated.
User privacy settings allow Twitter to share the exact GPS coordinates of approximately 16\% of tweets in our dataset.
For the remaining tweets, privacy settings restrict the accuracy that the tweet's location can be shared to a higher level, typically the city.
In this case, we take the center of mass of the associated city as the true GPS location of the tweet.
There are approximately 3 million unique city-level locations reported by the Twitter API.
The WNUT \citep{han2016twitter}, WORLD \citep{han2012geolocation}, and WORLD+ML \citep{han2014text} datasets generated class labels using a complex procedure to:
(i) select approximately the 3000 most populated of these cities,
(ii) combine them with nearby cities into a single metropolitan area which serves as the class label,
and (iii) discard all tweets from cities not from these metropolitan areas.
Clearly, significant amounts of information are lost when creating class labels in this way.
Our method of using the GPS coordinates preserves as much information from the tweet as possible and creates a more challenging prediction problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    %\resizebox{0.225\textwidth}{!}{\input{img/country}}
    %\centering

    \hspace{0.5in}
    \resizebox{0.45\textwidth}{!}{\input{img/lang}}
    \hspace{-0.5in}

    \textbf{
    \caption{
        Language distribution of tweets in our dataset.
        Most tweets are not written in English,
        but prior work focuses on this special case.
        (*) Twitter classifies approximately 8\% of tweets in our dataset as having an unknown language.
        This may be because the tweet is written in a language that Twitter does not officially support,
        or because the tweet has too little text.
        %Most tweets are neither located in the United States nor in English,
        %but prior work focuses on these two special cases.
        \fixme{beautify}
    }
    }
    \label{fig:country/lang}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}
\input{img/tables/all}
%\input{img/tables/all_minus_en}
\textbf{
\caption{
    The UnicodeCNN features give the best results across all measures,
    with the larger UnicodeCNN's giving better results than the smaller ones.
    Recall that smaller values indicate better performance for average distance and larger values are better for accuracy.
    \label{table:results}
}
}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item[Baseline Models.]

We compare our UnicodeCNN with three baseline models.
All three models predict the country and gps coordinates a tweet was sent from,
the only difference is the way the features are generated.

The first model, \str{lang} uses only the language of the tweet 1-hot encoded as an input feature.
This is the simplest reasonable model for the multilingual geolocation problem.
This model is able to capture the fact that different countries use different languages,
but is unable learn patterns about how different regions use different dialects of the same language or discuss different topics.

The second model, \str{lang+time} uses the language features above and time features.
Many languages (e.g.\ Spanish) are used in different countries,
and so the language alone cannot discriminate between these countries.
%Combining language and time
%The time that the tweet was sent may be able to disambiguate these countries,
%however.
We generate features from the day of week and time of day that the tweet was sent,
and the \str{lang+time} model uses these time features in addition to the language features.

%Neither the \str{lang} nor \str{lang+time} models directly inspect the text of the tweet.
The final model, \str{lang+time+bow} adds bag-of-words features to the model generated from the text.
Bag-of-words features are a classic method for analyzing English-language text corpora,
and our model follows best practices.
In particular, we use feature hashing \citep{weinberger2009feature} with L1 regularization to automatically select the best features.
Bag-of-words features are the most popular features used in content-based tweet geolocation,
and they have been used in all the following papers:
\citep{cheng2010you,li2012towards,han2013stacking,mahmud2014home,compton2014geotagging,zhang2014geocoding,rahimi2015twitter,dredze2016geolocation,rahimi2017neural}.

\fixme{
The results from these papers are not directly comparable to the results in our paper for two reasons.
First, we consider a harder geolocation problem than previous papers.
We perform no filtering of the dataset to remove difficult tweets,
and we attempt to determine exact GPS positions rather than just nearby cities.
Second, many of the previous papers augment the bag-of-words features using special preprocessing of the text that is only applicable to the English language.
For example, \citep{} use an English-language gazetteer to additionally create features for locations that appear within the text,
and \citep{} use the English-only lexical analyzer XXX \citep{}.
}

%\begin{figure*}
%\noindent\input{img/hr-en}
%\noindent\input{img/hr-es}
%\textbf{
%\caption{
    %Different countries tweet in different languages and at different times of the day.
    %For example, tweets sent in English are almost always sent from the United States,
    %except around 9AM UTC (2AM PST).
    %Tweets sent in Spanish are likely to be sent from either Argentina (from 2200UTC-0400UTC), Mexico (0500-0600UTC), or Spain (0600-2200UTC).
    %\fixme{Remove whitespace.}
%}
%}
%\label{fig:time-lang}
%\end{figure*}

\fixme{
Metadata associated with each tweet has also been used to generate features,
and this metadata can significantly improve geolocation performance.
For example, 
\citep{hecht2011tweets,schulz2013multi,han2014text}.
And the social graph \citep{}.
We do not compare against these models, however, because we are only interested in content-based geolocation.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\item[User Metadata.]
%The JSON object also contains significant information about the user who sent the tweet.
%This metadata can be used to greatly increase the accuracy of geolocation 
%\citep{hecht2011tweets,schulz2013multi,han2014text}.
%We do not consider this metadata in our prediction problem;
%however, because it obscures the ability to predict locations from the text.
%
%For example, the metadata contains a user specified timezone field.
%The timezone provides a surprisingly large amount of detail about a tweet's location.
%At the bare minimum, 
%the timezone determines the tweets longitude with high accuracy,
%and so the problem of geolocating with timezone information reduces to the problem of determining the tweet's latitude.
%Some timezones (like Hawaiian Standard Time) determine both longitude and latitude with high accuracy,
%and so the tweet's text doesn't even need to be examined to perform geolocation.
%Furthermore, weets in Hawaiian Standard Time fit within a small 100km radius, which means the text doesn't even have to be examined.
%For timezones like Hawaiian Standard Time,
%this is an exceedingly easy problem.
%Knowledge of the timezone transforms the problem into only predicting latitude.
%For example, essentially all tweets in Hawaiian Standard Time fit within a small 100km radius, which means the text doesn't even have to be examined.
%Time zone information can also encode country location directly.
%For example, Korea and Japan have essentially the same longitude, 
%and so use the same offset from UTC for their time.
%But they have different timezones (denoted Korea Standard Time and Japan Standard Time).
%Knowledge of the timezone in these cases is sufficient to completely determine the country of origin.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item[Training and Evaluation Procedure.]
We train and evaluate our models in an online fashion using the Adam optimizer \citep{kingma2014adam}.
That is, we perform a single pass over the dataset.
For each tweet, we first compute the loss of the model on that tweet;
then we update the model's parameters.
The total loss is then the sum of the individual tweet losses.
We choose this evaluation method because it is a natural model for tweet geolocation where new data can easily be obtained and it is efficient computationally.

We use three types of losses to evaluate the models' performance.
The \defn{distance} measures the average distance between a tweet's true location and the point estimate given by the MFvM output.
The \defn{accuracy @50km} is the percent of tweets whose distance error is less than 50km.
The {accuracy @100km, @500km, 1000km, 2000km, and 3000km} measures are similar.
Finally, the \defn{accuracy @country} measures the percent of tweets whose country of origin was correctly predicted.
Numeric results are shown in Table \ref{table:results}.

To train the UnicodeCNN models,
we use a single machine with 16 CPUs, 64 GB of RAM, and 6 NVidia Titan x80 GPUs. 
We train using the Adam optimizer \citep{kingma2014adam} with a learning rate of $5\times10^{-4}$ and a batch size of 600.
We train in using data parallelism, so that each GPU processes 100 tweets per batch, 
and then the gradients are averaged together.
%The learning rate was set by training only on a small held-out subset of tweets,
The batch size of 100 tweets/GPU is the largest batchsize we could fit on a GPU with the huge model.
We observed about a 5-fold speed up using all 6 GPUs,
and in total, training the large model took about four weeks with this setup.
Training the huge model seems to take about \fixme{} time.
We let it train for two weeks but then ran out of time.
We expect the huge model's performance would continue to slightly improve given more training time.

All baseline models were trained using Adam \citep{kingma2014adam} and random hyperparameter search \citep{bergstra2012random}.
Random hyperparameter search is a state-of-the-art hyperparameter tuning method that requires no manual intervention and works well when there is more than one hyperparameter to tune.
For each model, we randomly selected a learning rate, L2 regularization strength, and (for the \str{lang+time+bow} model only) L1 regularization strength in the range $10^{-6}$ to $10^0$ distributed uniformly over the logarithm.
We trained 20 versions of each model and report only the best model.

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ignore{
\newpage
\section{Visualization}

A major disadvantage of CNNs is that they lack interpretability compared to simpler models.
To alleviate this problem, many techniques have been proposed to visualize CNNs in the context of image classification \citep{zeiler2014visualizing,seifert2017visualizations}.
We adapt these methods to provide the first visualization method of CNNs applicable to the text domain.
This visualization technique helps us understand the dialectical patterns that \uniloc~CNN discovers in the text.

The method is simple and inspired by the occlusion method proposed by \citet{zeiler2014visualizing} for visualizing image CNNs.
Given an input text with $n$ characters,
we generate $n-1$ new texts by removing the $i$th character from the original text.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\ignore{
\section{Languages and Day of Week}
\noindent\input{img/day-en}

\noindent\input{img/day-ar}

\noindent\input{img/day-es}

\noindent\input{img/day-fr}

\noindent\input{img/day-in}

\noindent\input{img/day-tl}

\noindent\input{img/day-tr}

\noindent\input{img/day-ja}

\noindent\input{img/day-ko}

\noindent\input{img/day-pt}

\noindent\input{img/day-zh}

\noindent\input{img/day-und}

\noindent\input{img/day-de}

\noindent\input{img/day-nl}

\noindent\input{img/day-cs}

\noindent\input{img/day-da}

\noindent\input{img/day-el}

\noindent\input{img/day-pl}

\noindent\input{img/day-ru}

\noindent\input{img/day-vi}

\noindent\input{img/day-th}

\onecolumn
\section{Languages and Time of Day}
\noindent\input{img/hr-ar}

\noindent\input{img/hr-en}

\noindent\input{img/hr-es}

\noindent\input{img/hr-fr}

\noindent\input{img/hr-in}

\noindent\input{img/hr-tl}

\noindent\input{img/hr-tr}

\noindent\input{img/hr-ja}

\noindent\input{img/hr-ko}

\noindent\input{img/hr-pt}

\noindent\input{img/hr-zh}

\noindent\input{img/hr-und}

\noindent\input{img/hr-de}

\noindent\input{img/hr-nl}

\noindent\input{img/hr-cs}

\noindent\input{img/hr-da}

\noindent\input{img/hr-el}

\noindent\input{img/hr-pl}

\noindent\input{img/hr-ru}

\noindent\input{img/hr-vi}

\noindent\input{img/hr-th}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ignore{
\newpage
\section{Reference notes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Languages}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Twitter applications}

\citet{cheng2010you} uses a probabilistic framework to generate city-level user home location using only tweet text.
Introduces a naive bayes model, but for some reason uses a sum instead of a product (bad math?).
Shows that this model does poorly,
and introduces a 'local words' weighting of the naive model to improve performance.
Introduces several smoothing mechanisms to help with the sparsity of tweet data.
They only consider tweets in the contiguous US.

\citet{kinsella2011m} predicts the location of individual tweets and a user's home location.
Provides a simple probabilistic model: 
For each location, estimate a distribution of terms associated with that location.
Does not incorporate time.
A straightforward generalization of \citet{cheng2010you} to tweet location instead of user location.

\citet{li2012towards} uses a probabilistic framework to generate city-level location using the content of the tweet and the network of tweet replies.
Does not use a gazetteer or the underlying social graph.
This seems like a straightforward extension of \citet{cheng2010you}.

\citet{han2013stacking} predicts the city of a twitter user using both text and metadeta using stacking.
I believe this is the first paper to consider the effect of semantic shift in geolocation.
They show that user declared location metadeta is more sensitive to temporal change than the message text.

\citet{mahmud2014home} identifies the home location of a user rather than the location of an individual tweet.
Incorporates temporal information in the tweets to identify when a user is travelling.
\fixme{Has good twitter geolocation references that I don't have elsewhere.}
Uses an ensemble classifier with a gazeteer as one of the key features.
Lots of manual constructions and NLP-based preprocessing.
Hierarchical model that first predicts a general geographic region (e.g. timezone or state), then predicts the city.

\citet{han2014text} has many new ideas.
Introduce a multilingual dataset and the first methods for geolocating non-English tweets.
They use a hierarchical model that first determines the language,
then selects a model appropriate for the language.
Make heavy use of twitter metadata (e.g. tweet time) to determine location.
Perform a test on time where they evaluate their model on data collected 1 year after the training data, and show a significant performance loss.
Studies the privacy implications of geolocation.

\citet{compton2014geotagging} propose a simple convex problem for geolocating twitter users to city level accuracy using the social network graph only.
Whereas previous methods rely on local heuristics, their convex program uses global properties of the social graph.
Has lots of empirical results showing accuracy of self reported locations, location homophily among friends, and typical travel habits of twitter users.

\citet{rahimi2015twitter} uses both twitter text and the network graph for geolocation, but does not include time.
\fixme{Has good twitter geolocation references that I don't have elsewhere.}
Good experiments with good datasets.
Uses kd-tree for faster search.
Spatial labels are discretized over an adaptive grid based on the number of users in the region.
The @-mention information is used to build an undirected graph between users.
They convert the graph into a ``collapsed graph'', and there's lots of subtleties here about how they handle the train/test split and edges that pass between the two sets.
Uses Model Adsorption over the graph to predict geolocations within the test set, 
with two key modification:
(i) removing celebrity nodes from the network graph (lets them scale to larger networks)
and (ii) incorporating textual information as ``dongle nodes''.

\citet{dredze2016geolocation} studies time's effect on geolocation of individual tweets to the city level.
Demonstrates cyclical temporal effects on geolocation accuracy and rapid drops in accuracy as test data moves beyond the training data's time period.
They show that this temporal drift can be countered with modest online model updates.
Introduces a particularly large new dataset.
Used vowpal wabbit to learn the model.

\citet{duong2016near} does regression to predict the gps coordinates without taking into account the geometry of the earth.
Uses matrix factorization of a bag-of-words type model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Twitter analytics}

\citet{hecht2011tweets} measures the accuracy of the location field in twitter user profiles.

\citet{dredze2013carmen} introduces the Carmen system for twitter geolocation.
Then use it to improve influenza surveillance.
Carmen does not do prediction of location from text,
but instead only measures accuracy of the various location fields.

\citet{he2015hawkestopic} uses Hawkes process to model the social graph of Twitter,
but does not apply the idea to geolocation.

\citet{graham2014world} survey of geolocation methods for geographers.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Emojis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Generic applications}

\citet{eisenstein2010latent} presents a multilevel generative model that resents jointly about latent topics and geographical regions.
Highly cited, and probably the right foundation for my graphical model.
Does not incorporate time or model the geometry of the regions.
Uses mean field variational inference.
\fixme{Think about this more.}

\citet{speriosu2010connecting} models language and geography outside the Twitter context for toponym resolution (disambiguating place names).
Uses a graphical model based on probabalistic topic models,
where regions of the earth are represented by different topics.
Inference is done with a collapsed Gibbs sampler.
Does not incorporate the Earth's spherical geometry or any distance relations between locations.

\citet{wu2017link} proposes a model for predicting the generation of new links in social networks. 
Uses a convex optimization problem with closed form solution.

\citet{yu2017temporally} uses matrix factorization to predict the formation of new edges in social networks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Less important work}

\citet{leidner2011detecting} provides a tutorial on methods for parsing geographical references in natural language.

\citet{jurgens2015geolocation} surveys existing methods.
They show a large performance gap between real world performance and the idealized laboratory-performance reported in the compared methods' publications.
\fixme{Review their references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Other applications of social network analysis}
%
%\citet{ruiz2012correlating} uses tweets to predict financial markets.
%
%\citet{wiley2014pharmaceutical} uses tweets to measure the effectiveness and user satisfaction of new drugs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Deep learning}
%
%\citet{kim2016character} proposes using character level CNNs, RNNs, and Highway Networks for language translation.
%\citet{chung2016character} also proposes character level RNNs and highway nets for translation.
%\citet{jozefowicz2016exploring} is a generic classification paper for character level text processing.  
%Uses a combination of CNNs and RNNs, and a hierarchical softmax which might be useful for locations.
%
%\citet{dhingra2016tweet2vec} creates a vector space model of tweets using a character level recurrent GRU network.
%\citet{severyn2015unitn} use a word level CNN for sentiment classification on tweets.
%
%\citet{conneau2017very} use resnet like connections to create very deep character cnns for text classification.
%\citet{zhang2015character} use a smaller depth character CNN, which is what I've implemented so far.
%
%\citet{weyand2016planet} do geolocation of photos using image CNNs.
%They divide the globe into 26263 regions, and use a xentropy loss over those regions.
%Regions are of different sizes so that they all contain approximately the same number of photos.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ignore{
\section{Problem Overview}

Previous work focuses on learning a parameterized model for tweet location.
The advantages of these methods are:
\begin{enumerate*}[label=(\arabic*)]
    \item the resulting models are simple, and
    \item the models can be trained and deployed on low-power devices.
\end{enumerate*}
The disadvantages are:
\begin{enumerate}
    \item These methods can model recurring space/time interactions 
        (e.g. patterns caused by timezone differences and weekend/nonweekend behavioral patterns), 
        but they cannot handle one time outlier events such as natural disasters or entertainment events.
    %\item They provide point estimates a tweet's location rather than a distribution of possible locations.
    \item Experiments by \citet{mahmud2014home} and \citet{dredze2016geolocation} show that the models do not generalize well to unseen time periods.
\end{enumerate}
I propose a nonparametric approach to geolocation that should improve these disadvantages.
The tradeoff is that the model is more complex and not deployable on low-power devices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proposed Solution}

Let $\{e_i\}_{i=1}^n$ be the set of observed tweets,
and $e$ be a new tweet not in the database.
Then define the nonparametric distribution over $e$ 
\begin{equation}
    \prob e
    \propto
    %\frac 1 n 
    \sum_{i=1}^n \exp\left(-\dist_\theta(e,e_i)^2\right)
    ,
\end{equation}
where $\dist_\theta$ is a distance metric between two tweets that depends on parameter vector $\theta$.
If we let $e(\ell)$ denote the tweet $e$ updated to have location $\ell$,
then the distribution of the tweet's location is given by
\begin{equation}
    \prob {e(\ell)} 
    \propto
    %\frac 1 n 
    \sum_{i=1}^n \exp\left(-\dist_\theta(e(\ell),e_i)^2\right)
    ,
\end{equation}
and the maximum likelihood point estimate of the tweet's location is
\begin{equation}
    \label{eq:hatell}
    \hat\ell
    =
    \argmax_{\ell} 
    \sum_{i=1}^n \exp\left(-\dist_\theta(e(\ell),e_i)^2\right)
    .
\end{equation}
The quality of the estimate $\hat\ell$ is affected by three factors:
\begin{enumerate}
    \item \emph{The number of tweets in the database.}
        Standard results in nonparametric distributions show that as $n\to\infty$, 
        the distribution $\prob{e}$ will approach the true underlying distribution.
        %Since the underlying distribution is highly complex, 
        %many samples will be needed to get a reasonable approximation.
        %Fortunately, we have many tweets available to us,
%
        The downside of this strategy is that the summations above are summations over the entire database.
        This is not practical, so the summation will need to be approximated.
        There are many good metric data structures that can restrict the summation to only the most relevant portions of the space.

    \item \emph{The family of distance metrics $\dist_\theta$.}
        The distance metric needs to tie information about a tweet's location to information about the other features in a tweet.
        There are many possible metric families,
        and finding the optimal one is likely a difficult challenge.

        A simple approach is to use an ``ensemble of metrics.''
        Let $\{d^{(i)}\}_{i=1}^m$ be a set of $m$ metrics.
        Then define the Mahalanobis distance
        \begin{equation}
            \label{eq:mahalanobis}
            \dist_\theta(e_1,e_2) ^2
            =
            \trans{
            \begin{pmatrix}
                \dist^{(1)}(e_1,e_2) \\
                \dist^{(2)}(e_1,e_2) \\
                \vdots \\
                \dist^{(m)}(e_1,e_2) \\
            \end{pmatrix}
            }
            \theta
            \begin{pmatrix}
                \dist^{(1)}(e_1,e_2) \\
                \dist^{(2)}(e_1,e_2) \\
                \vdots \\
                \dist^{(m)}(e_1,e_2) \\
            \end{pmatrix}
        \end{equation}
        %where $A : \R^{m\times m}$ is the parameter vector $\theta$ that is to be learned.
        where $\theta$ is a $m\times m$ matrix that needs to be estimated from the data.

        The ensemble should include several metrics related to gps coordinates 
        (e.g. the geodesic distance \citep{vincenty1975direct}, the google maps distance),
        time 
        (e.g. total distance in time, and the distance in time mod hourly, daily, weekly, monthly, and yearly intervals),
        social graph distances
        (e.g. total number of hops in a friendship graph),
        and any features of the tweet itself.

        There are two possible drawbacks of a large ensemble.
        First, a larger ensemble will increase the computational burden of computing distances.
        This can possibly be ameliorated by a pruning strategy that only evaluates the more expensive distances after the cheaper ones if it is actually necessary.
        Second, more metrics increases the number of parameters, increasing the possiblity of overfitting.
        The number of available tweets is so large, however, that this seems unlikely for a linear Mahalinobis distance.

        %These ensemble metrics can be extended by:

    \item \emph{The quality of the estimated parameter vector $\theta$.}
        %We can solve for $\hat\theta$ using the formula
        A good parameter vector $\hat\theta$ will maximize the likelihood that
        \begin{equation}
            \label{eq:hattheta}
            \hat\theta
            =
            \argmax_{\theta} 
            \sum_{i=1}^n 
            \sum_{j=1}^{i-1}
            \exp\left(-\dist_\theta(e_i,e_j)^2\right)
            .
        \end{equation}
        This can be solved with an SGD procedure.
        Using only a small sample of the data should be sufficient.
        %For the Mahalanobis metric above,
        %this optimization is convex.
        %In general, however, for other metrics, it need not be.
        %An important decision will be the choice of regularizer for $\theta$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newpage
\section{Research Questions}

\noindent
Applied questions
\begin{enumerate}
    %\item \textbf{How do we efficiently perform the sums?}
    \item Does jointly training the feature space and metric parameters improve performance?
    \item Many possible choices of metric to experiment with to determine which is best for different applications.
        For example, how should we incorporate information related to ``location'' vs ``gps'' fields of the tweet?
    \item Which regularizer should be chosen to optimize $\hat\theta$ in \eqref{eq:hattheta}?
        Is there a better loss function to use?
    \item Queries in this framework can be significantly more complicated:
        \begin{enumerate}
        \item We can easily in incorporate constraints into the queries.
            For example: Given that I know the tweet came from the midwest,
            which city was it most likely to come from?
        \item Given that a tweet came from a particular location,
            what time was it most likely tweeted?
            What user mostly likely sent the tweet?
        \item
            Generate a tweet that is likely to have been sent from location X at time Y.
            (This requires that the metrics in the ensemble have a generative semantics.)
        \end{enumerate}
\end{enumerate}

\noindent
Theoretical questions:
\begin{enumerate}
    \item \emph{Nonconvex optimization.}
        The optimization in \eqref{eq:hatell} is nonconvex.
        Finding a unique global optimum, however, is not likely to be important for practitioners.
        Instead, reporting several representative solutions will likely be more useful.
        These solutions should:
        \begin{enumerate}
            \item not get stuck in ``small'' local optima, and
            \item represent all the largest modes in the distribution.
        \end{enumerate}
        What is the best way to optimize under these constraints?

    \item \emph{Metric ensembles.}
        The theoretical properties of ensembles of metrics like proposed in \eqref{eq:mahalanobis} have not been explicitly studied before.
        \begin{enumerate}
            \item How does the dimension of the ``submetrics'' influence the dimension of the resulting ensembled metric?
                It is likely that each metric will have ``different dimensions at different scales,''
                and how do we incorporate this information to improve runtime bounds on queries?
            \item What are the best ways to ensemble the metrics? 
                \begin{enumerate}
                    \item Other $L_p$ combinations can be used besides $L_2$.
                    \item The matrix $\theta$ could be replaced by a higher order tensor to capture more complicated relationships between the submetrics.
                    \item We can kernelize the Mahalanobis distance to create a nonlinear family of metrics.
                    \item Using parameterized metrics as the base metrics naturally results in a ``deep'' metric learning problem.
                \end{enumerate}
            \item Can we develop metric data structures specifically tuned for ensemble metrics?
                \begin{enumerate}
                    \item Can we efficiently add/delete/modify metrics in the ensemble without recreating the data structure?
                    \item Can we query using only a portion of the metrics in the ensemble?
                    \item Can we support database style queries distributed over multiple machines? 
                \end{enumerate}
        \end{enumerate}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Problem description}

Existing work on tweet geolocation uses only text features to predict location.
I want to use time as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Intuitive motivation}

Time by itself is a good predictor of tweet location.
People are more likely to tweet during daytime hours,
and daytime is determined by longitude.
Combining time with textual features is even more powerful.

Consider a tweet with the text ``I am at the Taylor Swift concert''
(or alternatively a video of Taylor Swift performing).
If we knew the time of the tweet, 
and we knew Taylor Swift's concert schedule,
then we could get a good prediction of the location of the tweet.
Without knowing the time, however, we cannot say which city the tweet was sent from.
We actually don't even need to know Taylor Swift's concert schedule.
These concerts have many thousands of attendees, many of whom are tweeting.
If any of these attendees tweets about Taylor Swift and has geotagging enabled,
we should be able to use this information to infer that the original tweet was from the same location
(since it happened at the same time).

The example of a Taylor Swift concert happens over a narrow time scale and in a small location.
Other examples of space/time dependencies occur at larger scale.
For example: 
(i) natural disasters such as floods can affect multiple cities over many weeks;
(ii) normal weather conditions (such as snow, rain, or heat) affect latitudinal bands over the course of seasons;
and (iii) elections can affect entire cities, states, and countries for months.
We would like a system that can automatically identify these space/time dependencies and use them to predict tweet location.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection {Formal problem statement}

We call each tweet an event $e$ and decompose it into three components: 
\begin{align*}
    %g    &= \text{the gps coordinates} \\ 
    %\ell &= \text{the location (either the location field in the tweet or exact gps coordinates)} \\
    \ell : \R^2 &= \text{the location represented as gps coordinates} \\
    x    : \R^d &= \text{document features (constructed from any text/images/video/urls in the tweet)} \\
    t    : \R~ &= \text{time}
\end{align*}
%Existing approaches model the probability distribution $\cprob{\ell}{d}$.
%Existing baseline approaches estimate the distribution $\cprob{\ell}{d}$.
Our goal is to estimate the conditional distribution $\cprob{\ell}{x,t}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection {Proposed technique}

I propose a nonparametric model for estimating the density.
Specifically, 
\begin{equation}
    \label{eq:nonparam}
    %\prob{e} = \frac 1 n \sum_{i=1}^n k(e,e_i)
    \prob{e} = \frac 1 n \sum_{i=1}^n \exp(-\dist(e,e_i)^2)
\end{equation}
where $\dist$ is a metric distance that describes the similarity of two tweets.
Learning the model will consist of setting parameters in the distance function.
The main challenge of large scale nonparametric models is that the summation in \eqref{eq:nonparam} is over the entire data set.
A clever metric data structure will be needed so that only a small fraction of the data will need to be searched.

Many metrics can be defined between tweets,
and each metric will determine a corresponding probability distribution.
The simplest metrics for location estimation ignore all the information in the tweets except the location.
The most obvious distance function is to use the great circle distance $\dist\gcirc$ between two gps coordinates.
The obvious formula contains numerical instabilities,
and so Vincenty's formula should be used instead \citep{vincenty1975direct}.
A bandwidth parameter would need to be estimated from the data.
%More powerful distance functions would take into account the rate that information can travel between two locations.
%We can define the google maps distance $\dist\gmap$ to be the length of time that google maps says it takes to drive between two locations.
%This distance is more likely to be too expensive to compute;
%and while it may be more accurate than $\dist\gcirc$,
%it is unlikely to be the best distance measure possible.
%Ideally, we would use a metric learning algorithm to achieve the best possible metric.

I think a learned metric would be a good way to associate textual and spatial features.
If $\Dist$ is a family of metrics and $\loss$ a loss function,
then the training procedure solves
\begin{equation}
    \hat\dist = \argmin_{\dist\in\Dist} \sum_{e_1,e_2\in E} \loss(\dist(e_1,e_2))
    .
\end{equation}
As in the testing procedure, evaluating the sum is computationally expensive and clever data structures are needed to compute it efficiently.
The efficacy of the method will be determined by the choice of distance family $\Dist$.

\ignore{
If the location is independent of the document and time, then $\cprob{\ell}{x,t} = \prob{\ell}$.
This can be modeled as a mixture of $n$ isotropic Gaussians.
That is,
\begin{equation}
    \prob{\ell} = \sum_{i=1}^n w_i\normal{\mu_i}{\eye\sigma_i}
    .
\end{equation}
There are 488 cities in the worldwith at least 1 million people, 
so around 1000 seems like a reasonable choice of $n$.
The parameters $w_i$, $\mu_i$, and $\sigma_i$ must be learned from the data.

Next we incorporate a dependence on time.
Let $k : E \times E \to \R$ be a kernel function that assigns a similarity to two tweets.
As simple gaussian kernel could be
\begin{equation}
    k(e_1,e_2) 
    = \exp(-\tau\abs{e_1(t) - e_2(t)}^2
           -\lambda\ltwo{e_1(\ell) - e_2(\ell)}^2)
\end{equation}
where $\tau$ and $\lambda$ are parameters that control the influence of temporal and spatial distance in the events' similarity.
We can then define the conditional distribution
\begin{equation}
    \cprob{\ell}{t} = p(\ell)\exp(-\sum_{\substack{\text{events $e'$ s.t.}\\e'(t)<t}} k(e,e'))
    .
\end{equation}

Finally, we can incorporate a dependence on the textual features simply by updating the kernel to include a dependence on the text.
}

%A Hawkes process is an appropriate model for $\cprob{\ell}{t}$.
%In a Hawkes process,
%the likelihood of an event at location $\ell$ is high when events have recently happened in nearby locations,
%and low when events have not recently happened nearby.
%Formally,
%\begin{equation}
    %\cprob{\ell}{t} 
    %%\propto
    %=
    %\prob{\ell}\exp(-\lambda(\ell;t))
%\end{equation}
%where $\lambda$ is called the rate function and is given by
%\begin{equation}
    %\label{eq:kernel}
    %\lambda(\ell;t) = \sum_{e} k(\ell,t,e,\theta)
%\end{equation}
%where $k$ is a kernel function that depends on parameter $\theta$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
}

\ignore{
\subsection{Computational notes}

\begin{enumerate}
    \item
        The summation in \eqref{eq:kernel} is over all tweets,
        which is obviously infeasible.
        Any reasonable choice of kernel will have a large value only for tweets nearby in space/time.
        Therefore distant points can be pruned from the summation to improve computation time.

    \item
        As formulated, the features can be computed completely independently from modeling of the space/time relations.
        It may be possible to improve the performance by jointly optimizing the feature selection algorithms with the space/time objectives.

    \item
        Modeling the locations $\ell$ as points in $\R^2$ induces distortions,
        because the data is actually gps coordinates on a sphere.
        Restricting the data to lie on the sphere would be better,
        but I'm not sure how to enforce that type of constraint.
        It might be easier (and more accurate!) to learn the topology of the underlying space from the data.
        This could help incorporate non-gps location data as well,
        and make the system usable on non-twitter data.
        For example, we could predict the university/funding agency for a piece of research in a citation social network.

    \item
        We should be able to use a semisupervised learning procedure to use all the tweets without location data.

\end{enumerate}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ignore{
\appendix
\newpage
\section{Limitations of Prior Work}

%\begin{description}
%\item[Prior work on geolocating tweets.]
%A large body of work has formed around the problem of geolocating tweets,
%but existing solutions work only on a small subset of available tweets that are particularly easy to geolocate.
%The earliest methods work for English language tweets in the United States \citep{},
%and state-of-the-art methods generalize to English tweets from major cities around the world \citep{}.
%These methods all take advantage of the fact that certain English words are highly correlated with certain regions.
%\fixme{For example, the word \str{Chargers} is highly correlated with tweets from San Diego, CA,
%because the Chargers football team is located there.}
%These existing methods do not work for most tweets, however,
%because most tweets are neither located in the United States nor in English (see Figure \ref{fig:country/lang}).
%\citet{maier2014language} considers geolocating Spanish tweets.
%Our method works on tweets in any language sent from any location in the world.
%
%\citet{park2013emoticon} studies emoticon usage in the first three years of the twitter platform.
%They show, for example, that vertical emoticons are indicative of asian cultures and horizontal emoticons of european cultures.
%\citet{lu2016learning} show that emoji usage varies around the world on smart phone SMS messages using a keyboard app.
%\citet{ljubevsic2016global} shows that emoji usage can be used to predict tweet location.
%
%\citet{nguyen2017kernel} use a RKHS method for geolocation.

%\citet{hays2008im2gps} performs image geolocation down to the gps coordinate level using $k$-nearest neighbor queries and then builds a kernel density estimate of the distribution.
%\citet{crandall2009mapping} uses a classification strategy.
%\citet{weyand2016planet} discretizes the world in a semantically meaningful method.

%\item[Prior work on multilanguage models.]
%Bilingual models have been developed for many tasks,
%but models supporting more than 2 languages remain uncommon \citep{ruder2017survey}.
%
%\citet{zaidan2014arabic} Arabic dialects.
%
%\citet{refaee2014arabic} Arabic with twitter.
%
%\citet{mohammad2016translation} translation alters sentiment.
%\end{description}

%%%%%%%%%%%%%%%%%%%%
\subsection{Problems with word-level features}
\label{sec:words}

Traditional text mining generates word level features in a three step process:
First, the input text is tokenized to extract words from the text.
Second, similar tokens are grouped together via lemmatization and stemming subroutines.
Finally, a feature vector is created using simple summary statistics such as $n$-grams or TF-IDF \citep{}.
This process works well for English-only text corpora,
but is essentially impossible in the highly multi-lingual corpus we consider.
In this section, we highlight four problems with these word-level features.

\begin{description}
    \item[Tokenization in multilingual corpora is difficult.]
        Tokenization of English text is easy because spaces are used to separate lexemes.%
        \footnote{Recall that a \defn{lexeme} is the basic lexical unit in language.
        In English and other European languages a lexeme and word are essentially the same.}
        Other languages, however, do not have similar cues to separate lexemes.
        For example, Chinese and Japanese do not use spaces at all,
        and Vietnamese traditionally uses spaces between syllables even within words.
        %Germanic languages use frequent compound words.
        Good tokenizers for these languages have existed for decades \citep[e.g.][]{fung1998extracting,huyen2008hybrid},
        however these tokenizers have four limitations that make them unsuitable for our task:
        First, they require that the text's language be known a priori so that an appropriate tokenization routine can be called.
        The Twitter API has a \str{language} field associated with each tweet,
        however the identified language is often wrong.
        Second, they cannot be used simultaneously on the same text.
        Many tweets are written in multiple languages simultaneously in what linguists call \defn{code switching}.
        No existing tokenizers support code switching.
        Third, existing tokenizers do not consider dialectical differences in their target language which may be of importance in geolocating.
        For example, \citet{blodgett2016demographic} show that standard tools for parsing tweets do not work well for the African American English dialect.
        Finally, these tools require clean input that has both proper spelling and grammar,
        but social data is notoriously unclean.
        Besides obvious mistakes, tweets often use language in nonstandard ways to create new meaning, for example with emoticons \citep{}.
        Twitter specific tokenizers have been developed, 
        but only for the English language \citep{o2010tweetmotif,gimpel2011part,owoputi2013improved}.

    \item[Many languages use multiple writing systems.]
        The choice of writing system both provides clues about the location of a tweet.
        For example, The small European country of Moldova uses Romanian as its official writing system.
        Romanian was traditionally written with the Cyrillic (Russian) alphabet,
        but after the collapse of the Soviet Union in 1991,
        Moldova officially switched to using the Latin alphabet instead.
        Except for the province of Transnistria which continues to use the Cyrillic alphabet.

        Similarly, the Indonesian family of languages was traditionally written using the \defn{Jawi} alphabet,
        which is a variant of the Arabic alphabet.
        Recently, the countries of Indonesia, Malaysia, and Singapore stopped officially using the Jawi alphabet in favor of the latin-based \defn{Rumi} alphabet.
        Brunei, in contrast, continues to officially use both the Jawi and Rumi alphabets.
        On twitter, approximately \fixme{} tweets in these languages use the Jawi alphabet.

        The choice of alphabet to use can be highly location-indicative.
        \defn{Transliteration} is the process of converting a text written in one writing system to another.
        There is an official transliteration system between the Jawi and Rumi systems,
        but official transliterations are not always available or followed.
        For example, the \defn{Arabizi} writing system has developed in Arabic speaking countries as a way to write arabic on social media platforms like Twitter in Latin script.
        Many users mix Arabizi with standard arabic characters \citep{bies2014transliteration,tobaili2016arabizi,van2016simple}.

        The Japanese language uses three different writing systems.
        For example, the capital city Tokyo may be written in
        Hiragana as \begin{CJK}{UTF8}{min}とうきょう\end{CJK}, 
        Katakana as \begin{CJK}{UTF8}{min}トンキン\end{CJK}, 
        or Kanji as \begin{CJK}{UTF8}{min}東京\end{CJK}.


    \item[Verb conjugations.]
        English verbs are relatively simple.
        For example, the verb \str{talk} has only three forms:
        \str{talk},\str{talks},and \str{talking}.
        Verbs in other languages, however, 
        are much more complicated and exhibit regional variation.
        Consider the following example from Spanish.
        The verb \text{poder} (which means ``to be able to'') is written

        Even English has the \str{Imma} conjugation.

        AAE frequently omits the copula (i.e.\ \str{to be}) \citep{pullum1999african} 

        \citet{tinoco2017variation} use twitter to study the geographic differences of \str{iste} vs \str{istes}.

    \item[Spelling errors.]
    %\item[Phonology influences spelling errors].
        \citep{treiman2000dialect} studies the differences in spelling mistakes between American and British English.
        \citet{ahmed2015lexical} proposes a method for automatically fixing the spelling mistakes inherrent in twitter data,
        but the method is limitted only to English and does not account for dialectical variations.
%
    %\item[Consonant cluster reduction].
        One particular form of  
        For example, the word \str{left} is likely to get simplified to \str{lef} when the subsequent word is a vowel.
        \citet{guy1991contextual,tagliamonte2005new} study this phenomenon in standard English,
        \citet{pullum1999african} argues that this is a particular feature of the African American English dialect,
        and \citet{eisenstein2013phonological} shows that this phenomenon carries over into the context of tweets.

    \item[Relationships between words.]
        In standard German the word \str{radio} is neuter and so is written \str{das radio}.
        In Swiss German, however, the word \str{radio} is masculine and so is written \str{der radio}. \citep{hollenstein2014compilation}

        Compounding.

\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problems with output types}
\label{sec:problems:output}
}

\end{document}
